<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anthony Zhou</title>
    <description>Psychology, Economics, Programming, or anything interesting. Updated Weekly.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 03 Apr 2020 13:27:29 -0500</pubDate>
    <lastBuildDate>Fri, 03 Apr 2020 13:27:29 -0500</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Get Involved: Open Source Technology Fighting COVID-19</title>
        <description>&lt;p&gt;The purpose of this document is to provide a guide to the open source initiatives working to fight the novel coronavirus (COVID-19). Read on if you would like to help out.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Open source technology&lt;/strong&gt; refers to projects where the source code is freely available to the public, many of which would welcome anyone who can contribute.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you’re interested in contributing, see if these projects have issues in their GitHub repositories or other tasks that you might be able to help with. If you don’t think you have enough technical expertise, help spread the word! See if any of your friends and family might be able to help out with open source (1) Software or (2) Hardware.&lt;/p&gt;

&lt;h2 id=&quot;1-software&quot;&gt;1. Software&lt;/h2&gt;
&lt;p&gt;In the table below you will find a few open source projects to contribute to/look at. &lt;a href=&quot;https://github.com/soroushchehresa/awesome-coronavirus&quot;&gt;Here is a more complete list&lt;/a&gt; (note that the projects with more stars are probably more impactful and better to contribute to).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Project Name&lt;/th&gt;
      &lt;th&gt;Objective&lt;/th&gt;
      &lt;th&gt;Language&lt;/th&gt;
      &lt;th&gt;Code&lt;/th&gt;
      &lt;th&gt;Demo&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CHIME: Covid-19 Hospital Impact Model for Epidemics&lt;/td&gt;
      &lt;td&gt;Estimates the resources needed for hospitals (# of patients requiring ICUs, ventilation, etc.) for capacity planning.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/CodeForPhilly/chime/&quot;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://penn-chime.phl.io/&quot;&gt;Demo&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Locale.ai Covid-19 live visualization&lt;/td&gt;
      &lt;td&gt;Visualizes the pandemic using open source data.&lt;/td&gt;
      &lt;td&gt;JavaScript (Vue.js)&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/localeai/covid19-live-visualization&quot;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://covid19.locale.ai/&quot;&gt;Demo&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Tokyo Covid-19 Task Force Website&lt;/td&gt;
      &lt;td&gt;Official government website for all information about Covid-19 in Tokyo.&lt;/td&gt;
      &lt;td&gt;JavaScript (Node.js)&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tokyo-metropolitan-gov/covid19&quot;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://stopcovid19.metro.tokyo.lg.jp/&quot;&gt;Demo&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Novel Coronavirus Dataset (from JHU CSSE)&lt;/td&gt;
      &lt;td&gt;Largest available official dataset of Covid-19 cases.&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/CSSEGISandData/COVID-19&quot;&gt;Dataset&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6&quot;&gt;Visualization&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;2-hardware&quot;&gt;2. Hardware&lt;/h2&gt;
&lt;p&gt;Probably even more important than software right now is open source hardware. Current projects include low-cost ventilators, PCR tools, and distributed computing. Whether you’re a designer, builder, or anything in between, these projects could use your help! Contributing is not as standardized as with software, so I’d suggest you go join the &lt;a href=&quot;https://discord.gg/duAtG5h&quot;&gt;Open Hardware Summit Discord Server&lt;/a&gt; to find out how you can help (they have a #covid-19 channel).&lt;/p&gt;

&lt;p&gt;For an introduction to some of the biggest projects going on right now, check out &lt;a href=&quot;https://opensource.com/article/20/3/open-hardware-covid19&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading and best of luck contributing. Stay safe out there!&lt;/p&gt;

&lt;p&gt;— Anthony Zhou, open source advocate&lt;/p&gt;
</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/covid-19-open-source-technology/</link>
        <guid isPermaLink="true">http://localhost:4000/covid-19-open-source-technology/</guid>
        
        <category>coronavirus</category>
        
        <category>COVID-19</category>
        
        <category>open source</category>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>My Attempt at On-Device Machine Learning</title>
        <description>&lt;p&gt;In the past few years, on-device machine learning has taken the world by storm (or at least the worlds of some mobile app developers). The innovation — under development at companies like Apple, Google, and Facebook — promises to make any application of machine learning, from object classification to movie recommendation, seamless to experience on a mobile device. The underlying idea is simple: rather than using up ridiculous amounts of data sending files over the internet to be processed, why not do it all on the device itself?&lt;/p&gt;

&lt;p&gt;On-device machine learning opens up revolutionary possibilities for apps in every industry. As a spring break project, I decided to explore what it might mean for automated audio identification, which could help those with hearing impairment identify the sounds in their environments. In documenting my progress, I made sure to note challenges and failure as well as achievements and success, so that this report can be as helpful as possible to anyone attempting such a project in the future.&lt;/p&gt;

&lt;h1 id=&quot;planning&quot;&gt;Planning&lt;/h1&gt;

&lt;h2 id=&quot;my-goal&quot;&gt;My goal&lt;/h2&gt;
&lt;p&gt;To create a mobile app that can identify sounds in a recorded audio clip using machine learning.&lt;/p&gt;

&lt;h2 id=&quot;my-approach&quot;&gt;My approach&lt;/h2&gt;
&lt;p&gt;I hoped to use YAMNet, a pretrained machine learning model developed at Google, to run audio classification on an iOS app developed in Swift. The model makes inferences by taking in a WAV file as input and returning the sounds that might be present in the recording (helicopter, guitar, explosion, whistle, etc.).&lt;/p&gt;

&lt;p&gt;There are two ways I could go about this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;On-Device Machine Learning:&lt;/strong&gt; pre-train a machine-learning model and load it into a device, such as an iPhone or a Raspberry Pi. The model can then make inferences using the computational power of the device itself.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Traditional Machine Learning:&lt;/strong&gt; host the machine-learning model on a server, and make requests to the server whenever inference is performed. The server then responds to the request with the results of running the model on the given data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So what do these two options mean in the context of this project? Traditional machine learning would involve sending a request across the internet every time we need to make inferences (i.e., every time we need to identify the sounds in a recording). On-device machine learning eliminates the need to make these requests, so it is not only faster but also can run offline in areas with no data service.&lt;/p&gt;

&lt;p&gt;Given the clear advantages in speed and reliability, I decided to attempt on-device machine learning:&lt;/p&gt;

&lt;h1 id=&quot;making-the-app&quot;&gt;Making the App&lt;/h1&gt;

&lt;h2 id=&quot;1-install-and-run-the-model&quot;&gt;1. Install and run the model&lt;/h2&gt;
&lt;p&gt;For this step, I followed the &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/audioset/yamnet&quot;&gt;installation instructions&lt;/a&gt; in the YAMNet GitHub repository. Then, I downloaded some sample wav files from free audio sources online (such as &lt;a href=&quot;http://soundbible.com/&quot;&gt;this one&lt;/a&gt;) to test out the model.&lt;/p&gt;

&lt;p&gt;This whole process went smoothly because the installation instructions were clear.&lt;/p&gt;

&lt;h2 id=&quot;2-set-up-an-ios-recording-app&quot;&gt;2. Set up an iOS Recording App&lt;/h2&gt;
&lt;p&gt;Setting up a recording app in Swift was relatively simple, because of how much Apple documentation and Stack Overflow discussions explain the topic. Specifically, I decided to follow &lt;a href=&quot;https://www.youtube.com/watch?v=CrLbuzM9IDs&quot;&gt;this tutorial&lt;/a&gt; from The Swift Guy on Youtube (parts one and two). I ended up with a basic recording app that stores and plays back m4a recordings.&lt;/p&gt;

&lt;p&gt;This was my first time using Swift and XCode, and I must say I was quite impressed with the simplicity and ease of this work environment (especially the exceptional autocomplete in XCode). If the developer’s time is the most important resource in software development, this iOS development environment seems to get the job done.&lt;/p&gt;

&lt;h2 id=&quot;3-run-the-model-on-the-ios-app&quot;&gt;3. Run the model on the iOS App&lt;/h2&gt;
&lt;p&gt;Here’s where we get into uncharted territory, and where I get to choose from the two options I listed earlier.&lt;/p&gt;

&lt;p&gt;For machine learning on an iOS device, the two solutions I tried out were Google’s &lt;a href=&quot;https://www.tensorflow.org/lite&quot;&gt;Tensorflow Lite&lt;/a&gt; and Apple’s &lt;a href=&quot;https://developer.apple.com/documentation/coreml&quot;&gt;Core ML&lt;/a&gt; framework. Sparing the details, I did not end up succeeding with either approach, because I was not able to convert the pre-trained YAMNet model, made in Keras, to either a Tensorflow Lite (using TFLiteConverter) or Core ML model (using coremltools in Python). I believe the reason I kept getting errors was because the Keras model loaded pre-trained weights from an h5 file, resulting in a model structure that is slightly different from a freshly trained Keras model. As a result, the conversion libraries, like TFLiteConverter and coremltools, did not recognize the model.&lt;/p&gt;

&lt;p&gt;At this point, I could decide either to retrain the entire model and risk it not working either, or pivot to a different approach.&lt;/p&gt;

&lt;h3 id=&quot;creating-an-api&quot;&gt;Creating an API&lt;/h3&gt;
&lt;p&gt;As an alternative to on-device machine learning, I decided to deploy the model as an API using Flask, a lightweight Python library. For an introduction to Flask APIs, check out &lt;a href=&quot;https://programminghistorian.org/en/lessons/creating-apis-with-python-and-flask&quot;&gt;this tutorial&lt;/a&gt;. My API would accept a POST request with a wav file and return the sounds discovered by the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Audio App Data Model.jpg&quot; alt=&quot;Audio App Data Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Specifically, my function for inference, returning the top five identified classes, was a slightly modified version of the sample prediction function provided in the YAMNet GitHub repository:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    def infer(file):
        graph = tf.Graph()
        with graph.as_default():
            yamnet = yamnet_model.yamnet_frames_model(params)
            yamnet.load_weights('yamnet.h5')
        yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')

        # Decode the WAV file.
        wav_data, sr = sf.read(io.BytesIO(file), dtype=np.int16)
        assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype
        waveform = wav_data / 32768.0  # Convert to [-1.0, +1.0]

        # Convert to mono and the sample rate expected by YAMNet.
        if len(waveform.shape) &amp;gt; 1:
            waveform = np.mean(waveform, axis=1)
        if sr != params.SAMPLE_RATE:
            waveform = resampy.resample(waveform, sr, params.SAMPLE_RATE)

        # Predict YAMNet classes.
        # Second output is log-mel-spectrogram array (used for visualizations).
        # (steps=1 is a work around for Keras batching limitations.)
        with graph.as_default():
            scores, _ = yamnet.predict(np.reshape(waveform, [1, -1]), steps=1)
        # Scores is a matrix of (time_frames, num_classes) classifier scores.
        # Average them along time to get an overall classifier output for the clip.
        prediction = np.mean(scores, axis=0)
        # Report the highest-scoring classes and their scores.
        top5_i = np.argsort(prediction)[::-1][:5]
        return {&quot;predictions&quot;: [(yamnet_classes[i], str(prediction[i])) for i in top5_i ]}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It loads the model weights, reads in the file, adjusts the data to fit model requirements, makes the prediction, and returns a dictionary of 5 predictions.&lt;/p&gt;

&lt;h3 id=&quot;attempting-to-deploy-the-api-unsuccessful&quot;&gt;Attempting to deploy the API (Unsuccessful)&lt;/h3&gt;
&lt;p&gt;After making the API, I decided the next step was to deploy it online, so that I could call it from any device. However, due to the extraordinary size of the &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow&lt;/code&gt; module on Python, the program required over 550 MB of space, which could not be hosted on any free-tier service I experimented with, including Heroku and PythonAnywhere.&lt;/p&gt;

&lt;h3 id=&quot;calling-the-api-from-the-ios-app&quot;&gt;Calling the API from the iOS App&lt;/h3&gt;
&lt;p&gt;Finally, I decided to simply call the API locally from the iOS App, assuming that deploying the API online would be simple if I decided to pursue a paid or more complicated hosting plan.&lt;/p&gt;

&lt;p&gt;To call the API, I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;URLSession&lt;/code&gt; module in Swift, and simply displayed the response as an alert. My request was as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    func postRequest() {
        let url = URL(string: &quot;http://localhost:5000/api/v1.0/classify&quot;)!
        var request = URLRequest(url: url)
        request.setValue(&quot;audio/wav&quot;, forHTTPHeaderField: &quot;Content-Type&quot;)
        request.httpMethod = &quot;POST&quot;
        
        let filename = getDirectory().appendingPathComponent(&quot;\(numberOfRecords).wav&quot;)
        
        let task = URLSession.shared.uploadTask(with: request, fromFile: filename) { data, response, error in
            guard let data = data,
                let response = response as? HTTPURLResponse,
                error == nil else {                                              // check for fundamental networking error
                print(&quot;error&quot;, error ?? &quot;Unknown error&quot;)
                return
            }

            guard (200 ... 299) ~= response.statusCode else {                    // check for http errors
                print(&quot;statusCode should be 2xx, but is \(response.statusCode)&quot;)
                print(&quot;response = \(response)&quot;)
                return
            }

            let responseString = String(data: data, encoding: .utf8)
            
        
            
            do {
                let json = try JSONSerialization.jsonObject(with: data, options: [])
                if let object = json as? [String: [NSArray]] {
                    print(object[&quot;predictions&quot;]!)
                }
            } catch {
                print(error.localizedDescription)
            }
            UI {
                self.displayAlert(title: &quot;Response received&quot;, message: responseString ?? &quot;No message received&quot;)
            }
        }

        task.resume()
    }
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;final-product&quot;&gt;Final product&lt;/h1&gt;
&lt;p&gt;I ended up with an iOS app that takes recordings and identifies the sounds in the recordings. Here is a screen recording of the app where I recorded myself whistling (video has no sound):&lt;/p&gt;

&lt;video style=&quot;margin:auto&quot; controls=&quot;&quot;&gt;
  &lt;source src=&quot;/assets/videos/Audio App Screen Recording.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Overall, even though I was not successful in converting the machine learning model to an on-device version, I was happy with the end product of an audio identification app. Maybe the technology of on-device machine learning is not yet mature enough for use in all contexts, or maybe I’m just taking the wrong approach. Have any thoughts, questions, or suggestions? Leave a comment below!&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Hershey, Shawn, et al. “CNN Architectures for Large-Scale Audio Classification.” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, doi:10.1109/icassp.2017.7952132.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 19 Mar 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/audio-identification-app/</link>
        <guid isPermaLink="true">http://localhost:4000/audio-identification-app/</guid>
        
        <category>tensorflow</category>
        
        <category>keras</category>
        
        <category>python</category>
        
        <category>swift</category>
        
        <category>ios</category>
        
        
        <category>narrative</category>
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Why Sustainability Makes Economic Sense</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“We are on the edge of a fundamental reshaping of finance.”&lt;br /&gt;
– Larry Fink, 2020&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This January, BlackRock CEO Larry Fink announced in his &lt;a href=&quot;https://www.blackrock.com/corporate/investor-relations/larry-fink-ceo-letter&quot;&gt;annual letter to CEOs&lt;/a&gt; that climate change needs to be a major priority of companies.&lt;/p&gt;

&lt;p&gt;BlackRock – and by extension Larry Fink – manages nearly &lt;em&gt;$7 trillion&lt;/em&gt; in investments, making it the largest asset manager in the world. With a proven track record as an astute financial advisor, the billionaire is a trusted voice in the world of business. If he says we should protect the climate, he is not acting as a tree-hugging hippie but as a representative of his clients’ financial interests.&lt;/p&gt;

&lt;p&gt;In other words, Fink is saying that it makes financial sense for a company to invest in sustainability. To me, this was counterintuitive. Although I certainly hope we can slow climate change, I had always assumed that the profit models for companies that damage the environment demanded exploitation, even though such a model would be unsustainable in the long term. In other words, I thought that short-term interests will trump long-term goals.&lt;/p&gt;

&lt;h2 id=&quot;the-tragedy-of-the-commons&quot;&gt;The Tragedy of the Commons&lt;/h2&gt;

&lt;p&gt;Why would I and so many others think so lowly of companies? For me, this was a classic case of William Lloyd’s &lt;em&gt;Tragedy of the Commons&lt;/em&gt;. If you’ve never heard of it, here is a brief example: in Lloyd’s time (1833), English villagers customarily shared a piece of land where all their cattle grazed. Lloyd hypothesized that if one villager let too many cattle graze on the land, overgrazing would result. The renegade villager would profit greatly in the short term from owning so many cows, at the expense of the pasture’s long-term health. What if the other herders decide to do the same, so they can continue to profit? After a few years of this, the common would be completely destroyed, just because each one of the villagers made the individually rational decision to allow more cattle graze in the common. Wikipedia has &lt;a href=&quot;https://en.wikipedia.org/wiki/Tragedy_of_the_commons#Examples&quot;&gt;a great list of similar examples in the real world&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Cows_on_Selsley_Common.jpg&quot; alt=&quot;Cows on a pasture&quot; /&gt;
&lt;em style=&quot;display: block; font-size: 0.75em&quot;&gt;Cows on a typical English common. (Image: Sharon Loxton / Cows on Selsley Common)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It’s not hard to see how the same tragedy of the commons can apply to environmental destruction. In a capitalist economy, companies act in their economic self-interest, using up resources like coal and oil and forests and fish and even water. Abusing natural resources gives companies a clear competitive advantage over others who attempt to protect the environment. To a large extent, this is the economic dynamic that has caused rampant environmental destruction in the modern world.&lt;/p&gt;

&lt;p&gt;But what about climate change? At first glance, the tragedy of the commons seems to fit perfectly: the corporations are the herders, their manufacturing processes are the cattle, and the grass is the carbon they are releasing into the atmosphere, causing global warming. Unsustainable manufacturing processes are simply more profitable. Any company that tries to slow climate change is either just giving lip service for PR or is bound to fail, and even governments fall into the same trap as they compete for the investments of multinational corporations – climate change is inevitable and the world is going to burn! Tragedy of the Commons, QED!&lt;/p&gt;

&lt;p&gt;And that would be the case, except for one eccentric group of people: investors. Most investors, like Larry Fink, are not interested in the success of individual companies, but rather in the success of their portfolios as a whole, over the long term. It just so happens that investors control many of the world’s largest companies, because the investors provide their lifeblood: money. So it’s not exactly right to say that companies are always interested in short-term profitability. In fact, they are interested in whatever their stakeholders happen to be interested in, and for the past few decades the answer was always short-term profitability. But the tide is turning.&lt;/p&gt;

&lt;p&gt;Evidence is beginning to pile on for researchers who argue that climate change will fundamentally undermine the way our economy and society work. For investors, the evidence points to a simple axiom: &lt;strong&gt;climate risk is investment risk.&lt;/strong&gt; As Fink describes in his 2020 letter to CEOs, climate change touches nearly every building block of the modern economy, including municipal bonds, the 30-year mortgage, inflation, interest rates, and even emerging markets where extreme heat causes productivity to decline. It throws a wrench into every economic model and growth projection.&lt;/p&gt;

&lt;p&gt;Many investors are beginning to ask themselves whether it is still safe to invest in unsustainable practices. Perhaps even more than the rest of us, investors have a clear economic stake in avoiding climate risk: to protect the long-term sustainability of their portfolios. As a result, companies are starting to embrace sustainability disclosures and frameworks for managing environmental issues. Close scrutiny is still needed, but the foundations are in place for a revolutionary shift towards sustainability.&lt;/p&gt;

&lt;h2 id=&quot;the-implications-of-investor-incentives&quot;&gt;The Implications of Investor Incentives&lt;/h2&gt;

&lt;p&gt;It’s easy to get lost in the complicated turns of logic and the cascading of incentives, but the takeaway from Fink’s letter is simply that there is now a strong economic incentive to invest in sustainability. In other words, perhaps for the first time in history, environmentalism is no longer a conflict between economic and moral interests – it is a collaboration.&lt;/p&gt;

&lt;p&gt;This is such a critical point that I think it’s worth repeating:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Perhaps for the first time in history, environmentalism is no longer a conflict between economic and moral interests – it is a collaboration.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That’s why Fink’s letter has given me hope for the Earth’s climate. While certainly touching, moralizing and guilt-tripping have clearly been ineffective methods of catalyzing change, despite the impressive efforts of climate activists. Finally, economics has given us a real way out, a tangible answer to the crisis.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is not to say that everything is okay or that we should stop caring about the climate. In fact, human-caused climate change has already caused irreversible damage to the planet. Rather, having an economic understanding of climate change should give us motivation to pursue climate activism even more strongly, scrutinizing corporate sustainability disclosures and demanding environmentally conscious actions, because we know our actions have a real economic impact. As the evidence piles on, growing numbers of people are beginning to realize that the economic impact of climate change will be a net negative. Simply put,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sustainability makes economic sense.&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Mar 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/climate-change-economic-sustainability/</link>
        <guid isPermaLink="true">http://localhost:4000/climate-change-economic-sustainability/</guid>
        
        <category>climate change</category>
        
        <category>investment</category>
        
        <category>sustainability</category>
        
        
        <category>essay</category>
        
        <category>economics</category>
        
      </item>
    
      <item>
        <title>Can we achieve symbiosis with machines? Neuralink and Facebook try to answer the question.</title>
        <description>&lt;p&gt;Today’s internet-enabled devices are competing for our attention, all the time. What if there was some way we could escape the loop of digital addiction? What if we could become one with our machines?&lt;/p&gt;

&lt;p&gt;Enter the Brain-Machine Interface (BMI), also known as a brain-computer interface (BCI), mind-machine interface (MMI), or direct neural interface (DNI): a device that translates neural signals into commands that control hardware or software.&lt;/p&gt;

&lt;p&gt;The premise is simple. Currently, we interface with technology, and the outside world in general, through low-bandwidth options like keyboards and speech. For argument’s sake, assume that your words-per-minute (WPM) when speaking is 150 (about average, according to the National Center for Voice and Speech), and that your WPM when typing is 75 (you can get a rough estimate of your typing speed with &lt;a href=&quot;https://thetypingcat.com/typing-speed-test/1m&quot;&gt;this minute-long test&lt;/a&gt;). Compare that with your brain, a highly efficient computer that may have as many as 50 processes running at the same time, as noted in this article from &lt;a href=&quot;https://www.technologyreview.com/s/532291/fmri-data-reveals-the-number-of-parallel-processes-running-in-the-brain/&quot;&gt;the MIT Technology Review&lt;/a&gt;. Each one of us has a brain full of countless ideas and connections that never make it out into the real world, due simply to the ridiculously slow rate of communication between our powerful brains and the outside world.&lt;/p&gt;

&lt;p&gt;Think about how much time the average American spends on any kind of technology. &lt;a href=&quot;https://www.inc.com/melanie-curtin/are-you-on-your-phone-too-much-average-person-spends-this-many-hours-on-it-every-day.html&quot;&gt;According to Inc magazine&lt;/a&gt;, the answer is &lt;em&gt;over four hours&lt;/em&gt;. Our society is just beginning to realize the way digital overload can isolate us from nature and the people closest to us, but becoming a hermit and moving to a cabin in the woods doesn’t seem so feasible. Is there any way we could use technology to enhance our lives rather than distracting from them? This is the vision Facebook Reality Labs has for its Brain-Computer Interface:&lt;/p&gt;

&lt;h2 id=&quot;facebook-reality-labs&quot;&gt;Facebook Reality Labs&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Imagine a world where all the knowledge, fun, and utility of today’s smartphones were instantly accessible and completely hands-free. Where you could spend quality time with the people who matter most in your life, whenever you want, no matter where in the world you happen to be. And where you could connect with others in a meaningful way, regardless of external distractions, geographic constraints, and even physical disabilities and limitations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In July of 2019, Facebook Reality Labs released &lt;a href=&quot;https://tech.fb.com/imagining-a-new-interface-hands-free-communication-without-saying-a-word/&quot;&gt;this report&lt;/a&gt; detailing some cutting edge work with the BCI. The lab sponsored a group of University of California, San Francisco (UCSF) researchers who are working to help patients with neurological diseases like ALS use brain-computer interfaces to communicate. BCI technology, as the report notes, is not new: it already helps people “feed themselves, hold the hand of a loved one, and even fly a jet simulator.” So why don’t we all have computers in our brains? There are two limiting factors in BCI development: speed and invasiveness.&lt;/p&gt;

&lt;h3 id=&quot;talking-fast-and-talking-slow-balancing-speed-and-invasiveness-in-bmis&quot;&gt;Talking fast and talking slow: balancing speed and invasiveness in BMIs&lt;/h3&gt;

&lt;p&gt;Emily Mugler, an engineer on the Facebook Reality Labs BCI team, describes how the conventional approach of electroencephalography (EEG), where a cap of electrodes is placed on the subjects head, was just not fast enough for patients with ALS to communicate their ideas effectively. “It sometimes took 70 minutes for a patient to type a single sentence,” she says.&lt;/p&gt;

&lt;p&gt;Later, some labs attempted using electrocorticography (ECoG), which was faster but required a surgical operation inserting electrodes into the brain. Herein lies the problem that plagued past attempt to develop an effective BCI: fast technologies tend to be too invasive, whereas noninvasive solutions tend to be too slow. Researchers found themselves perplexed with the problem of getting inside the brain without physically getting inside the brain.&lt;/p&gt;

&lt;p&gt;Facebook Reality Labs, together with the lab at UCSF, plans to solve this problem with near-infrared light: by beaming harmless light waves into the subject’s brain, a wearable device can sense blood oxygenation—much like in an fMRI (functional magnetic resonance imaging) machine—and use the measurements to guess at brain activity. While the system is currently “bulky, slow, and unreliable,” the UCSF researchers have successfully converted thoughts to words in real time and hope to reach a speed of 100 words per minute on a 1000 word vocabulary with an error of less than 17% (&lt;a href=&quot;https://www.nature.com/articles/s41467-019-10994-4&quot;&gt;Read more in their &lt;em&gt;Nature Communications&lt;/em&gt; journal article&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The results are promising. Within a decade we might see keyboards by and large replaced by brain-machine interfaces, bringing us into a world where we could spend our lives connecting with others in a meaningful way, enabled by rather than encumbered by technology. But the universal adoption of BCIs raises serious issues of privacy. Can privacy even exist when our inner thoughts are exposed to the public? Moreover, Facebook is not the company many would most trust to safeguard their personal information, given its recent scandals.&lt;/p&gt;

&lt;p&gt;Fear not, for Facebook is not the only one developing this technology: Elon Musk has founded a company developing the Neuralink, a brain-machine interface with no qualms about physically invading your brain.&lt;/p&gt;

&lt;h2 id=&quot;neuralink---elon-musk&quot;&gt;Neuralink - Elon Musk&lt;/h2&gt;

&lt;p&gt;Elon Musk’s justification for the need for a brain-machine interface goes something like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You could sort of think of humanity as a biological bootloader for digital superintelligence
— Elon Musk&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here, “bootloader” refers to the small piece of code needed to start up a computer. To explain this idea further, let’s examine the recent trends in artificial intelligence, as viewed in a Muskian mindset:&lt;/p&gt;

&lt;p&gt;AI is set to overtake humanity as the next stage in the evolution of consciousness. We’ll have gone from amoebas to lizards to chimps to humans to machines. Or something like that. Scary, right? While the truth of Musk’s statement is debatable, the underlying trend rings true: artificial intelligence, and technology in general, is expanding to replace many traditionally human tasks, from manufacturing to truck driving to being a cashier at McDonald’s. How can we escape the ultimate supremacy of AI?&lt;/p&gt;

&lt;p&gt;The answer is that we integrate the machines into ourselves, so that we can work together with our revolutionary technology rather than competing against it. A brain-machine interface would help us both drastically increase our productivity and avoid becoming obsolete. If you subscribe to Musk’s view of the inevitable march of technology (detailed in &lt;a href=&quot;https://www.youtube.com/watch?v=f3lUEnMaiAU&quot;&gt;this interview&lt;/a&gt;), we have two options: brain-machine interface or extinction as AI takes over.&lt;/p&gt;

&lt;p&gt;How does the Neuralink work? By inserting tiny electrodes linked to wires with diameter of 4-6 microns, or less than a quarter of the thickness of a typical human hair. The procedure goes like this: drill a 2 mm hole in the subject’s brain, then use a “sewing machine” to weave up to 96 tiny threads into the brain, creating 3,072 channels. As with all things Elon, Musk plans to push the Neuralink forward at a rapid pace, starting human clinical trials by the end of 2020, focusing on brain diseases, and envisioning widespread adoption in 4 to 5 years.&lt;/p&gt;

&lt;p&gt;It sounds scary because it is. With apparently little regard to the potential brain damage incurred by invasive surgery, the Neuralink in its current state creates two major health risks: glial scarring and broken electrodes. Glial scarring refers to scarring in the brain tissue that can inhibit communication and represents a reaction to serious brain damage. Meanwhile, the small electrodes used by the Neuralink are impossible to extract if broken, meaning they’ll just be stuck in the patient’s brain.&lt;/p&gt;

&lt;p&gt;Musk offers a compelling vision of the future—one in which we have achieved a “symbiosis with machines”—but the Neuralink’s radical approach to and bold vision for the BMI may end up rushing to offer the world a half-cooked dish that leaves us a little sick to the stomach. Whether or not the technical and health-related challenges are resolved, the Muskian view would tell us that the need to stay relevant in a world of increasing automation outweighs the health risks incurred.&lt;/p&gt;

&lt;h2 id=&quot;what-happens-next&quot;&gt;What happens next&lt;/h2&gt;

&lt;p&gt;As with any technology, the brain-machine interface has great benefits and drawbacks that will affect people unequally, although both the utopian and dystopian views are probably overstated. In the end, many of our fears will probably be nullified as the technology improves and as a lack of privacy becomes the new normal. After all, the subjective truths of today, like privacy, may not hold steady even ten years into the future.&lt;/p&gt;

&lt;p&gt;And China’s going to do it anyway.&lt;/p&gt;

&lt;p&gt;No matter what, it remains essential that the public holds Facebook, Neuralink, and the smorgasbord of BMI startups accountable, to ensure we don’t rush blindly into a technology without asking ourselves whether it truly makes the world a better place.&lt;/p&gt;

&lt;h3 id=&quot;further-reading-and-works-cited&quot;&gt;Further reading and works cited&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;An article on the Neuralink: https://venturebeat.com/2019/07/16/neuralinks-technology-embeds-tiny-wires-in-the-brain-to-read-electrical-pulses/&lt;/li&gt;
  &lt;li&gt;The Neuralink plans to start clinical trials by the end of 2020: https://www.cnbc.com/2019/07/17/elon-musk-brain-machine-startup-neuralink-plans-human-trials-in-2020.html&lt;/li&gt;
  &lt;li&gt;The Wikipedia article talking about glial scarring and broken electrodes: https://en.wikipedia.org/wiki/Neuralink&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 03 Feb 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/brain-machine-interface/</link>
        <guid isPermaLink="true">http://localhost:4000/brain-machine-interface/</guid>
        
        <category>neuroscience</category>
        
        <category>artificial intelligence</category>
        
        
        <category>psychology</category>
        
        <category>essays</category>
        
        <category>science</category>
        
      </item>
    
      <item>
        <title>The Many Definitions of Intelligence</title>
        <description>&lt;p&gt;What was Albert Einstein’s IQ? Many sources online will tell you it was 160. But a quick foray into the supporting evidence for this number reveals that most sources have no scientific proof of Einstein’s IQ score. In fact, 160 may just be a relic of the scale used for measurement: WAIS-IV (Wechsler Adult Intelligence Scale 4), probably the most common IQ scoring system, happens to have a max score of — you guessed it — 160. Einstein could be the dictionary definition of genius in today’s society, yet we don’t even know his IQ. How, then, can we be sure of his intelligence?&lt;/p&gt;

&lt;p&gt;Well, we could look at his creation of a revolutionary new way of understanding the world through theoretical physics. In addition, IQ is a construct related to but separate from intelligence. Given that IQ does not answer our question, how can we truly measure Albert Einstein’s intelligence?&lt;/p&gt;

&lt;p&gt;As many recent critics of IQ have noted (duly), intelligence is a highly complex trait that cannot be reduced to a single score. However, IQ does demonstrate some correlation with job success, as noted &lt;a href=&quot;https://www.inc.com/business-insider/why-iq-big-factor-future-success-job-performance-according-science-research.html&quot;&gt;by Inc magazine&lt;/a&gt;. That is not to say that &lt;em&gt;intelligence&lt;/em&gt; correlates to job success, however.&lt;/p&gt;

&lt;p&gt;What exactly is intelligence, how can we quantify it, and how does it differ from IQ? These are the questions this article will attempt to answer, based on psychological research. After outlining the broad types of intelligence, defining intelligence becomes even more problematic when examining the details. The cultural assumptions inherent in almost all attempts to measure intelligence lead to the formation of problems like exclusivity and perhaps even cultural supremacy.&lt;/p&gt;

&lt;h3 id=&quot;whats-the-difference-between-iq-and-intelligence&quot;&gt;What’s the difference between IQ and intelligence?&lt;/h3&gt;

&lt;p&gt;Beginning with some definitions will help us clarify this discussion. First, IQ (Intelligence Quotient) is an artificial score, much like a high score in Mario Kart, that measures a person’s ability to perform on certain metrics. With Mario Kart, these skills depend on a person’s ability to navigate complex terrain at a high pace, dodging banana peels and throwing turtle shells. IQ tests, meanwhile, measure a person’s aptitude for problem-solving and verbal and spatial skills. Simple enough.&lt;/p&gt;

&lt;p&gt;Intelligence, however, has an deceptively simple definition. Oxford dictionary defines intelligence as “the ability to acquire and apply knowledge and skills.” This statement actually encompasses a vast variety of cognitive skills: memory, flexibility, and focus, just to name a few. Critically, defining intelligence in this way implies its dependence on environmental factors, because evidence has demonstrated that education has a clear influence on an individual’s ability to learn. Environmentally-influenced intelligence is the type that IQ is attempting to measure.&lt;/p&gt;

&lt;p&gt;So how can we measure intelligence as an intrinsic trait, based in genetics? We start out by recognizing this type of intelligence as separate from the traditional idea of IQ. Let’s call it Intelligence A.&lt;/p&gt;

&lt;h3 id=&quot;intelligence-a-vs-intelligence-b-vs-intelligence-c&quot;&gt;Intelligence A vs. Intelligence B vs. Intelligence C&lt;/h3&gt;

&lt;p&gt;A more nuanced way of defining intelligence is to realize that there is more than one definition. In a book comparing genetic and environmental determinants of intelligence, Philip Vernon helps define three types of intelligence: Intelligence A is intrinsic. It depends on basic physiological factors like reaction time. Intelligence B includes the effects from the environment. It refers to the application of Intelligence A to the real world, and fits quite well with the Oxford definition of intelligence. Meanwhile, Intelligence C refers to the measurement of intelligence, as in IQ tests. Measured values of intelligence often correlate much more closely with Intelligence B, because it has closely matched the popular conception of intelligence for the last few decades.&lt;/p&gt;

&lt;p&gt;Why is this distinction useful? Intelligence B, while clearly correlated with success, depends on the way a society is structured. In other words, it varies widely between cultures, and therefore between people. In other words, attempts to measure Intelligence B are highly variable and require that a new test is developed for every different culture. Meanwhile, Intelligence A consists of biological skills that should not vary across cultures.&lt;/p&gt;

&lt;p&gt;So it’s not surprising that IQ tests correlate with success — they reflect traits valued by our society and are therefore strengthened by other success-enhancing factors like education and socioeconomic status. If we want to truly measure the influence of intelligence on success &lt;em&gt;irrespective of culture&lt;/em&gt;, we need to examine the biological basis of intelligence.&lt;/p&gt;

&lt;p&gt;What exactly does a test of this type look like? The twentieth-century psychologist Hans Eysenck, known especially for his theory of introversion and extroversion, gives a possible example in the form of AEP (Auditory Evoked Potential), a test that allows researchers to measure the reaction time of subjects. He cited preliminary studies that found at least a 0.83 correlation between the AEP results and those of a WAIS test (the standard IQ test mentioned earlier). Apparently, this level of correlation is higher than those between functionally similar IQ tests like the WAIS and Binet tests. However, the study finding this correlation was performed on a notably small sample of individuals; therefore, it poorly represents the vast diversity in levels of education and socioeconomic status around the world.&lt;/p&gt;

&lt;h3 id=&quot;the-cultural-result-of-defining-intelligence&quot;&gt;The cultural result of defining intelligence&lt;/h3&gt;

&lt;p&gt;As noted by &lt;a href=&quot;https://www.apa.org/monitor/feb03/intelligence&quot;&gt;this American Psychological Association (APA) cover story&lt;/a&gt;, traditional measures of intelligence translate quite poorly across borders, raising problematic implications for cross-cultural interaction — an IQ test as currently given encapsulates the traits that are considered intelligent &lt;em&gt;in a particular culture&lt;/em&gt;. If a person comes from a culture where those traits are not so valued, they may suddenly find themselves perceived as less intelligent for arbitrary cultural reasons. As noted earlier, IQ is correlated with success, so this cultural disparity may be a major factor behind the cultural barriers to success afflicting diverse modern societies.&lt;/p&gt;

&lt;p&gt;The APA cover story explains that many psychologists no longer believe in the idea that a test can be completely absent of cultural bias, and probably for good reason. After all, the tests they are thinking of measure &lt;em&gt;Intelligence B&lt;/em&gt;, which by definition encompasses cultural knowledge. Perhaps focusing instead on intelligence A as the standard definition for intelligence, as Hans Eysenck suggested, may be a better way to obviate cultural biases by directly measuring brain activity. Modern brain-scanning technology certainly makes this a promising alternative to current tests. As the world as a whole moves to adopt fair values that appreciate, rather than punish, diversity, intelligence, critical to economic success, should be reconceived in a form that transcends cultural boundaries, as Intelligence A.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Intelligence, as traditionally defined, represents the sum of myriad genetic and cultural factors, including reaction time, education, socioeconomic status, and so on. Also known as Intelligence B, this is the type of trait commonly measured by IQ tests. Isolating the purely biological elements of intelligence as Intelligence A demonstrates that intelligence does not have to be the politically loaded, culturally and economically insensitive term that it is now, that there is hope for refining our definition of intelligence as both a biological and cultural phenomenon. Only then can we fully acknowledge the potential for growth that our current paradigm ignores all too frequently.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Here are some works that I drew content from, other than those linked in the article:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Eysenck, H. J. (1988). The biological basis of intelligence. In S. H. Irvine &amp;amp; J. W. Berry (Eds.), Human abilities in cultural context (p. 87–104). Cambridge University Press. https://psycnet.apa.org/record/1988-98683-003&lt;/li&gt;
  &lt;li&gt;Wikipedia page on Intelligence Quotient: https://en.wikipedia.org/wiki/Intelligence_quotient&lt;/li&gt;
  &lt;li&gt;Oxford Dictionary definition of intelligence: https://www.lexico.com/definition/intelligence&lt;/li&gt;
  &lt;li&gt;Philip Vernon’s book on intelligence: https://trove.nla.gov.au/work/11823287?q&amp;amp;versionId=45253153&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further reading&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Definition of cultural intelligence from David C. Thomas: https://doi.org/10.1002/9781118785317.weom060051&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 15 Jan 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/cultural-intelligence/</link>
        <guid isPermaLink="true">http://localhost:4000/cultural-intelligence/</guid>
        
        <category>intelligence</category>
        
        
        <category>psychology</category>
        
        <category>essays</category>
        
        <category>science</category>
        
      </item>
    
      <item>
        <title>Mood and Productivity, Pt. 1: Does happiness live up to the hype?</title>
        <description>&lt;p&gt;Today, Vincent Van Gogh is indisputably recognized as a gifted creative. In his lifetime, however, he was unsuccessful by nearly every measure of the word: his art was never commercially appreciated, and many of his contemporaries considered him a madman. Suffering from severe depression for his whole life, van Gogh famously severed part of his own left ear, after an argument prompted him to start hearing voices. When he was thirty-seven years old, van Gogh walked out into a field and shot himself with a revolver.&lt;/p&gt;

&lt;p&gt;Van Gogh represents the classic ideal of the tortured artist. Alienated, misunderstood, yet ruthlessly innovative. Strangely enough, he is not alone in this predicament. Some of history’s greatest creatives were notably depressed, including Ernest Hemingway (&lt;em&gt;The Old Man and the Sea&lt;/em&gt;, &lt;em&gt;A Farewell to Arms&lt;/em&gt;), Sylvia Plath (&lt;em&gt;The Bell Jar&lt;/em&gt;), and George Orwell (&lt;em&gt;1984&lt;/em&gt;, &lt;em&gt;Animal Farm&lt;/em&gt;). It’s enough to make one wonder if the trope of the tortured genius is not the exception but in fact is the rule.&lt;/p&gt;

&lt;p&gt;Our increasing understanding of neuroscience and psychology gives us revolutionary insights into how our brains work. This knowledge allows us to explore the surprising links between mood and productivity. Used properly, this knowledge could enhance creativity and work output, all without compromising life satisfaction. In this series of articles, I will draw on history and science to address a potentially controversial series of questions relating mood and productivity:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Is there any truth to the trope of the tortured genius?&lt;/li&gt;
  &lt;li&gt;How can happiness be enhanced?&lt;/li&gt;
  &lt;li&gt;What does happiness do for productivity?&lt;/li&gt;
  &lt;li&gt;Is negativity good for anything?&lt;/li&gt;
  &lt;li&gt;How can &lt;em&gt;negativity&lt;/em&gt; be enhanced?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First off, let’s turn our attention to a hot topic of the moment: happiness. There is certainly great value to the pursuit of happiness, but neglecting the less desirable emotions may not be sustainable. Where should happiness be applied, and when does it hinder us more than it helps? Read on to find out.&lt;/p&gt;

&lt;h1 id=&quot;mood-and-productivity-part-one-what-is-happiness-good-for&quot;&gt;Mood and productivity, Part One: What is happiness good for?&lt;/h1&gt;

&lt;p&gt;In the modern world, there is no shortage of “positive psychology” books, videos, and courses promising to help people find happiness in life. All these sources take for granted that happiness is an ideal goal. Intuitively, it seems obvious: a happy life is a good life. And yet, a growing body of evidence demonstrates that happiness is just one facet in living a life of fulfilment. What follows is an attempt to question the assumptions of those who promise the “good life” through happiness alone by presenting a more nuanced and scientific perspective, along with a guide to the areas where happiness might be best applied and where other emotions are more effective.&lt;/p&gt;

&lt;h2 id=&quot;the-pragmatic-view&quot;&gt;The pragmatic view&lt;/h2&gt;

&lt;p&gt;To determine the usefulness of a positive mood in measurable terms, we need to somehow measure how “good at thinking” a person can be, then see if those metrics change for happy and sad people. How can we measure that? First, by looking at easily studied traits, like short-term memory, then by applying such specific knowledge to a broader concept like creativity.&lt;/p&gt;

&lt;p&gt;So how does happiness stack up on these metrics? Well, studies have demonstrated that there appears to be a link between positive mood and improved memory. Meanwhile, creativity is an area where being happier might not always improve performance.&lt;/p&gt;

&lt;h3 id=&quot;happiness-improves-short-term-memory&quot;&gt;Happiness improves short-term memory&lt;/h3&gt;

&lt;p&gt;Justin Storbeck, a psychology professor at the City University of New York, studies the link between emotion and cognition. In 2015, a study of his found that a positive mood increases &lt;em&gt;working memory capacity&lt;/em&gt;—that is, the more correct way of describing “short-term memory”—when compared to neutral and negative moods.&lt;/p&gt;

&lt;p&gt;The study considered both verbal and spatial memory. Undergrad students were first induced to have a certain mood: the happy group was shown a 5-minute clip from &lt;em&gt;Jerry Seinfeld: Stand up in New York&lt;/em&gt;, the negative group 5 minutes from &lt;em&gt;The Champ&lt;/em&gt; and the neutral group 5 minutes from &lt;em&gt;If Dolphins Could Talk&lt;/em&gt;. The participants were then instructed to complete a verbal task and an operation task. In the verbal task, participants were shown a list of 3, 5 or 7 words for 3 seconds each. Then—here’s the catch—the operation task interrupted their focus: after the last word was shown, participants were asked to analyze several math problems (e.g., “6/(3-2) = 0”) and indicate whether the equations were correct. Finally, they were asked to recall what words were presented in the verbal task, and in what order. The researchers then repeated the experiment, this time substituting the word-recall task for a task where participants memorize the locations of boxes presented in a grid.&lt;/p&gt;

&lt;p&gt;Earlier research seemed to indicate that positive moods were better for &lt;em&gt;verbal&lt;/em&gt; memory, whereas negativity enhances &lt;em&gt;spatial&lt;/em&gt; memory. In this study, however, the math problems served to help measure the persistence of memories through an unrelated task. This method revealed that a positive mood improved participants’ performance on both verbal and spatial memory, supporting the idea that happiness is good for remembering all kinds of things. Storbeck goes on to cite a possible neurological explanation: “positive mood increases dopamine, which is an important underlying biological mechanism for executive control and working memory.”&lt;/p&gt;

&lt;p&gt;While focused on a relatively narrow segment of the population (undergrad students in New York), this study’s implications for society at large could be enormous: if happy people truly think better than sad people, governments should consciously invest in the happiness of their citizens. Likewise, companies should recognize (as many already have) that a happy employee tends to be a productive employee. In this light, promoting positive psychology is not just a moral obligation but a pragmatic choice with quantifiable benefits.&lt;/p&gt;

&lt;p&gt;But that’s assuming that Storbeck’s findings with regard to working memory capacity hold true for all the other metrics of cognition. Is that really the case? Let’s take a closer look by examining another measure of brain function: creativity. In today’s high tech economy, innovation is crucial to economic growth. Creativity is a requirement for innovators, and the question for many ambitious individuals is how to nurture and sustain creativity. Specifically relevant to our current discussion is how mood, positive or negative, can be leveraged to increase creativity.&lt;/p&gt;

&lt;h3 id=&quot;happiness-can-improve-or-hurt-creativity&quot;&gt;Happiness can improve or hurt creativity&lt;/h3&gt;

&lt;p&gt;Creativity is often thrown around as a blanket description for a complex batch of useful brain functions. But what is it exactly? For now, let’s call it the production and execution of original ideas.&lt;/p&gt;

&lt;p&gt;Depression, obviously, is not a sustainable method of creativity. In fact, depression debilitates an individual, and we should not wish on anyone the fate of the tortured geniuses like Vincent Van Gogh and Sylvia Plath. Mild sadness, however, is a normal and healthy emotion that plays a vastly underappreciated role in our lives. In fact, research completed by professor Joseph P. Forgas at the University of New South Wales has determined that sadness can, among other things, &lt;a href=&quot;https://greatergood.berkeley.edu/article/item/four_ways_sadness_may_be_good_for_you&quot;&gt;improve (long-term) memory, improve judgement, and increase motivation&lt;/a&gt;. Happiness, meanwhile, caused the opposite to occur in these studies, diminishing the participants’ abilities to perform these critical cognitive functions.&lt;/p&gt;

&lt;p&gt;So maybe happier people aren’t necessarily smarter people. Barring the negative extreme of depression, a little bit of sadness can go a long way in improving performance at certain tasks. In other words, sadness can be an asset in pursuing the second part of creativity: the execution of ideas. But what about the production of original ideas?&lt;/p&gt;

&lt;p&gt;Here, evidence favors the happy. A 2008 meta-analysis of mood-creativity research revealed that happiness produces a measurable increase in a person’s creativity, whereas emotions like fear and anxiety are linked to &lt;em&gt;lower&lt;/em&gt; levels of creativity. Meanwhile, less active moods like relaxation and sadness demonstrated little correlation with creativity. In other words, being happy allows you to come up with more original ideas, which are the first requirement of creativity.&lt;/p&gt;

&lt;p&gt;It seems that happiness is good for producing ideas, while sadness can help us follow through with them. Both are critical stages of the creative process: Ideation is nothing without execution. Execution is worthless without ideation. In the same way, perhaps, happiness is nothing without sadness, and the full array of human emotion as well. Amidst the deafening hype of positive psychology, it’s often useful to slow down and remind ourselves that there are specific areas where happiness is appropriate, and others where sadness might serve us better. “Being happy” is just one element of the diverse source of fulfilment.&lt;/p&gt;

&lt;p&gt;Rather than using our increasing understanding of our minds as a weapon to promote an ideology, we can apply a rigorous approach of questioning, so that, even in this nuanced mass of information, we can eventually puzzle out a better way of life. When we do discover it—given the spectacular complexity of human psychology—we can probably rest assured that the secret to a good life doesn’t fit into a soundbite.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Here are some works that I drew content from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Vincent_van_Gogh&quot;&gt;Vincent Van Gogh’s Wikipedia page&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/25947579&quot;&gt;finding that happiness increases working memory capacity&lt;/a&gt; is presented by Storbeck and Maswood.&lt;/li&gt;
  &lt;li&gt;Baas, M., De Dreu, C. K. W., &amp;amp; Nijstad, B. A. (2008). A meta-analysis of 25 years of mood-creativity research: Hedonic tone, activation, or regulatory focus? Psychological Bulletin, 134(6), 779–806. &lt;a href=&quot;https://doi.org/10.1037/a0012815&quot;&gt;https://doi.org/10.1037/a0012815&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further reading&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Catherine Weismann-Arcache and Sylvie Tordjman have studied the relationship between depression and high intellectual potential in children. Their article can be found at &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3356869/&quot;&gt;nih.gov&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;This Fast Company article provides an executive summary of scientific results promoting the usefulness of negativity: &lt;a href=&quot;https://www.fastcompany.com/3038199/the-positive-results-of-being-negative&quot;&gt;https://www.fastcompany.com/3038199/the-positive-results-of-being-negative&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.toptenz.net/top-10-tortured-artists.php&quot;&gt;A top-ten list of some tortured artists&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Karuna Subramaniam and Sophia Vinogradov &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fnhum.2013.00452/full&quot;&gt;address links between happiness and improved cognition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 08 Jan 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/mood-and-productivity-one/</link>
        <guid isPermaLink="true">http://localhost:4000/mood-and-productivity-one/</guid>
        
        <category>happiness</category>
        
        
        <category>psychology</category>
        
        <category>essays</category>
        
        <category>science</category>
        
      </item>
    
      <item>
        <title>Version Management with a JavaScript Service Worker</title>
        <description>&lt;p&gt;The JavaScript service worker API is a way to give online web apps offline functionality. By running in the background of the browser, service workers are able to cache important files and give users a smooth offline experience.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-service-worker&quot;&gt;What is a service worker?&lt;/h2&gt;
&lt;p&gt;A service worker is a script (i.e., a JavaScript file) that the browser runs in the background. It can be used for features such as offline apps and push notifications. In the case of an offline app, service workers can be used to determine whether a new version of the app is available as an update.&lt;/p&gt;

&lt;p&gt;A tip for Chrome users: to see what service workers are currently running in your browser, open Developer Tools (F12), click “Application” on the top bar, and click “Service Workers” on the sidebar that comes up.&lt;/p&gt;

&lt;h2 id=&quot;how-a-service-worker-works&quot;&gt;How a service worker works&lt;/h2&gt;
&lt;p&gt;A service worker has three steps in its lifecycle:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Registration&lt;/li&gt;
  &lt;li&gt;Installation&lt;/li&gt;
  &lt;li&gt;Activation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once these three steps are finished, the service worker stays in the browser indefinitely, even if the user refreshes the page, making it useful for storing information in the cache to be used offline. Read more on the service worker lifecycle &lt;a href=&quot;https://developers.google.com/web/fundamentals/primers/service-workers/lifecycle&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a &lt;a href=&quot;https://developers.google.com/web/fundamentals/primers/service-workers&quot;&gt;guide from Google&lt;/a&gt; detailing the code you need to set up a service worker.&lt;/p&gt;

&lt;h2 id=&quot;using-service-workers-for-version-management&quot;&gt;Using service workers for version management&lt;/h2&gt;
&lt;p&gt;Each time a page is refreshed, the service worker file is checked for any changes. If even a byte has changed, a new service worker is loaded, ready to place the old one whenever the &lt;code class=&quot;highlighter-rouge&quot;&gt;skipWaiting&lt;/code&gt; event is triggered. We can take advantage of this fact by changing the service worker file in each new version of the app.&lt;/p&gt;

&lt;p&gt;Typically, the top of a service worker file will have something like the following variable declaration:
    const CACHE_NAME = ‘my-site-static-v1’;&lt;/p&gt;

&lt;p&gt;Each time you create a new version of the app, just update the service worker script, changing ‘v1’ to whatever the new version is (e.g., ‘v4.5.2’) and a new service worker will be created automatically.&lt;/p&gt;

&lt;p&gt;If you don’t want to do this manually, you can replace the text in your service worker’s JavaScript file automatically on build. The specific implementation of this differs depending on what system the app is using for build tasks.&lt;/p&gt;

&lt;p&gt;Note: if you happen to be using &lt;code class=&quot;highlighter-rouge&quot;&gt;grunt&lt;/code&gt; for build tasks, make sure to use &lt;code class=&quot;highlighter-rouge&quot;&gt;grunt-text-replace&lt;/code&gt; to replace the version number (&lt;a href=&quot;https://www.npmjs.com/package/grunt-text-replace&quot;&gt;installation instructions for grunt-text-replace&lt;/a&gt;), as &lt;code class=&quot;highlighter-rouge&quot;&gt;grunt-text-replace&lt;/code&gt; supports in-place overwriting, which means you can directly edit the source file for your service worker. All of the other text-replace modules I found for grunt only allow editing in the build files.&lt;/p&gt;

&lt;h3 id=&quot;fetching-the-new-service-workers&quot;&gt;Fetching the new service workers&lt;/h3&gt;
&lt;p&gt;Now that you have a new service worker for each new version of the app, the user should be prompted to fetch the new version. This involves triggering the &lt;code class=&quot;highlighter-rouge&quot;&gt;skipWaiting&lt;/code&gt; event on a service worker and reloading the page. A working implementation of this feature can be found &lt;a href=&quot;https://deanhume.com/displaying-a-new-version-available-progressive-web-app/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Service workers are not easy to deal with, but they can enable effective version management with offline web apps. This tutorial represents just one of the many ways version management can be implemented in an offline web app.&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Jan 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/service-workers-for-version-management/</link>
        <guid isPermaLink="true">http://localhost:4000/service-workers-for-version-management/</guid>
        
        <category>javascript</category>
        
        
        <category>programming</category>
        
        <category>tutorials</category>
        
      </item>
    
      <item>
        <title>Oatmeal and Emergence: How a whole can be greater than the sum of its parts</title>
        <description>&lt;p&gt;Oatmeal. At best, it is a nutritious and filling breakfast. At worst, a sloppy and unappetizing gruel. In short, oatmeal exhibits stunning diversity. However, amidst all these seemingly disparate varieties of oatmeal lies a surprising commonality, one that may leave you thinking about your breakfast long after the last bite.&lt;/p&gt;

&lt;p&gt;Imagine you got up early to prepare breakfast, and you’re cooking a bowl of oatmeal, as one does. Only today’s oatmeal is different—today, you heat up the water and get out the bag of oatmeal, to find that just one singular oat remains in the bag. Persisting in the face of adversity, you decide to cook the oatmeal anyways.&lt;/p&gt;

&lt;p&gt;As usual, you turn down the fire and let the oatmeal simmer until it reaches the desired consistency, that perfect level of softness and coherence that we all know and love. But it never does. Try as you might, a singular oat will never achieve that consistency, because the lonely flake has no companions to stick to.&lt;/p&gt;

&lt;p&gt;This brings to mind a scientific property called emergence: the idea that a whole can be greater than the sum of its parts. Truly great-tasting oatmeal reflects something more than just the taste of individual oats combined—it is the interaction between oats that creates oatmeal’s hallmark texture. The texture of oatmeal is what’s known as an emergent property.&lt;/p&gt;

&lt;p&gt;Breakfast foods are not the only things that demonstrate emergence.&lt;/p&gt;

&lt;p&gt;Water, for example, is very similar to cooked oatmeal in that water molecules like to stick to each other. Water’s intermolecular interactions give rise to emergent properties like cohesion, high specific heat, and evaporative cooling. Without emergence, we might not be alive today.&lt;/p&gt;

&lt;p&gt;Our brains also have emergent properties. Looking at an individual neuron, it is impossible to explain how thoughts might be formed. It’s only when we look at the brain as a system of neurons that we can study how these remarkable organs can process information, form memories, and read articles about oatmeal.&lt;/p&gt;

&lt;p&gt;The animal world is full of emergence. Ant colonies, along with those of other social insects (and perhaps even humans), are especially interesting in their collective intelligence. An individual ant/bee/human is relatively unintelligent. Put together, the colony of ants/bees/humans can form tunnel networks, scout for food, and even invent the Internet.&lt;/p&gt;

&lt;p&gt;But what’s so special about emergence? Why do we care?&lt;/p&gt;

&lt;p&gt;Emergence while mentioned in earlier texts, didn’t begin to take on serious scientific interest until the late 19th century. In the early days of science, reductionism, the idea that anything can be understood if we can break it down into its simpler components, was the prevailing mode of thought. If you want to know how a car works, take it apart and you’ll see the engine, battery, radiator, and so on, each with a specific function that contributes logically to the functioning of the car. Reductionism works for simple examples like motor vehicles, but breaks down when we start to analyze many natural systems.&lt;/p&gt;

&lt;p&gt;What if you want to know how the human brain works? Nothing in an individual neuron suggests the ability to think. Taking a brain apart doesn’t help us understand it. Here, we need to recognize that cognition is an emergent property—rather than analyzing brain cells, we can only consider the functioning of the brain as a whole.&lt;/p&gt;

&lt;p&gt;Some things can’t be explained as a combination of parts.&lt;/p&gt;

&lt;p&gt;Your morning oatmeal might be delicious, or disgusting, or one of the million options in between. Nevertheless, something magical happens every time oats (plural) are cooked. An unsuspecting individual, the singular oat, has potential for greatness, if only allowed to join a whole and form emergent properties.&lt;/p&gt;

&lt;p&gt;Behind a humble breakfast food lies a scientific principle essential to our understanding of the world. Simple as it may be, oatmeal is symbolic of the power of a whole to be greater than the sum of its parts, giving us humans (plural) a good reason to play with our food in the morning.&lt;/p&gt;
</description>
        <pubDate>Tue, 31 Dec 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/oatmeal-and-emergence/</link>
        <guid isPermaLink="true">http://localhost:4000/oatmeal-and-emergence/</guid>
        
        <category>emergence</category>
        
        
        <category>food</category>
        
        <category>essays</category>
        
        <category>science</category>
        
      </item>
    
      <item>
        <title>How to Download an Image as a PDF using jsPDF</title>
        <description>&lt;p&gt;PDF documents are great because they preserve the format of a page. In other words, the content in a PDF document typically does not get distorted, pixelated, stretched, or compressed. Because of this, an image, once it’s been converted to a PDF, will typically preserve its dimensions and clarity. Making sure an image always looks sharp and focused is almost always a concern of designers, developers, and even just regular old people. When making a site that deals with images, developers may increasingly consider implementing “Save as PDF” as an attractive option for exporting an image. That’s why I decided to document some of the successes and struggles I encountered while working on this feature.&lt;/p&gt;

&lt;p&gt;My goal was to add a “Save as PDF” option to an image processing web app. The dependency I chose to use was &lt;a href=&quot;https://www.npmjs.com/package/jspdf&quot;&gt;jspdf&lt;/a&gt;, which is a free library that lets you use Javascript to generate PDFs of all kinds. Because the library is not designed specifically for converting images to PDF documents, the main struggle I faced was creating a PDF document with dimensions that matched those of the image, so I could avoid those distasteful white margins and unintentionally cropped images that would result from using a standard-sized page. Here’s the way I set up jsPDF to make sure images could be smoothly downloaded as PDFs:&lt;/p&gt;

&lt;h1 id=&quot;set-up-jspdf&quot;&gt;Set up jsPDF&lt;/h1&gt;
&lt;p&gt;Follow the instructions from &lt;a href=&quot;https://www.npmjs.com/package/jspdf&quot;&gt;npm&lt;/a&gt;. The easiest way to use jsPDF in a web-based project is to add a script tag linking to the cloud:
        &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/jspdf/1.5.1/jspdf.debug.js&quot; integrity=&quot;sha384-THVO/sM0mFD9h7dfSndI6TS0PgAGavwKvB5hAxRRvc0o9cPLohB0wb/PTA7LdUHs&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h1 id=&quot;convert-the-image-to-a-data-uri&quot;&gt;Convert the image to a data URI&lt;/h1&gt;
&lt;p&gt;For now, jsPDF can only add an image to a PDF if the image is in &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_URI_scheme&quot;&gt;data URI format&lt;/a&gt;. For the web app I was working on, the image I wanted to download already had a data URI as its source. In other words, it looked something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;  &amp;lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAA
  ANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4
  //8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU
  5ErkJggg==&quot; alt=&quot;Red dot&quot; /&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(Example taken from &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_URI_scheme#HTML&quot;&gt;Wikipedia&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;This example would show up as a red dot: &lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAA ANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4 //8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU 5ErkJggg==&quot; alt=&quot;Red dot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If your image is not already a data URI, it’s relatively easy to convert it. First, put the image on a canvas. Next, call canvas.toDataURL() to get the data URI of the image. You can find a great implementation of this method &lt;a href=&quot;https://davidwalsh.name/convert-image-data-uri-javascript&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;generate-the-pdf-based-on-the-data-uri&quot;&gt;Generate the PDF based on the data URI&lt;/h1&gt;
&lt;p&gt;The key to downloading an image as a PDF is to get the right image dimensions, to make sure the PDF page dimensions match those of the image.
In my case, the image dimensions were conveniently stored in order to facilitate image processing, but in a more general case you could get the image dimensions using &lt;code class=&quot;highlighter-rouge&quot;&gt;image.naturalHeight&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;image.naturalWidth&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;// Download the given image URL as a PDF file.
function savePDF(imageDataURL) {
  // Get the dimensions of the image.
  var image = new Image();

  image.onload = function() {
    let pageWidth = image.naturalWidth;
    let pageHeight = image.naturalHeight;

    // Create a new PDF with the same dimensions as the image.
    const pdf = new jsPDF({
      orientation: pageHeight &amp;gt; pageWidth ? &quot;portrait&quot;: &quot;landscape&quot;,
      unit: &quot;px&quot;,
      format: [pageHeight, pageWidth]
    });

    // Add the image to the PDF with dimensions equal to the internal dimensions of the page.
    pdf.addImage(imageDataURL, 0, 0, pdf.internal.pageSize.getWidth(), pdf.internal.pageSize.getHeight());

    // Save the PDF with the filename specified here:
    pdf.save(&quot;index.pdf&quot;);
  }

  image.src = imageDataURL;
} Note: naturalWidth and naturalHeight are only supported in modern browsers. [These are the browsers that support these properties.](https://caniuse.com/#feat=img-naturalwidth-naturalheight)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And that’s how to use Javascript to give users the option to download an image as a PDF. To summarize:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Set up jsPDF.&lt;/li&gt;
  &lt;li&gt;Make sure the image you want to download is in data URI form.&lt;/li&gt;
  &lt;li&gt;Use jsPDF to add the image to a new PDF document.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;a-footnote-data-url-vs-data-uri&quot;&gt;A footnote: Data URL vs. Data URI&lt;/h4&gt;
&lt;p&gt;The form of image data dealt with in this tutorial can be called both a data URL and a data URI. This is because URLs (Uniform Resource Locators) are a subset of URIs (Uniform Resource Identifiers). As to whether HTML data URIs can be considered URLs,&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Dec 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/how-to-download-an-image-as-a-pdf-using-jsPDF/</link>
        <guid isPermaLink="true">http://localhost:4000/how-to-download-an-image-as-a-pdf-using-jsPDF/</guid>
        
        <category>javascript</category>
        
        
        <category>programming</category>
        
        <category>tutorials</category>
        
      </item>
    
      <item>
        <title>How to Scan a Barcode with Flutter</title>
        <description>&lt;p&gt;Here’s how to set up a simple barcode scanning function, starting from a basic flutter app. In this tutorial we’ll be using the &lt;a href=&quot;https://pub.dev/packages/barcode_scan&quot;&gt;barcode_scan Flutter package&lt;/a&gt;. The package site has its own tutorial, but the tutorial has some outdated information, so I thought I’d try and correct that here.&lt;/p&gt;

&lt;h1 id=&quot;set-up-the-configuration-files&quot;&gt;Set up the configuration files&lt;/h1&gt;
&lt;p&gt;As listed on &lt;a href=&quot;https://pub.dev/packages/barcode_scan&quot;&gt;barcode_scan’s about page&lt;/a&gt; on Dart Pub, start by making the following changes:&lt;/p&gt;

&lt;h2 id=&quot;for-android&quot;&gt;For Android:&lt;/h2&gt;

&lt;p&gt;Add this line to your AndroidManifest.xml to request permission to use the user’s camera:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    &amp;lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add this line (to allow the BarcodeScanner activity) to your AndroidManifest.xml. Do NOT modify the name.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    &amp;lt;activity android:name=&quot;com.apptreesoftware.barcodescan.BarcodeScannerActivity&quot;/&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Install the Kotlin plugin for Android Studio or your default editor&lt;/p&gt;

&lt;p&gt;Edit your project-level build.gradle file to look like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;      buildscript {
          ext.kotlin_version = '1.2.31'
          ...
          dependencies {
              ...
              classpath &quot;org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version&quot;
          }
      }
      ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Edit your app-level build.gradle file to look like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    apply plugin: 'kotlin-android'
    ...
    dependencies {
        implementation &quot;org.jetbrains.kotlin:kotlin-stdlib-jre7:$kotlin_version&quot;
        ...
    }
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In your pubspec.yaml file, add &lt;code class=&quot;highlighter-rouge&quot;&gt;barcode_scan: ^1.0.0&lt;/code&gt; to the list of dependencies, as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    dependencies:
      flutter:
        sdk: flutter
      # other dependencies...
      barcode_scan: ^1.0.0

Note: for some reason many tutorials, including that of the publishers themselves, say to add `barcode_scan: ^0.0.3` to your pubspec.yaml file. This tends to cause problems because 0.0.3 is an unsupported version of this dependency.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Click “Packages get” in Android Studio or run flutter packages get in your project folder.&lt;/p&gt;

&lt;h2 id=&quot;for-ios&quot;&gt;For iOS:&lt;/h2&gt;
&lt;p&gt;To use on iOS, you must add the the camera usage description to your Info.plist&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    &amp;lt;key&amp;gt;NSCameraUsageDescription&amp;lt;/key&amp;gt;
    &amp;lt;string&amp;gt;Camera permission is required for barcode scanning.&amp;lt;/string&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;write-the-testing-code&quot;&gt;Write the testing code&lt;/h1&gt;
&lt;p&gt;The package barcode_scan should now be successfully installed and configured in your flutter application. Now all that remains is to use the package in your app!&lt;/p&gt;

&lt;p&gt;Outside of the formatting, the key to this example is the call to &lt;code class=&quot;highlighter-rouge&quot;&gt;BarcodeScanner.scan()&lt;/code&gt;, which uses the imported package &lt;code class=&quot;highlighter-rouge&quot;&gt;barcode_scan&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;completed-code-in-maindart&quot;&gt;Completed code in main.dart&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import 'dart:async';
import 'package:barcode_scan/barcode_scan.dart';
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';

void main() {
  runApp(new MyApp());
}

class MyApp extends StatefulWidget {
  @override
  _MyAppState createState() =&amp;gt; new _MyAppState();
}

class _MyAppState extends State&amp;lt;MyApp&amp;gt; {
  String barcode = &quot;&quot;;

  @override
  initState() {
    super.initState();
  }

  @override
  Widget build(BuildContext context) {
    return new MaterialApp(
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: Scaffold(
          appBar: AppBar(
            title: Text('Barcode Scanner'),
          ),
          body: Center(
            child: Column(
              children: &amp;lt;Widget&amp;gt;[
                Padding(
                  padding: const EdgeInsets.all(8.0),
                  child: Text(&quot;Press the button to scan an item:&quot;),
                ),
                Container(
                  child: RaisedButton(onPressed: scan, child: Text(&quot;Scan&quot;)),
                  padding: const EdgeInsets.all(8.0),
                ),
                Padding(
                  padding: const EdgeInsets.all(8.0),
                  child: Text(barcode),
                ),
              ],
            ),
          )),
    );
  }

  //scan a barcode, store result in this.barcode
  Future scan() async {
    try {
      //use barcode_scan to scan the barcode
      String barcode = await BarcodeScanner.scan();

      //if the barcode has been obtained, display it
      if (barcode.length &amp;gt; 0)
        setState(
            () =&amp;gt; this.barcode = &quot;The barcode you scanned was: &quot; + barcode);
    } on PlatformException catch (e) {
      if (e.code == BarcodeScanner.CameraAccessDenied) {
        setState(() {
          this.barcode = 'Camera permission was not granted';
        });
      } else {
        setState(() =&amp;gt; this.barcode = 'Unknown error: $e');
      }
    } on FormatException {
      //the user probably just pressed the back button, no need to print an error message
      setState(() =&amp;gt; this.barcode = '');
    } catch (e) {
      setState(() =&amp;gt; this.barcode = 'Unknown error: $e');
    }
  }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Tue, 26 Nov 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/how-to-scan-a-barcode-in-flutter/</link>
        <guid isPermaLink="true">http://localhost:4000/how-to-scan-a-barcode-in-flutter/</guid>
        
        <category>flutter</category>
        
        <category>dart</category>
        
        
        <category>programming</category>
        
        <category>tutorials</category>
        
      </item>
    
  </channel>
</rss>
