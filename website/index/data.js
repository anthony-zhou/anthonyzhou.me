const posts = [{"id":"air-quality-report","layout":"post","title":"A dream to save the air - public reflection","date":"25 Jun 2023","description":"Reflecting on our project to implement community-level air quality monitoring","image":"/assets/images/2023/06/air_pollution.jpeg","author":"anthony","featured":true,"categories":["climate","report"],"tags":["air","pollution","sensors"],"content":"\nI spent last summer working to found a startup with the goal of improving air quality. By August 2022, we decided to shut down. But why? Perhaps the best reason is that our other opportunities were more exciting. However, even that answer begs the question of why this air quality project wasn’t as exciting in the first place. \n\nTo make the story short, I would cite two reasons for failure. First, sensor networks would require a lot of public funds, and those funds could be used more effectively. Instead of pursuing local sensors, survey-based and remote-sensing based approaches would likely be sufficient. More importantly, we found that low-cost sensors are not quite precise enough yet. To build our sensor network, we would need to perform hardware research to design better devices. \n\nThat said, we learned a lot. For people interested in climate tech, this account conveys insights and stories which might inspire future work. More generally, I believe there is much to gain from stories of failure, and that failures often contain the glimmers of future opportunity. \n\n## What we wanted to do\n\nAir quality can vary by as much as 8x from one end of a block to another. These localized regions of high pollution stick around, so people living in these areas get harmed more and more as they get continually exposed to the pollutants. But then the entire city of San Francisco only has a few regulatory-grade air quality monitors, and the problem is much worse elsewhere. So city officials often don’t even know about these pollution hotspots. \n\nWhy do pollutants even matter? The answer depends on the kind of pollutant. The most concerning pollutant seems to be PM2.5 – particulate matter smaller than 2.5 micrometers – because it tends to get lodged in your lungs and cause breathing problems. Other pollutants, like ozone and carbon monoxide, make it harder for people to breathe. Like middle school bullies, these pollutants adapt into new forms when they meet, which become dangerous in different ways. Just as an overgrown physical bully joins forces with a self-hating wannabe insult comic, nitrogen dioxide and volatile organic compounds combine to form ozone and particulate matter. Meanwhile, many of these pollutants – along with methane and carbon dioxide – are also greenhouse gasses, which means they help trap heat in the atmosphere and move us ever closer towards becoming a jungle planet. \n\nIf we care about improving air quality, the first step is to diagnose the problem and figure out what’s causing it. To this end, we wanted to create a dense network of air quality sensors, such that we could collect block-level data on air pollution. With this data, city officials could locally target their air quality interventions and measure their effectiveness. \n\n## What we thought would happen\n\nOur plan was to begin by understanding the problem from the people we thought we were helping. So we scheduled dozens of meetings with officials from groups like CARB (the California Air Resource Board) and BAAQMD (Bay Area Air Quality Management District) to discuss whether they needed higher-resolution data on air quality measurements. The straight answer was “no.” But this reflected a concern with feasibility more than a true denial. Several people argued that low-cost sensors would never be precise enough to produce actionable insights for policy. \n\nDespite this resistance, we pushed on. After all, government agencies are notoriously slow to adopt new technologies. But might we first find early adopters in industry? We identified a few other potential customers: utility, weather prediction, and insurance companies. In utilities, we found out that Pacific Gas and Electric employs a workforce of several hundred people walking on foot with sensors to detect potential gas leaks in their pipelines. And in fact, about two years prior, another aspiring air quality enthusiast had been in late-stage talks to start a pilot project with PG&E making just such a network of low-cost air quality sensors, focused on methane. \n\nThe other area we sought to understand was climate intelligence, including weather forecasting and climate risk modeling. In many cases where we think weather apps are wrong — for example, they tell you it's raining when it's not really raining — the forecast is actually just not localized enough. Maybe it’s actually raining two miles away from you. Hyperlocal weather stations help bridge this gap by providing local data. Our air quality sensors could serve this purpose by providing temperature, humidity, and wind speed data to weather models. \n\nDespite the lack of interest from air quality management districts, we hoped to find eager markets in both utilities and climate intelligence.\n\n## What actually happened\n\nAfter identifying some potential customers, we started asking them about their problems. PG&E did indeed have this big problem with gas leaks. While we thought that the company would want to avoid gas leaks due to the loss of revenue, this loss is actually miniscule — gas companies freely burn extra natural gas at their drilling sites to lower the pressure in their pipes. Instead, their main concern is public health. In 2018, the San Bruno pipeline (owned by PG&E) exploded and killed 8 people. The explosion cost the company $1.5 billion in fines and about the same amount in insurance claims. For reference, the company’s annual budget is around $20 billion. With slim margins, an event like this marks a significant setback on company profits. \n\nLow-cost air quality sensors could be a great solution for PG&E. They would provide continuous monitoring at the site of residential gas leaks, which is better than what that hundred-strong workforce of walking sensors could achieve. The problem, they told us, was that existing hardware simply wasn’t good enough. Cheap sensors aren’t precise enough to detect gas leaks. \n\nIn order to solve PG&E’s methane problems, we would need a much better device than any of the low-cost methane sensors currently available. So the business need is real, but the next step will require investments into better hardware. \n\nSo utility companies didn’t seem like a good first customer. What about weather forecasting? The key question here was whether local sensor networks could improve forecasting. \n\nIn the past few decades, weather forecasts have been improving by about a day each decade. In other words, if in 2000 we could predict 8 days in advance with 90% accuracy, by 2010 we could predict 9 days in advance with 90% accuracy. However, we are projected to lose a day of prediction with every 3 degrees in global warming — so our weather models need to improve if we are to preserve our predictive power. \n\nOne of the main ways we could improve our weather forecasting is by incorporating deep learning into forecasting models. The Dark Sky founders did this a few years ago, and their results were good enough that they got acquired by Apple and incorporated into Apple weather forecasts. But deep learning thrives on good input data, so improving deep learning forecasts also requires that we improve the quality and quantity of underlying data that we collect — this is precisely the gap filled by local sensor networks. \n\nLocal sensor networks are not new. Weather Underground (aka Wunderground) got its start by helping people set up a network of weather stations. In the end, however, they ended up only making money from their commercially available forecasts, which relied minimally upon local sensor networks. Just like PG&E, they saw these low-cost sensors as too unreliable to be useful. In fact, this is how most weather forecasting companies have made money historically — they take free national weather service forecasts and offer them with ads or a subscription plan, with their main value-add being a more user-friendly AI. \n  \nThe experience of Wunderground might prompt some to be skeptical about supplementing government weather stations with cheaper community-based stations. Indeed, community-based weather stations might never be a good enough data source, but more spatial coverage is likely needed. The European weather forecasting center (ECMWF) complains that it doesn’t have enough sensor coverage in Africa to predict when storms are coming from that direction. More importantly, the lack of weather stations in developing countries limits their forecast accuracy, making it harder for truck drivers to plan their routes and for farmers to plan their crop care practices. \n\n## What's next\n\nAt the end of the summer, we decided to stop working on the air quality sensor network, and return all funds to those who had pre-ordered our devices. It was a difficult decision. On one hand, we’d discovered some real problems, with real potential for social impact. On the other hand, we had no real customers, and no one wanted low-cost sensor data until we could prove its accuracy. In the end, we agreed unanimously that shutting down was the right thing to do. \n\nThat said, poor air quality is still a problem. It’s killing millions of people a year. Even if official numbers seem healthy, local communities often suffer from persistent pollution hotspots. And there are clear next steps for our communities. When it comes to improving the air, the first step is diagnosing the problem – we need better sensors, better models, and better coverage. At the same time, we need to actually reduce emissions, primarily by reducing the burning of fossil fuels, but also by managing construction sites and the built environment. These actions bring us ever closer to a world where everyone – on every block – gets to breathe clean air. \n","preview":"\nI spent last summer working to found a startup with the goal of improving air quality. By August 2022, we decided to shut down. But...","isoDate":"2023-06-26T00:00:00.000Z"},{"id":"on-logical-consistency","layout":"post","title":"On Logical Consistency","date":"15 Nov 2022","description":"Exploring the implications of logical consistency for moral and practical goals.","image":"/assets/images/2022/11/rowboat.png","author":"anthony","featured":true,"categories":["ethics","philosophy"],"tags":["empiricism","rationality","decision-making"],"content":"\nLet’s say you believe that the best way to write a computer program quickly is to avoid early optimization. One day, you notice yourself tinkering with a for loop for hours to optimize its runtime, before having finished the method you’re writing. Fortunately, you’re a rational person. As a result, you decide that this behavior is inconsistent with your belief in avoiding early optimization, so you finish writing the method before optimizing the for loop further. \n\nNow suppose you believe that animal suffering is important. On one hand, you support laws forbidding animal cruelty. On the other hand, you have not acted to support laws against factory farming, which arguably places animals in even worse conditions than animal cruelty does. Clearly, this behavior is inconsistent with your belief that animal suffering is important.\n\nAt first glance, these two scenarios look similar. Given that you start with a belief, the rational thing to do is to act in consistency with that belief. In practice, however, these cases are quite different. The programming scenario deals with a question of reality: on average, do you achieve a high-quality program more quickly when you optimize early or when you don’t? The animal suffering scenario deals with a question of morality: is it right to value animal suffering? \n\nQuestions of reality and morality are actually very different — claims about reality are falsifiable, whereas claims about morality are taken on faith. Demanding full consistency with a moral belief is equivalent to putting full confidence in that belief. \n\nWithin moral philosophy, we’re often faced with conclusions that go against what we intuitively think is right. Consider, for example, the repugnant conclusion, or the supposed utopia in which we all live in pleasure machines. You might think that these scenarios are necessary for the sake of moral consistency. In fact, though, once we realize that moral consistency presupposes full moral confidence, it becomes less clear that a rational person should support these scenarios. \n\nThis begs the question: is moral consistency rational? Surprisingly, I think the answer is no, because it presumes a seemingly irrational degree of confidence in moral beliefs. Given the lack of objective evidence for moral beliefs, the perspective most consistent with reality is actually to never have complete faith in a moral belief. Thus, in the sense of being consistent with reality, moral consistency may in fact be irrational. \n\nMaybe this all sounds reasonable in theory. But what should we do in practice, if we place little to no confidence in each moral belief? Do we not have to act as if we hold certain morals to be true, even if at an intellectual level we don’t believe them? \n\nNot exactly. In a world where we are uncertain about most morals, we can at least optimize for things that are instrumentally valuable in almost all moral frameworks. For example, most people believe it is good to survive. Money, physical fitness, and healthy relationships are all important for this goal. So even someone who is uncertain about their morals should accept wealth, fitness, and relationships as instrumental objectives.\n\nBut a life lived solely at the corporeal level might seem empty. Is there some way we could make this amoral life more fulfilling? Maybe we should add “life satisfaction” as another basic objective. \n\nTo address this metric, we can start by doing things that are roughly good. People generally report feeling more satisfied after doing good things, whether giving gifts or serving food at a soup kitchen. Note, however, that maximally good acts do not increase life satisfaction more than roughly good acts do. Beyond just diminishing marginal returns, the effect may actually go in the opposite direction (see research by Barry Schwartz on satisficers and maximizers).\n\nSwitching to this frame of mind — doing things that are roughly good — should not drastically change the behavior of altruists in general. For example, you might still think that it seems roughly good to stop factory farming. But you would no longer do so for the sake of moral consistency, using a moral law generalized from your behavior towards people who abuse their dogs. Instead, you would suffice to say that ending factory farming is roughly good. You might even argue further that it is typically (but not always) good to reduce suffering for sentient beings. \n\nRefusing moral consistency is useful in two contexts: first, it gives us reason to reject beliefs that are internally consistent and intuitively repugnant. Second, it frees us to consider the full realm of moral possibility. \n\n## Alternative maximizing frameworks\n\nBeyond simply maximizing human welfare, we can consider other metrics to optimize for, adopting these frames of mind when the context demands it. \n\nOne compelling strategy is to maximize knowledge — often called “seeking the truth.” Within this framework, scientific progress is clearly valuable. In addition, technology entrepreneurship can be seen as an extension of experimental science, because technology involves changing our reality and learning from the results. \n\nRoles that are only instrumentally valuable in this framework include most legal, financial, and service sector jobs. They help us preserve the stable society that leads to increases in knowledge. In the same way, education is also instrumentally valuable, though in a more direct way, because it enables more people to reach the edge of our collective knowledge and push its bounds.\n\nAnother strategy is to maximize accordance with nature. In this framework, we should allow ecological-looking systems — like markets — to decide what’s right. Thus there are two kinds of moral actions: either you can work to free the markets from regulation and bad actors — as the libertarians often do — or you can work to maximize your personal wealth, because wealth is a sign that you have delivered value to the market. (See Principles by Ray Dalio for further explanation). If you also subscribe to cultural evolution, then you should believe that the cultures that currently exist are in fact natural and therefore good. \n\nYet another strategy is to maximize people’s ability to uniquely appreciate the universe. According to this perspective, we have a moral imperative to try things that other people haven’t before. In fact, this framework seems like a special case of the knowledge-maximizing strategy, which gives more credence to art, music, and other experiences that we don’t typically consider “knowledge.” \n\nOne final strategy is dataism, which seeks to maximize our collective capacity to process information. This approach differs slightly from the knowledge-seeking mindset because information processing does not imply an increase in knowledge. An information-processing agent could just as well use information purely for action, rather than accumulating knowledge. (See Homo Deus by Yuval Noah Harari for further explanation). \n\nWe can easily see how certain professions adopt certain of these frameworks, so that they can see their day-to-day work as serving a broader moral purpose. For example, scientific researchers are more likely than the average person to adopt a knowledge-maximizing perspective. Each of these moral frameworks has its benefits and drawbacks, so they are best used in moderation. \n\n**Footnote: Definitions of Rationality**\n\nWe often draw a distinction between epistemic and instrumental rationality. Epistemic rationality is defined as updating beliefs in accordance with reality, whereas instrumental rationality is defined as acting in accordance with our existing goals. \n\nMy proposal here is to split instrumental rationality into two types, depending on the type of goal you are pursuing. I argue that it is rational to act in accordance with measurable goals (e.g., complete a computer program by the end of the day, or start a billion-dollar company), whereas it is not necessarily rational or irrational to act in accordance with moral goals (e.g. maximize human welfare). This conclusion seems counterintuitive, and that is precisely why pseudo-rational justifications for moral beliefs sound so appealing (often of the form “it is obvious that…” or “any rational person would agree that…”). ","preview":"\nLet’s say you believe that the best way to write a computer program quickly is to avoid early optimization. One day, you notice yourself tinkering...","isoDate":"2022-11-16T00:00:00.000Z"},{"id":"time-vs-attention","layout":"post","title":"Managing time vs. managing attention","date":"15 Aug 2022","description":"Difference between managing time and managing attention.","image":"/assets/images/2022/08/clock.jpg","author":"anthony","categories":["productivity"],"tags":["time","focus","attention","decision-making"],"content":"\nWe often hear the refrain: “I don’t have time for this!” In reality, what this often means is: “I don’t have enough attention for this.” For people whose schedules are flexible — like students — the latter case is more often true. That is, time is not our limiting factor; attention is.\n\nSo we come to reframe certain decisions. For example, consider the decision of driving to work versus walking to work. Both activities occupy your time, but only one — driving — occupies your attention. A more extreme case is taking walks for the sake of walking. You would think that taking time away from work would decrease your productive output, but these walks can actually increase your output, by replenishing your ability to focus.\n\nMany people recognize this truth. That’s why we take breaks even when we have lots of work to do. What is less obvious, though, is the flip side of this premise. If you believe that attention is the limiting factor, then you would start prioritizing tasks based on the attention they demand, rather than the time commitment involved. For example, washing the dishes demands time, but not attention. Planning a social event might not take lots of time, but it tends to take more attention — for example, you might have to respond to text messages every few hours leading up to the event.\n\nYou can often trade attention for time, and vice versa. For example, optimizing your dishwashing routine (plates first, then forks? Using a bowl to store soap water?) will probably save you time, but cost more attention. Running an inefficient computer program saves you from spending attention to optimize it, but takes more time. Being able to trade attention for time prompts a new kind of decision: when should you make the trade?\n\nOne way to make this trade is by optimizing for learning. Attentional activities tend to teach you more than mindless ones do. However, if we assume that attention gets used up over time, then it makes sense to maximize attention on things we care about learning. For me, this often involves writing or programming, but any attentional task should suffice.\n\nPerhaps, instead of asking if we have time for something, we should be asking if we have attention.","preview":"\nWe often hear the refrain: “I don’t have time for this!” In reality, what this often means is: “I don’t have enough attention for this.”...","isoDate":"2022-08-16T00:00:00.000Z"},{"id":"consensus","layout":"post","title":"Understanding blockchain from a distributed computing perspective","date":"15 Jul 2022","description":"A foray into the history of distributed computing and how it relates to blockchain technology.","image":"/assets/images/2022/07/network_earth.jpg","author":"anthony","categories":["programming","technology"],"tags":["blockchain","distributed-computing","cryptography"],"content":"\nRunning programs on a single computer is great, but what if you want to run a program on multiple computers? For example, Amazon’s database is never going to fit on one computer’s hard drive, and nor will any one computer serve all Amazon user’s web requests — so you end up scaling to a system involving multiple computers. That’s when you’ll need distributed computing. The difficulty with distributed computing is that communication is hard — computers fail, networks fail, and some computers may even be malicious. \n\nHere’s an example: let’s say we run a website that sells office supplies. Alice wants to buy a stapler that costs $3 — but she only has $3 in her gift card balance. Let’s say our website is pretty popular, so we decide to serve its content from 2 separate servers. We decide to randomly split web traffic between the two servers.\n\nImagine Alice, while being served by Server A, requests to buy the stapler, paying via gift card. On its own copy of the database, Server A changes Alice’s gift card balance to $0 and begins shipping the stapler. Then, Server A sends a message to Server B telling it to make these same updates. But before the message reaches Server B, Alice has already made the same request to Server B, which has also shipped Alice a stapler. Using the same $3 gift card balance, Alice was able to buy two $3 staplers!\n\nThis is known as the double-spend problem, and it’s just one of a broader class of consensus problems. In the context of distributed computing, consensus refers to the process by which a group of machines agree upon a shared state (e.g., Alice’s gift card balance) and execute operations on that state (e.g., subtract $3 from Alice’s gift card balance). \n\n## History of Consensus\n\nThe first attempts at designing consensus protocols for distributed systems began in the 1980s.  In 1989, Leslie Lamport introduced an algorithm called [Paxos](https://en.wikipedia.org/wiki/Paxos_(computer_science)), based on earlier work, which was one of the first rigorously proved fault-tolerant protocols. What does it mean to be fault tolerant? In this case, Lamport proved that Paxos is guaranteed to be *safe* (that is, all non-faulty nodes are guaranteed to agree on the same value [1]) as long as the number of nodes is greater than *2f+1*, where *f* represents the number of faulty nodes. In other words, as long as there are more functioning nodes than faulty ones, the functioning nodes will agree on the same value. \n\nPaxos was pretty cool, because it meant that system administrators could provably keep their distributed systems safe — with one caveat. In its original form, Paxos relies on the assumption that failures only occur via crash and omission. In other words, it doesn’t tolerate *Byzantine* faults. \n\nWhat is a Byzantine fault? The term comes from Lamport’s Byzantine General Problem, in which you imagine a group of generals, some of whom may be traitors, that has to reach a common decision — for example, whether to attack or retreat, and at what time. The Byzantine generals problem inspired the concept of a *Byzantine Fault*, in which some nodes on a network might be malicious. *Byzantine Fault Tolerance* (BFT) describes a system’s ability to work properly despite having malicious nodes. While it may sound like technical jargon, the term BFT is relevant because it’s the only type of network that works under all of the realistic risks we stated earlier — where computers fail, networks fail, and some computers may even be malicious. \n\nPerhaps the earliest significant protocol for Byzantine Fault Tolerant Consensus was [Practical Byzantine Fault Tolerance](https://pmg.csail.mit.edu/papers/osdi99.pdf) pBFT), published in 1999. Newer BFT protocols include Honey Badger BFT and Tendermint. \n\nPaxos was great for cloud data centers, because it helped them ensure consistency across nodes (in fact, Google, Amazon, Microsoft, etc. all use Paxos in production environments). pBFT was great for data center applications that required byzantine fault tolerance, as well as permissioned blockchains like Hyperledger Fabric. \n\nBut none of these protocols on their own are sufficient for securing distributed systems outside of enterprise clouds — in other words, systems where anyone can spin up a node and join the network. \n\nHere’s why. \n\n## Getting from distributed computing to blockchain\n\nLamport and his contemporaries were largely focused on a concept called *permissioned consensus* — achieving consensus under the assumption that all participating network nodes have some permission to participate. The permissioned assumption allows nodes to achieve consensus via a simple majority vote, because they can assume that a fixed and known number of nodes are participating in the network — this property generally holds true on systems like enterprise clouds and Hyperledger [2]. \n\nBut what if you don’t know how many nodes are on the network? What if anyone can join or leave the network at any time without permission (in a *permissionless* consensus model)? If so, you can no longer rely on a majority vote, because any bad actor could just flood the network with fake  nodes and vote for its own preferences (this is called a *Sybil Attack*). \n\nBut there is a way to make sure you can’t add an arbitrary number of voters, if you could just make it hard to add nodes to the system. If only there was a way to make sure each new node had to put in real work to get a vote…\n\nThat, in essence, is what Bitcoin did — it solved the permissionless consensus problem in a BFT context (which is the only practical context for a crypto-type application). In Bitcoin’s consensus model, only nodes who submit valid proof-of-work get to make decisions. \n\nWith Bitcoin, it doesn’t matter how many additional nodes you add. As long as their computing power is far smaller than that of the rest of the network, you’ll never be able to rig the network’s decision. \n\nBitcoin’s security relies on one key insight: **cryptographic protocols allow you to achieve consensus without having control over the hardware**. This allows large groups of computers to cooperate on previously impossible tasks, without any one entity controlling all the hardware [3]. \n\nSeen this way, a permissioned consensus system is really just a consensus system that lacks a cryptographic permission protocol — because it assumes hardware-level control. \n\n## Moving on from Bitcoin\n\nIn fact, though, you can replace the “permission” in a permissioned consensus model with any cryptographic proof protocol — Bitcoin was just the start.\n\nEthereum 2.0 uses proof-of-stake as permission. Solana uses proof-of-stake [4]. Helium uses proof-of-coverage. The key here is that to prevent Sybil attacks, all of these proofs must be hard to fake. \n\nAre these systems Byzantine fault tolerant? Yes. In fact, it seems like **permissionless consensus protocols are Byzantine fault tolerant by default**, because they make no assumptions about the validity of other nodes in the network. However, the converse is not always true. BFT networks are not always permissionless, because they may derive part of their security from assuming permission has been granted. \n\nIn theory, designing a new blockchain is as simple as combining a consensus protocol with a permission protocol. For example, Bitcoin relies on combining the longest chain rule for consensus (choose the longest chain of transaction hashes) with proof-of-work. Solana uses proof-of-history and proof-of-stake (in a more traditional BFT-style system) for consensus and proof-of-stake alone for permission. HoneyBadger BFT assumes permission is established and creates its own protocol for consensus. Each of these combinations comes with tradeoffs. A good way to think about these tradeoffs comes from the [Decentralized Thoughts Blog](https://decentralizedthoughts.github.io/2022-03-03-blockchain-resource-pools-and-a-cap-esque-impossibility-result/):\n\n> **Theorem:** No blockchain protocol:\n> \n> 1. operates in the unsized setting;\n> 2. is adaptively live in the synchronous setting; and\n> 3. satisfies probabilistic finality in the partially synchronous setting.\n\nHere, *unsized* refers to the environment where the permissions are allocated — for example, Bitcoin is unsized, because anyone can add on PoW and participate, whereas proof-of-stake (PoS) blockchains are sized, because they derive stakes from a fixed pool of currency. As described in the blog, Bitcoin only satisfies (1) and (2), whereas PoS blockchains only satisfy (2) and (3). Note that “adaptive liveness” simply describes the system’s ability to adapt to shifts in the balance of the permissions pool, e.g. a 100x change in computing power for Bitcoin. Because this property is pretty essential, blockchains generally assume (2) and choose either (1) or (3).\n\nWhile Helium’s proof-of-coverage might seem like an unsized pool, it is actually sized, because nodes providing proof-of-coverage are observable via the blockchain state and managed via Miner scores — the pool of coverage-certified miners is updated at every epoch. \n\nThe cool thing about sized permission protocols is that they can use any existing BFT consensus protocol that assumes a permissioned model, substituting hardware-level permission for its own permission protocol. \n\nTo sum up, the key questions to ask when designing a blockchain are: what is the permission protocol, and what is the consensus protocol? \n\nFor the permission protocol, it’s important to know: \n\n1. Is this protocol sized (like proof-of-stake) or unsized (like proof-of-work)? \n    1. In general, sized protocols can confirm transactions with less assumptions about network timing, with the downside that they require some friction to join (e.g., waiting for an epoch to start). \n    2. Sized protocols can be easily combined with traditional BFT consensus models. \n2. Is the protocol hard to fake? \n    1. Being hard-to-fake is necessary to prevent Sybil attacks\n\nFor the consensus protocol, it’s important to know:\n\n1. What is this protocol’s throughput?\n2. What trade-offs does the network make to achieve this throughput? \n\n## What’s next\n\nSince the 1980s, distributed systems researchers have published reams of papers on consensus protocols. But it wasn’t until Bitcoin’s 2009 debut that we saw a real-world application of a cryptographic permission protocol. \n\nBecause of their 30-year head start, consensus protocols were relatively well-developed by 2009. Meanwhile, permission protocols like proof-of-work, proof-of-stake, and proof-of-coverage are still in their infancy (or maybe adolescence). \n\nBut these new permission protocols have also enabled distributed systems of unprecedented scale, which are not always well served by existing consensus protocols. In the next few years, we’ll probably make lots of progress in both areas: developing new permission protocols and scaling consensus protocols to ensure high throughput across thousands or millions of machines.  \n\n[1] “Safety” is a bit of a misnomer — it should probably be called “consistency,” because saying all non-faulty nodes will agree on the same value is the same as saying all non-faulty nodes are consistent with each other.\n\n[2] The “permissioned” in permissioned blockchain refers to *data permissioning*, which allows operators to designate different levels of data access for the nodes on a network. For the rest of this article, I’m using “permissioned” to describe *consensus permissioning*, which tells you whether nodes need hardware-level permission to participate in network consensus.\n\n[3] It’s not guaranteed that these permission protocols always allow you to achieve consensus, nor is it true that these protocols are necessarily cryptographic. Even Bitcoin, for example, is vulnerable to computing power Sybil attacks. I’m not sure if any permission protocol is provably resistant to this type of attack, but in practice most of them make the attacks very difficult to execute. It’s also fun to think about what alternative permission protocols exist other than cryptographic ones. For example, you might consider using a retinal scan or other biometrics. \n\n[4] Solana actually relies on a combination of proof-of-stake and proof-of-history. Proof-of-stake is more relevant to the permission protocol while proof-of-history is important for block ordering. \n\n*Thanks to Ryan Cao and Lance Tan for reading versions of this post.*","preview":"\nRunning programs on a single computer is great, but what if you want to run a program on multiple computers? For example, Amazon’s database is...","isoDate":"2022-07-16T00:00:00.000Z"},{"id":"airports","layout":"post","title":"Long Walks at the Airport","date":"19 Jan 2022","description":"What access patterns can tell us about airport design","image":"/assets/images/2022/01/airports/airport.jpg","author":"anthony","categories":["optimization"],"tags":["travel"],"content":"\nTerminal B of the Salt Lake City airport is unique among the airport terminals I’ve been to. Perhaps as an homage to Utah’s love of nature, this destination features a mile-long scenic hike from the gate to the airport exit. And no, it’s not optional.\n\nBut it’s not just the Salt Lake City airport that does this. Which leads us to a broader question: why do we spend so much time walking to the gate at airports?\n\nThe answer to this question lies in access patterns. In software design, access patterns are the queries people make on a given set of data. For a social media app, the most frequent access patterns might include the following:\n\n- Given a user, fetch a list of their followers\n- Given a user, fetch a list of their posts\n- Given a search query (”Jane”), fetch a list of users matching the query\n- Given a user’s contacts, determine which ones are already using the app\n\nFor software developers, access patterns are important for designing data models and backend infrastructure. Following this train of thought, I started wondering: what are the access patterns determining the design of the Salt Lake City (SLC) airport?\n\n## SLC Airport - Access Patterns\n\nHere’s what I came up with initially:\n\n- Passengers arriving\n- Passengers departing\n\nPretty straightforward so far. But then I realized there were more access patterns, with each  representing different stakeholders:\n\n- Planes arriving\n- Planes departing\n- Cars picking up passengers\n- Cars dropping off passengers\n- Cars parking at the airport\n- Planes parking at the airport\n\nWhile there might be other access patterns for an airport, these seem to be the most frequent. Given these access patterns, the next logical question was: what access patterns is this airport optimized for?\n\n## SLC Airport - Design\n\nAs I learned firsthand, passengers departing from terminal B have to make the same mile-long trek as when they arrived. Notwithstanding the benefits of 20 minutes of light-intensity exercise, this airport is clearly not optimized for passengers.\n\nIf not passengers, who is the airport designed for? Planes, apparently. The SLC airfield follows a big H shape, which works quite nicely for plane taxiing and parking. The shape of the H maximizes the number of runways while making it easy to service each, and therefore is optimized for planes taking off and landing. Here’s a map from the [SLC airport website](https://slcairport.com/maps/airport-map/):\n\n![Screen Shot 2022-01-20 at 10.34.33 PM.png](/assets/images/2022/01/airports/Screen_Shot_2022-01-20_at_10.34.33_PM.png)\n\nThe bottom half of the H contains a large looping road, allowing easy access for cars picking up, dropping off, and parking at the airport. \n\nJust like that, we’ve optimized the airport for all of our access patterns! Except, of course, for passengers walking to their terminal. Here’s what the walkable portion of the airport looks like:\n\n![The walkable portion of the SLC airport: a neat little sideways H nestled inside the larger H of the airfield. Aesthetic but ultimately suboptimal. ](/assets/images/2022/01/airports/Screen_Shot_2022-01-20_at_10.18.21_PM.png)\n\nThe walkable portion of the SLC airport: a neat little sideways H nestled inside the larger H of the airfield. Aesthetic but ultimately suboptimal. \n\nHere’s where the access patterns truly come into conflict: \n\nFor the planes, the airport is most useful when the surface area of the two concourses is maximized (via the sideways H structure), because that’s what maximizes plane parking. Meanwhile, cars arriving at the airport are channeled into a centralized entry point (labelled “Terminal” on the map), which is relatively convenient. \n\nSo planes and cars are satisfied. However, thanks to these constraints, pedestrians must enter from a single entry point and traverse the full surface-area-maximizing H shape — including the half-mile tunnel between Concourses A and B — in order to reach their desired destination, which is Concourse B (aka Terminal B).\n\nThe Salt Lake City airport is a case study in how, when access patterns for different stakeholders come into conflict, not everyone wins. As in pickup frisbee games, the winners stay and the losers walk. But are there any airports that manage to solve this conflict successfully?\n\nCase in point: the DFW International Airport. \n\n## DFW Airport - Design\n\n![The DFW International Airport: a masterpiece of optimized access patterns. Roads are highlighted in orange.](/assets/images/2022/01/airports/Screen_Shot_2022-01-20_at_10.31.07_PM.png)\n\nThe DFW International Airport: a masterpiece of optimized access patterns. Roads are highlighted in orange.\n\nAs with the SLC Airport, the DFW Airport features an H-shaped airfield that maximizes runway space. However, the airport building itself is designed instead as a set of independent semicircles. When cars arrive at the airport, they can drive into the loop of a semicircle (say, Terminal C) and pick up or drop off passengers directly at a point on its inner edge. Once passengers pass through the security checkpoint, they’ve basically arrived at their gate. \n\nSo passengers and cars are both satisfied. What about planes? Thanks to its semicircular subunits, the airport provides optimal external surface area for planes to dock at each gate. Just as in the SLC airport, planes receive plenty of space for parking at the gate. \n\nBut what if passengers want to travel between gates? While this is a bit of an edge case, it’s certainly a common one. For example, some passengers might need to transfer between flights. Fortunately, these passengers can take the Skylink, which is a tram that connects all of the terminals. \n\nOne additional bonus of the DFW Airport’s structure is its scalability. If the airport so decides, they could easily add any number of new terminals to the north or south of its existing terminals without affecting access times for passengers, cars, or planes. \n\nMeanwhile, Salt Lake City would have to add another half-mile tunnel. \n\nAs the DFW Airport demonstrates, walking at the airport is not an inevitability. Whether due to space constraints, budget constraints, or simply close-mindedness, airports often make life difficult for passengers walking to and from their gates, especially when said passengers are running late. But it doesn’t have to be that way. \n\nPerhaps, by accounting for all the important access patterns, we’ll finally be able to just visit the airport and board the plane, without the mandatory half-hour cardio session on the way to the gate.\n","preview":"\nTerminal B of the Salt Lake City airport is unique among the airport terminals I’ve been to. Perhaps as an homage to Utah’s love of...","isoDate":"2022-01-20T00:00:00.000Z"},{"id":"abstraction","layout":"post","title":"Abstraction and Problem Solving","date":"7 Jan 2022","description":"When and why traversing layers of abstraction is useful. ","image":"/assets/images/2022/01/blossoms-4836548_1920.jpg","author":"anthony","categories":["problem-solving","productivity"],"content":"\nHere's a contention that you may or may not agree with:\n\n> The ability to solve problems is synonymous with the ability to create and apply abstractions. \n\nIs this statement true? Assuming we care about solving problems, the answer to this question has important implications for what we should spend our time on. \n\nOne way to approach this question is to look at how abstraction is applied in the real world. To begin with, rarely is a problem solved that is truly unique. Instead, most problems are simply derived from existing problems, combined and arranged in a new way.\n\nLet's look at programming, as an example. When it comes to writing an effective program, there are two kinds of abstraction at play. One kind is *functional abstraction*, in which blocks of code are wrapped in modules or functions that hide their implementation details from high-level usage. The other variety is *conceptual abstraction*, which is the way the programmer generalizes from a specific problem to a more general problem that has already been solved, then implements the abstraction with slight modifications to fit the particular situation. \n\nNow consider the design of a computer. This seemingly complex operation is in fact a collection of simpler subproblems: at the most basic level, you might need a module for memory, a module for arithmetic, and a module for reading and writing data. These basic components can then be composed at higher levels of abstraction to form rudimentary programs, which culminate in the advanced software that run on computers today. \n\nExamining the design of a computer from this sort of first-principles, top-down approach is an act of functional abstraction: starting from the highest level, you identify at each stage what the desired inputs and outputs of the machine are, then determine the subfunctions that will produce the outputs given the inputs. \n\nThe difference between various technological solutions (software, hardware, etc.) is simply the maximum and minimum level of functional abstraction at which you can take higher or lower level interfaces as a given. \n\nAt this point, functional abstraction has left us with a long list of simple subproblems, and the way these small modules can be connected. Once it comes to actually implementing these modules, however, functional abstraction is not useful -- this is where conceptual abstraction comes in. Implementing the solution to the problem involves taking these small modules and implementing them one by one, drawing on knowledge of existing solutions. Applying existing solutions to new problems requires a conceptual understanding of the solutions themselves. By the end of this process -- decomposition via functional abstraction and recomposition via conceptual abstraction -- we end up with a finished solution to our problem. \n\nBut why is it useful to draw this distinction between functional and conceptual abstraction? Are top-down and bottom-up design approaches not sufficient to characterize this approach to problem-solving? I think the answer is no -- top-down and bottom-up describe the observed behavior of efficient problem solvers (the \"what\"), but not the underlying skills that enable this problem solving (the \"how\" and \"why\"). Also, recognizing both approaches as instances of abstraction acknowledges how both rely on the same fundamental skill: generalizing information without reducing its usefulness. \n\nUnderstanding the \"how\" and \"why\" (that is, the underlying concepts) of problem solving in a field like computer engineering or software allows us to generalize this approach to other areas of life (this is yet another example of conceptual abstraction!).\n\nAt first glance, literature is diametrically opposed to computer science: where computer science is formulaic and measurable, literature feels more artistic and subjective. In fact, though, both fields are exercises in creating and applying abstractions. Authors use functional abstraction when they combine smaller character traits and symbols into broader themes, and conceptual abstraction when they reutilize thematic and plot devices from prior works in their own compositions. Perhaps more importantly, conceptual abstraction is the very reason why readers often resonate so deeply with literature. At some level abstracted away from the concrete details of the story, we find ourselves reapplying the themes of great literature in the context of our own lives. \n\nGiven the importance of abstraction across so many disciplines, strengthening our ability to reason with abstractions seems like it should be an important priority. How does our education system deal with this? As you might guess, not well. Most academic material is focused on the details of specific systems (cells, economies, societies, etc.). Rarely, if ever, does material cross subject lines, when in fact many of the mental models useful in, say, biology, might actually be applied quite usefully to economics. \n\nIn other words, learning is generally over-categorized in a formulaic way. This is great for churning out workers who can follow a prearranged formula, but not good for solving new kinds of problems or sharing knowledge across industries. Rather than categorizing by subject, we should categorize knowledge by mental model, with the flexibility to reapply different mental models over the same knowledge.\n\nMuch of this mental-model work is done by our brains automatically. Moreover, some disciplines -- especially among subfields of engineering -- encourage this type of thinking automatically, at least for problems within the industry. However, recognizing the importance of abstraction allows us to specifically develop this skill and further enhance human potential. \n\nIn other words, my working answer to the central question of this essay -- whether abstraction is the key skill underlying problem-solving -- is yes. Being able to reason with different levels of abstraction is indeed crucial for solving any significant problem, and this skill merits further recognition and practice. \n","preview":"\nHere's a contention that you may or may not agree with:\n\n> The ability to solve problems is synonymous with the ability to create and apply...","isoDate":"2022-01-08T00:00:00.000Z"},{"id":"thoughts-on-futarchy","layout":"post","title":"Thoughts on Futarchy","date":"12 Dec 2021","description":"A new system of governance","image":"/assets/images/2021/12/market.jpg","author":"anthony","categories":["politics","economics"],"tags":["prediction","voting"],"content":"\nDemocracy, the way we know it, has a lot of problems. Many Americans are now reporting record levels of discontent with the government, in a pattern echoed across other democratic countries globally. This is because, although we theoretically elect our representatives by popular support, policies often fail to represent the best interests of the population. \n\nDespite its shortcomings, we have learned to accept democracy as our main form of government, with the argument that the alternative -- autocracy -- is much worse. Indeed, autocracy easily devolves into corruption and instability, and an environment that is less than ideal for enhancing human welfare. But what if there's another alternative -- some way to modify democracy that maintains its strengths while improving on its weaknesses?\n\nIn fact, even the founders of American democracy (now the model for most democracies globally) recognized that pure democracy would not be effective. That's how we ended up with elected representatives -- the idea is that most people know little to nothing about the implications of this or that policy, so they should not be entrusted with those decisions. Instead, a small, educated elite (i.e. Congress) will make informed decisions on behalf of their constituents, in a way that benefits everyone. As a further enhancement, Congress will receive input from experts (often industry professionals, lobbyists, and academics), who provide important context for making policy in a given sector.\n\nClearly, though, this system doesn't really work as intended. Representatives, once elected, often start acting selfishly. Even if not, they tend to capitulate to powerful corporate lobbying arms, and make policy decisions that favor the powerful at the expense of their actual constituents.\n\nEven if their solution was imperfect, the problem identified by the Founding Fathers is a real one: if not everyone is equally capable of making beneficial decisions, then who should be entitled to define policies? In other words, how can we make a compromise between the values of pure democracy and the differential ability of experts to make good decisions?\n\nOne potential answer to this problem is futarchy: a system of government in which elected officials define and manage certain measures of welfare, and market speculators decide which policies are implemented. Robin Hanson, in his 2000 paper introducing the idea, puts it succinctly: \"vote on values, but bet on beliefs.\" In his [futarchy manifesto](https://mason.gmu.edu/~rhanson/futarchy.html), he defines one simple rule for this system of government: \n\n> When a betting market clearly estimates that a proposed policy would increase expected national welfare, that proposal becomes law. \n\nThe idea behind prediction markets is that, when people are forced to put their own money behind their decisions, they tend to become more serious, and experts with a higher likelihood of predicting correctly should tend to invest more heavily in support of their decisions. Meanwhile, individuals with limited knowledge would either not participate or quickly lose the money that they put in. \n\nPrediction markets, though not without problems, are generally more accurate than opinion polls, and, because they have yet to be deployed for large-scale decision-making, there are probably quite a few undiscovered design optimizations that could address current shortcomings -- an area that merits further research but is certainly promising. \n\nHowever, one key uncertainty is how you measure success. Let's say that elected representatives decide to optimize for GDP growth. Even if we assume that GDP growth is unambiguously good, how do we determine what effect a specific policy decision (say, implementing a tax incentive for electric vehicles) has on the GDP of the country as a whole? The central problem of evaluating whether policies worked, with or without prediction markets, is that we never have a true counterfactual -- we never would have known, had the policy not been enacted, what would have happened to the country. Furthermore, it is difficult (if not impossible) to disentangle the effect of an individual policy from all the other policies made concurrently. \n\nWithout a clear connection between policy decisions and welfare impact, prediction markets will have no basis for awarding either side's predictions -- even five or ten years after implementing an electric vehicle incentive, how will we know whether it truly improved GDP?\n\nThis seems to me like the central difficulty with futarchy -- the difficulty of measuring policy outcomes. One solution I could see would be to have these markets also define intermediate outcomes. For example, a tax incentive would likely boost sales of electric vehicles, sales of electric vehicles would likely lead to  decreased carbon emissions, decreased traditional car sales, and increased economic growth in adjacent industries like battery technology and electricity generation. Meanwhile, each of these downstream effects would have its own downstream effects, getting more and more granular until you can directly estimate the number of dollars a given change will contribute to GDP. \n\nIf you had some mechanism (ideally market-based) to successfully identify the most important downstream effects of each decision, then design prediction markets around each of those interactions, you would end up with a system that could efficiently determine the effect of a decision on an aggregated value like GDP. \n\nAssuming these sorts of systems can be implemented, futarchy looks like a promising improvement to democracy, one in which we can not only give voice to our personal values but also benefit from the knowledge of experts in their chosen domains. \n\nBut the implementation is precisely the hard part -- it's easy to see plenty of ways this system could go completely wrong, and there are many unresolved uncertainties. American-style democracy may be imperfect, but it has proven itself better than many proposed alternatives. As a result, the most logical first step is to design a small-scale test of the system, perhaps at the scale of a local government, as a way of seeing whether futarchy makes any sense whatsoever. \n\n## Further Reading\n\n- [Robin Hanson's original manifesto](http://mason.gmu.edu/~rhanson/futarchy.html)\n- [Wikipedia on Prediction Markets](https://en.wikipedia.org/wiki/Prediction_market)\n- [Ethereum blog article introducing futarchy](https://blog.ethereum.org/2014/08/21/introduction-futarchy/)\n","preview":"\nDemocracy, the way we know it, has a lot of problems. Many Americans are now reporting record levels of discontent with the government, in a...","isoDate":"2021-12-13T00:00:00.000Z"},{"id":"service-oriented-software","layout":"post","title":"Writing reliable software in a service-oriented architecture","date":"24 Mar 2021","description":"Building software reliably and quickly in a service-oriented world.","image":"/assets/images/2021/3/avocado.jpg","author":"anthony","categories":["programming","tutorials"],"tags":["startups","software architecture","business","microservices"],"content":"\nIn the process of starting a startup -- and writing code to make it work -- I've read a lot of advice. Thanks to the internet, we can now access everything from Medium articles introducing unit testing in JavaScript to full-length lectures about microservices at Netflix, for free! Some of the advice is great, some of it is outright wrong, and all of it broadened my perspective, but most of it was just irrelevant to a person in my position.\n\nAt a startup, the rapid pace of growth demands that every team member learns quickly, including the software engineers. As a result, in the whirlwind of the past year, I've encountered countless new companies, technologies, and ideas. More importantly, I've learned a few lessons about prioritization, in an environment where there's never enough time or money and always too much to do. \n\nWhile I don't hope to provide all the answers to building technology at a startup, I would like to share some of the lessons I've learned in the past year. Most importantly, I hope to spark a conversation on the best way to approach software at an early-stage startup, and on taking that software to scale. \n\nThe best measure of a great new technology is that it becomes boring quickly. Like, duh, who *wouldn't* use a countertop radiation chamber to heat their Hot Pockets faster! That's how we know the microwave was successful.\n\nTo get to the point of being boring, technology has to be reliable enough that people can take it for granted and make it a part of their daily lives. When it comes to software, reliability doesn't just improve the user experience -- with the right principles in mind, writing reliable code speeds up the development process. This way, we can spend less time fixing bugs, and more time creating awesome new experiences for our users.\n\n# Setting the scene\n\n## Monoliths and Waterfall Development\n\nWriting software used to be a lot like baking a sponge cake. You could combine all the ingredients into a gooey batter -- printer drivers, font families, image processing tools, and so on -- and bake the cake in one fell swoop. Once it was done, you'd have a finished software product -- say, Microsoft Word 1.0.\n\nA sponge cake is a <ins>monolith</ins>: a single, coherent piece of software that contains many sets of features.\n\nThe idea of baking the cake all at once is called <ins>waterfall development</ins>: where a team spends a long time (usually months to years) creating one big product release, with lots of new features all at once. After the first version of the software is released, the team starts working on v2.0, which is so different from v1.0 that it's basically a whole new cake.\n\nLike most (reasonable) people, I enjoy eating cake. But software is not the same as food (despite the deceptively named Android Jelly Bean operating system). \n\nThe difference? Software is consumed as it's made. These days, a Facebook engineer (let's call her Amanda) can add a new set of emoji reactions on Wednesday morning, and users will be using them to respond to stock market memes on Facebook by Wednesday afternoon. \n\n## Service-oriented architecture\n\nAs a whole, software companies no longer practice the cake-baking, waterfall model of software development. Modern internet software is more like guacamole: a combination of disparate features -- called <ins>services</ins> -- that are loosely linked to form a coherent user experience. For example, Spotify has one service for playlists, one service for search suggestions, and thousands of others handling every part of the user experience and behind-the-scenes logic. \n\nThe difference between cake and guac is huge: while old software took months to bake, modern software is ready to go right away. When Amanda adds a sticker pack to Facebook Messenger, regular people can start using it almost immediately. (Another difference between cake and guac is that only one of them goes well with quesadillas, but that's beside the point).\n\nA software composed of services is using <ins>service-oriented architecture</ins>.\n\nWith this model, teams iterate quickly, so they can continuously deliver new features to customers and get feedback on those changes -- this process of rapid iteration is the foundation of <ins>Agile software development</ins>, an alternative to the waterfall model.\n\n## Where we are now\n\nNearly every notable software company now uses the Agile development process. Meanwhile, service-oriented architecture is still relatively new. Much like how Takeru Kobayashi popularized the method of separating buns from sausages to dominate hot-dog eating competitions, companies like Spotify and Netflix are just starting to demonstrate the potential of this approach. \n\nAnd it makes sense. Let's say you want to add a new feature to your product. In our food analogy, a feature is like an ingredient: is it easier to mix tomatoes into your guac or mix tomatoes into your sponge cake? In the same way, it's generally much easier to add new features to a service-oriented architecture, when each piece of the app acts independently of the others. Otherwise, you'll end up with some very soggy cake. \n\nService-oriented architecture also allows the developer to choose which services to implement in-house, and which ones to borrow from third-party providers. As a company, outsourcing standard functions like error reporting and customer management systems to other companies lets us focus on what truly makes us different. \n\nIf they're so great, why isn't everyone using service-oriented architectures already? Do software companies just love serving soggy cake?\n\nThere's one major barrier to adoption for service-oriented architectures: it's hard to make the services interact correctly with each other. Having more services means having more connections and more moving parts that could break. \n\nFortunately, though, the food analogy comes in handy again: if we want to make sure the ingredients of our guacamole go well together, each and every time, we'll need a <ins>recipe</ins>. \n\nOne recipe for reliable service-oriented development, coming right up!\n\n# Goals\n\nIn this guide, we're looking to optimize the following metrics:\n- Development speed\n\nOh, looks like that's it! First, here's a quick definition:\n> <ins>Development speed</ins> is the rate at which a company develops valuable features for customers. \n\nNot every company cares about moving fast. But reliability and speed do not stand at odds with each other -- in fact, as we'll see in a second, reliable code is often easier to revise and improve quickly. \n\n# Avoiding Technical Debt\n\nSo we want to optimize development speed. Great! \n\nAt the beginning, it's really exhilirating to develop lots of new features really fast. Move fast and break things, right? But if you completely disregard code quality, your software will likely start to look like this:\n\n![Number of Bugs over time](../assets/images/2021/3/bugs_over_time.png)\n\nWhy? Because, the more complicated a software is, the more components it has that could go wrong. If your program is deeply coupled, that is, if every module depends on every other module, you end up exponentially increasing the chance of something going wrong at the connections between the parts. \n\n![Connections and nodes](../assets/images/2021/3/connections_and_nodes.png)\n\nMathematically speaking, if we let the number of modules equal *n*, the number of connections in the system is equal to the sum of the integers from 1 to n-1. For example, for a piece of software with 5 modules, the total number of connections is 1 + 2 + 3 + 4 = 10 connections.\n\nAnother way to get the same result is to evalute n(n-1)/2. As an example, for n=5, (5 * 4) / 2 = 10 connections. In other words, the number of connections depends on the **square** of the number of modules -- if the number of connections is proportional to the number of things that can go wrong, then the potential bugginess of the program increases quadratically with the number of modules!\n\nEven if not every piece of your software is connected to all the others, this math demonstrates how the number of connections in a program scales **superlinearly** with the number of modules it has. This sucks, because it means your time will start to look like this:\n\n![Connections and nodes](../assets/images/2021/3/bugs_vs_features.png)\n\nAnyone familiar with <ins>technical debt</ins> has likely seen these graphs before. But what does this mean in a service-oriented context? How do we still optimize developer time by making our software sustainable, reliable, and maintainable?\n\n# A 3-Tier Model \n\nOur strategy for developing reliable software requires 3 components:\n\n1. Testing\n2. Automated error tracking\n3. Customer support\n  \nEach level gets closer to the customer, and gets accordingly less scalable (testing can preemptively solve a lot of problems, whereas solving a customer support case might only address one).\n\nWhile these levels are not unique to the service-oriented world, I will focus here on what they mean in the context of services.\n\nWe'll tackle these components one by one, then see how they fit together to create reliable production software.\n\n## 1. Testing\n\nRemember this picture from earlier?\n\n![Connections and nodes](../assets/images/2021/3/connections_and_nodes.png)\n\nWith a service-oriented architecture, this kind of complexity is inevitable. Each individual service -- each node on the graph -- is very simple. The complexity lies in the connections. As we saw earlier, the more complex something is, the more likely it is to break!\n\nAs a result, with service-oriented architectures, it's most important to test the <ins>connections</ins> between the services, rather than the services themselves. Of course, it wouldn't hurt to have tests verifying that every little unit of code works, but remember that our developers don't have all day to write pretty-looking test cases -- we need time to ship working code to customers, and to enjoy some of the amazing free snacks stocked in the office snack bar.\n\nTesting to optimize developer time in a service-oriented architecture means **focusing on integration tests** that cover your app's use cases -- for Facebook, this would include actions like signing up for an account, sending a message on Messenger, or commenting on a friend's post. \n\nPrioritize testing for the features that are likely to break and are critical to your user experiences, or for the bugs that you're not likely to notice immediately. That last bit feels a bit counterintuitive at first, so here's an example: if your login page is broken, you'll probably notice right away, but it might take weeks before you discover a bug with the forgot password feature -- because no one remembered to manually test it! That's a place where automated testing would have really helped.\n\nIn fact, it might be useful to cover your entire app's functionality with end-to-end tests, so you can at least be sure that your major features work properly as you ship code to production. \n\nOne caveat: testing does slow down your development process at first. Aside from writing the tests themselves, you'll likely be allocating separate resources (databases, API endpoints, etc.) in a staging environment for testing, to mimic the production environment. In the long run, though, separating development, staging, and production environments is a useful habit, since it makes sure that nothing you do locally breaks the production software. \n\nCompared to integration tests, unit tests for services are a lot more painful, since you have so many services to mock out. However, it can speed up debugging (and even development) time to start with unit tests for the especially complicated pieces of code, and to move towards full unit test coverage later on. \n\nMaking automated integration tests part of a continuous integration (CI) pipeline will probably save you a lot of time and stress.\n\n## 2. Automated error tracking\n\nAssuming your testing didn't catch 100% of your errors (which it never seems to do), you'll have some sneaky bugs leak into production.\n\nThis is an especially common problem with client-side websites, because people use 2 gazillion different browsers with varying levels of support for new features, and we all know that one person who's still using Internet Explorer.\n\nAnother cause of production errors is when -- shockingly -- a user does something differently than you expected. One example would be a text box that breaks when someone types an emoji in. However good your tests are, there will always be cases you didn't account for. \n\nOur goal is usually to notice problems before the users do, which means setting up an error tracking software to automatically report when errors occur in production.\n\nWith these two main causes in mind (browser compatibility and edge cases), our production error tracking software has two simple requirements: it needs to report the user's browser, device, and (ideally) identity, and it needs to enrich errors with custom context -- like, for example, the text you were typing into that emoji-unfriendly textbox when it suddenly decided to break. These requirements are the same regardless of whether your app is a nutrition tracker or Instagram for dogs. \n\nWhen a feature is not unique to your app's main purpose, you might consider using an external service. Bug tracking platforms like Sentry and Bugsnag are getting better every day, and are relatively simple to set up.\n\nOne caveat: bug tracking softwares can be pricey, so you'll have to weigh the benefits of production bug detection with the material cost of the service. Compared to integration testing, bug reporting in a service-oriented architecture is more important at a larger scale, where there are many more edge cases and strange browsers to contend with. \n\nUltimately, an effective bug-tracking strategy lets you find and fix production errors before your users even notice -- and before they start complaining. Just as important, it saves you time debugging customer bug reports, because you already know the context of the issue at hand. But nothing ever works perfectly, and that's when people complain:\n\n## 3. Customer support\n\nWorking closely with customers is critical for improving the usability of your software and creating customer loyalty. At the same time, though, addressing customer complaints can become a serious drag on productivity. Sometimes customer bug reports are useful and reproducible; other times, they only exist in a narrow edge case that 0 other people in your user base are experiencing, and it's not worth spending 99% of your time fixing a problem that only 1% of your users are facing. \n\nThe hardest part of customer support is that you don't know which reports are actually useful. Still, though, it's important to at least read customer bug reports, because they serve as a last-resort method for catching the bugs you missed in testing and error reporting. \n\nIn a service-oriented world, the dynamics of customer service don't change much, but the way communication is implemented does. Services like Intercom can help you maximize your customer support efficiency, by directing common questions to a custom knowledge base and providing a streamlined interface for responding to customer questions and comments.\n\n# Wrapping Up\n\nIn my experience, testing, bug tracking, and customer support (with regards to bug reports, not product or business-related feedback) should generally be prioritized in that order. Because each level is less scalable, setting the right priorities lets developers focus on writing integration tests that make sure the software works reliably at scale.\n\nIn a service-oriented environment, bug tracking and customer support can be outsourced effectively to third-party services, freeing developers to work on the features that make your business stand out. Of course, this article only covers reliability, which is  one of the many unique challenges and opportunities we face when building services for production software. Because it's written from the perspective of a startup founder, my advice focuses on small to mid-scale operations, with none of the complexity of giants like Netflix or Spotify. \n\nReliable service-oriented software is more than possible, if we can just perfect the recipe.\n","preview":"\nIn the process of starting a startup -- and writing code to make it work -- I've read a lot of advice. Thanks to the...","isoDate":"2021-03-24T04:00:00.000Z"},{"id":"the-third-bowl","layout":"post","title":"The Third Bowl: Building Online Communities Centered Around Connection","date":"19 Jan 2021","description":"The loneliness problem, Social Networking 2.0, and the promise of better digital connection.","image":"/assets/images/2021/1/alsopeople.jpg","author":"anthony","categories":["psychology","business","society"],"tags":["loneliness","connection","social networks"],"content":"\nObesity, smoking, and alcohol are the typical targets of a United States Surgeon General. Dr. Vivek Murthy, however, realized that, outside of these well-publicized dangers, many of the problems he saw in practice stemmed not from any physical ailment but from the lack of human connection--from loneliness. In his time as Surgeon General of the United States, he helped bring loneliness to the forefront of the global medical conversation, popularizing the statistic that loneliness can be [\"as damaging to health as smoking 15 cigarettes a day.\"](https://www.hrsa.gov/enews/past-issues/2019/january-17/loneliness-epidemic)\n\nLast year, Dr. Murthy published a book about the topic: [*Together: the Healing Power of Human Connection in a Sometimes Lonely World*](https://www.vivekmurthy.com/together-book). The book covers everything from the Men's Shed movement to screen time, but one of his particularly striking ideas is the Third-Bowl Society.\n\nWhat is a Third-Bowl society? Murthy asks us to imagine collectivist cultures (such as those prominent in East Asia) as a deep and narrow bowl. Everyone is packed tightly together, with community support built into the system, but those who don't fit into the rigid expectations of such a society feel constricted. Individualist cultures, like the United States right now, are like a shallow and wide bowl: each individual has plenty of space to explore their identity, but we often find ourselves lacking adequate social connection. \n\nMurthy presents an alternative to these two undesirable options, in the form of a \"Third Bowl.\" This, he imagines, is a culture where each person is appreciated for their unique identity, but also has a strong network of social support to lean on. In *Together*, he goes on to explain how cities like Anaheim have fostered Third Bowl cultures [by actively valuing kindness](https://cityofkindness.org/the-first-city-of-kindness/). \n\nIt's truly heartwarming to see physical communities coming together over missions of kindness, but, as I read, I wondered whether digital connection could be engineered to produce the same effect. After all, the promise of the social internet was that anyone, anywhere, could find a community for themselves. While this dream has come true for some groups, others are left treading water in a world of shallow Facebook friendships. \n\nIn my quest to understand the failings of the social internet, I found solace in an unexpected place. Ben Thompson runs a blog and newsletter called [Stratechery](https://stratechery.com), where he publishes insights on tech trends and business strategy. He recently wrote an article describing [Social Networking 2.0](https://stratechery.com/2020/social-networking-2-0/), which explains how v1 digital products--the ones that simply copy what already exists online--devolved into a world of hateful tweets and professionally produced content. Social Networking 2.0, meanwhile, is online connection that draws on the unique benefits of digital technology. Facebook, for example, has embraced this trend by investing in apps like Messenger and WhatsApp, which create smaller, more trusting groups, centered on friendship rather than engagement. These groups are often centered around interests--anything from fly fishing to real estate investment--that allow us to explore and be appreciated for the many sides of ourselves. \n\nMessenger and WhatsApp are certainly a step in the right direction, but they mostly reflect an enhancement of existing connections with digital technology. They enable us to deepen our connections with the people we already met in person. But is it possible to go the opposite way? To form connections digitally, and bring them into our physical lives?\n\nThere are two major barriers to this sort of social connection: first, the norms of online connection are generally set up as ephemeral interactions--likes, retweets, and comments--that are not conducive to long-term friendship. Second, there's just so much choice--out of the billions of people on the internet, how do you find the five or six individuals who can become your closest circle of friends?\n\nI believe that, with conscious effort, we can readily surmount these barriers. We can develop online communities centered around kindness and long-term connection, designing systems to give each individual trusted friends who support their unique identity. Even within existing social networking platforms, we can work to establish norms of kindness and support, as if we are all neighbors in Anaheim. By adopting ideas from Third Bowl cultures, we can create a future where digital technology helps us to form real and deep friendships, and, as Dr. Vivek Murthy would say, to embrace the healing power of human connection in a sometimes lonely world. \n","preview":"\nObesity, smoking, and alcohol are the typical targets of a United States Surgeon General. Dr. Vivek Murthy, however, realized that, outside of these well-publicized dangers,...","isoDate":"2021-01-19T05:00:00.000Z"},{"id":"explore-exploit","layout":"post","title":"Exploration and Exploitation: Finding Consistency Across Business, Personality, and Food","date":"15 Dec 2020","description":"","image":"/assets/images/2020/12/nanjing.jpg","author":"anthony","categories":["philosophy","business","productivity"],"tags":["productivity","exploration","risk theory"],"content":"\nImagine you're a comedian, debating between two routines. The first is a safe bet, a tried-and-true routine where the audience is guaranteed to laugh. The second, however, is more experimental, and you've never tried it out to see if it works. Which do you choose?\n\nThis situation is a classic example of the exploration-exploitation dilemma: should you try something new or stick with what's working?\n\nLet's see if we can figure out the optimal proportion of exploration to exploitation, applied to success in both business and personal contexts. \n\n## Talking Business\n\nMost major tech companies value exploration. Reid Hoffman, co-founder of LinkedIn, says in a [Masters of Scale strategy session](https://mastersofscale.com/endeavor-strategy-session/) that a model for a growing company might be around \"70% core, 20% easy expansion, and 10% venture bets.\" Of course, the exact allocation of resources depends on the strategic position of the business--if the core area is growing too slowly or has a low total addressable market, it makes sense to invest more in expansion. \n\nHoffman defines core business as the established way of doing things, whereas easy expansion might involve new features (e.g., Youtube letting their creators make Instagram-style \"stories\"), and venture bets involve a totally new, unproven idea for the company, like adding Influencers to LinkedIn. In this case, Influencers redefined LinkedIn as not just a networking site but also a source of business-related information. \n\nVariations on the core-expansion-venture model are common and logical for high tech businesses. More importantly, though, they map quite directly onto our examination of the exploration-exploitation dilemma: spend the majority of resources exploit the core area and easy expansion, and use the rest to explore risky and unproven ideas. \n\nWhy might a tech company value exploration? We can understand them by modeling a business's growth as a function, where the input is all of the business's activites and the ouput is its growth rate. First, a business seeks to maximize its growth while generating consistent revenue, and taking risks allows the company to make sure they don't get stuck in the graph's local optima. Second, because of the constant change in the industry, the function to optimize is also changing constantly, and investing in exploration helps the company discover new opportunities that might not have existed before.\n\nIf the math analogy doesn't do it for you, the two reasons for exploration are (1) finding better ways of performing the same activity and (2) finding new things to do, because the old business model isn't going to work forever--optimization and insight. \n\nAnd investing the majority of resources into safe bets exposes you only to the positive side of risk. To understand this point, just look at the 10% spent on risky venture bets. If the bet pays off, the bold effort could expand the business into an entirely new industry. If it doesn't, you only lose 10% of revenue, at most. Nassim Nicholas Taleb advocates a similar approach with his [Barbell investing strategy](https://www.investopedia.com/articles/investing/013114/barbell-investment-strategy.asp).\n\nFor a business, then, it makes sense to invest most resources into exploitation, with some going into exploration. But does this work for humans?\n\n## Applying exploration-exploitation to personal success\n\nOn face value, it seems like the same approach applies to humans. Whatever we seek to optimize, whether productivity, happiness, or fulfillment, we're likely to find plenty of local optima, sets of activites that seem to maximize our productivity, but only to a fixed extent. At this level, a person would be competent but could never become wildly successful. \n\nThat is to say, just like businesses can explore new opportunities with venture bets, individuals can create their own luck by seeking out new experiences, both to enhance their core area and discover new interests. Optimization and insight. For example, if I'm a professional baseball player, it could be that going hiking will help me improve my endurance while running the bases, whereas investing helps me discover a new passion that I excel in. \n\nBut humans differ from businesses in one important respect: our daily activities deeply influence each other. Because learning and creativity depend on our brain's ability to analogize, discovering opportunities depends on cultivating a diverse set of experiences that can inform a unique worldview. Humans, then, are less like a one-function business and more like a bundle. Get a Gmail account, and you get access to Google Drive, Google Photos, and a whole ecosystem of new services. Each new service creates value for the bundle as a whole -- it's a network effect. \n\nBecause of this effect, exploration in humans is probably even more important than in companies, which isn't surprising when you look at the number of diverse activities pursued by some of the world's most successful people. Whole books, like David Epstein's *Range*, are built around the value of generalization. \n\nSo how does this philosophy of exploration work out in the real world? It can be applied to any number of domains as a way of finding the optimal process for success.\n\n## Examples of the balance between exploration-exploitation across industries\n\nLet's take personality as an example. If personality is a function of the information we take in over time, then the variety of information we take in could be seen as the input to an exploration-exploitation function. Specifically, we're interested in social information, which is distinct from abstract knowledge. They say we are the sum of our five closest friends, but part of this effect might just be because we select the friends who already fit with our personality, rather than adapting to fit them. Exacerbating this trend, social media (like Instagram or Facebook), thanks to personalized content recommendation algorithms, is likely to reinforce a person's existing tendencies in personality rather than exposing them to a new outlook. \n\nIn the realm of personality, a person seeking to spend some of their resources on exploration rather than exploitation (and hopefully discover a more optimal personality) would want to break out of their traditional social circle and meet new people. This desire explains the allure of [bubble-hopping](http://bites.bespokecph.com/blog/bubble-hopping), where a person can use the internet to find random events to attend and random people to meet. \n\nFor another example, let's look at food. Although individual preferences vary, humans generally desire both safety and novelty, to some extent. From an evolutionary perspective, trying that tasty-looking berry that we've never tried before is likely to give us diverse nutrition, compared to if we just ate wheat. But try too many strange berries and sooner or later one of them will be lethal. In the same way, modern humans are always looking for new foods to try, while still enjoying the foods that we know we'll like. \n\nOur desire for novelty seems to depend on our environment. In a tourist town, stores offering the local specialty might be more popular than the Hard Rock Cafe. Either way, though, the value of novelty is clear, as even chain restaurants constantly invent new foods to increase sales, some of which succeed and some of which are the McRib. Again, there seems to be a similar balance, where the majority of menu items are safe bets and a few are new, risky ventures.\n\nIn a world of increasing personalization brought on by the internet, it's becoming easier and easier to exploit our existing tendencies and strengths. But the exploration-explotation dilemma, whether examined through business, personality, or food, demonstrates the value of exploring new things. Moreover, it suggests an opportunity to take advantage of the often-overlooked market for exploration across industries. \n\nSo what's the answer: exploit or explore? Probably a combination of both. A fixed minority of resources allocated to exploration, whether time, money, or menu space, can reap major benefits, by helping us optimize our existing performance and find new tasks at which we excel. ","preview":"\nImagine you're a comedian, debating between two routines. The first is a safe bet, a tried-and-true routine where the audience is guaranteed to laugh. The...","isoDate":"2020-12-16T00:00:00.000Z"},{"id":"book-recommendations-2020","layout":"post","title":"My Favorite Books of 2020","date":"1 Dec 2020","description":"I read quite a few books this year. Here are my favorites.","image":"/assets/images/2020/12/gatsby.jpg","author":"anthony","categories":["books","reading"],"tags":["list"],"content":"\nI used to think books were like drugs. And I don't mean garden-variety pills like Benadryl or Tylenol--I'm talking hardcore drugs: nicotine, cocaine, and so on. How did I come to this conclusion? Like many kids, I would often start reading a book and struggle to put it down. I would start reading *Percy Jackson* in the morning and look up to find the sun was already setting. If addiction is a behavior where dependence on a substance interferes with a person's ability to live a full life, my younger self was a full-blown pothead, if only for books instead of weed.\n\nBut I was wrong about books. They're not addictive drugs; instead, they're more like food. I now believe in a more holistic way of thinking about information input and output, by imagining the ideas that come into my head are like ingredients in a blender. Podcasts. Magazines. Movies. Books. *Kerplunk*. *BZHHHH*. And out comes a smoothie of new and interesting ideas. This way, books come out looking like a relatively nutritious option (at least compared to TV shows and Instagram feeds).\n\nAll that to say, I decided to compile a list of the books I most enjoyed reading this year. Treat this as a list of ingredients, a diverse set of raw materials that can help you achieve a result that is both tasty and nutritious (and they might inspire you to read, too). \n\n## General nonfiction\n\n- *Thinking, Fast and Slow* -- Daniel Kahneman\n- *The Black Swan* -- Nassim Nicholas Taleb\n- *The Master Algorithm* -- Pedro Domingos\n- *Scale* -- Geoffrey West\n\n## Business\n\n- *How I Built This* -- Guy Raz\n- *The Innovator's Dilemma* -- Clayton Christensen\n\n## Memoir\n\n- *Educated* -- Tara Westover\n- *On Writing* -- Stephen King\n\n## Self-help\n\n- *Rich Dad, Poor Dad* -- Robert Kiyosaki\n- *How to Win Friends and Influence People* -- Dale Carnegie\n- *When* -- Daniel Pink\n\n## Fiction\n\n- *The Remains of the Day* -- Kazuo Ishiguro\n- *The Alchemist* -- Paulo Coehlo\n- *The Great Gatsby* -- F. Scott Fitzgerald\n- *The Road* -- Cormac McCarthy\n- *The Adventures of Huckleberry Finn* -- Mark Twain\n- *The Bell Jar* -- Sylvia Plath\n\nIf you've read any of these books or have other recommendations, feel free to leave a comment below, or drop me a line at [azhou.blog@gmail.com](mailto:azhou.blog@gmail.com).","preview":"\nI used to think books were like drugs. And I don't mean garden-variety pills like Benadryl or Tylenol--I'm talking hardcore drugs: nicotine, cocaine, and so...","isoDate":"2020-12-01T05:00:00.000Z"},{"id":"liupanshui-capital-of-cool","layout":"post","title":"The Two Sides of Liupanshui","date":"14 Oct 2020","description":"Musings from my trip to China.","image":"/assets/images/2020/10/liupanshui_skyscrapers.jpg","author":"anthony","categories":["narrative","travel"],"tags":["liupanshui","china","inequality"],"content":"\n\nLiupanshui is a city of incredible contrast. On one side is the “capital of cool” — a clean modern summer home for visitors escaping the heat of the big cities in the East of China. Go through a tunnel, however, and you are thrust into a different world.\n\nThe first thing you’ll notice is the air. Old Liupanshui is a steel town, and the air hangs thick with soot. Flecks of coal, ranging from microscopic to corn-flake size, transform the sky into a soupy sort of oatmeal. Once a major employer, the local steel company has taken a hard hit in recent years, burdened by the unprecedented rise of automation. \n\nWe coast through the unkempt streets in a sleek Honda CR-V — my brother and I, my mother and her sister — searching for the past. At last we arrive. In the span of a couple dozen years, the landscape has mutated so that the dilapidated buildings are now hundreds of feet from the road, barely visible through the thick smog. But the corrugated metal roofs and the ancient wooden walls reveal their identity:\n\nThis is the site of my mother’s childhood home. We pull off to the shoulder and snap a few photos. Then we pile back in the car. As we drive on, my mom regales us with tales of the past. when they were little, she and my aunt would have to make tea to cover up the flavor of water, because it was tainted thick with charcoal and chemicals. \n\nA familiar whistle makes my ears perk up. A train! We park the car and venture out to a landing that overlooks a chasm where overburdened trains lug sloughs of coal, rumbling on their rusty tracks. But the incredible size of the trains is not what captures my attention. On the landing, a ring of spectators gather round a small, raised pavilion — what could they be up to? I wander over to the wall of humanity and stand on tiptoe to get a peek. I catch a glimpse of dice and a small wad of cash on the table — all the trappings of a game, and yet the spectators' faces are frozen like stone. A couple weatherworn faces turn towards me silently, their ragged clothes and deep wrinkles screaming “you don’t belong here.” I turn away and walk, quickly.\n\nIn my hurry to get away, I nearly run headlong into a group of kids playing a game of Chinese jump rope — like cat’s cradle, but played with legs instead of fingers. Their faces, darkened by soot, shine brightly, full of joy and wonder and whimsy. In the midst of apparent poverty and desolation, not to mention rising unemployment, they smile and laugh as they try to untangle the rope tied between them. \n\nFor a while, I marvel at these carefree children. Then I turn to the disillusioned rabble of gamblers. Back and forth, back and forth. I study myself, an intruder from the world of white tablecloths and privilege and books and books and books. The rumble of the trains fades into the distance, and all I can hear is the sweet laughter of the children, blending with the raucous cry of a triumphant gambler and the murmur of my mother and aunt’s wistful reflections on this place now foreign to them. The sounds swirl together, young and old, rich and poor, past and present, as a symphony of colors. For just that moment, the gap between us all had shrunk from a gaping canyon to nearly nothing at all. \n\n![Image of Liupanshui railroads](/assets/images/2020/10/liupanshui_railroad.jpg)\n<em style=\"display: block; font-size: 0.75em\">Liupanshui railroads, used to transport coal.</em>\n\nAfterwards, we returned to the “capital of cool,” which, while not without its own problems, is a world apart from the soot-stained old town. As I looked around, however, the new town seemed different somehow. The clean-swept roads and gleaming shopping center now icons of impossible perfection. \n\nI realized that, despite our shiny new cars and quiet suburban houses and Fourth of July neighborhood barbecues, we are all the children and grandchildren of the disadvantaged. Inequality is a reality that we all too often tuck under the rug and seldom see in the mirror. \n\nThat day in the city of incredible contrast, I discovered a new perspective and a better way forward. To watch. To listen. To empathize.\n","preview":"\n\nLiupanshui is a city of incredible contrast. On one side is the “capital of cool” — a clean modern summer home for visitors escaping the...","isoDate":"2020-10-15T00:00:00.000Z"},{"id":"why-to-not-use-aws-sdk","layout":"post","title":"Why you shouldn't use the AWS SDK in your front-end","date":"9 Aug 2020","description":"When you should avoid the AWS SDK for JavaScript, and when you should use it.","image":"/assets/images/2020/8/browserscenario.png","author":"anthony","categories":["programming","tutorials"],"tags":["javascript","AWS"],"content":"\nDisclaimer: this article (as the title suggests) is full of personal opinions and does not necessarily reflect hard facts. \n\nAfter several months of hair-pulling errors and hard-to-find bugs, I have finally free myself from using the AWS SDK (Amazon Web Services Software Development Kit) for JavaScript in the front-end of a project I'm working on. Why was I using it and why do I recommend you don't?\n\n## Why I was using AWS SDK in the Front-end\n\nIn this project, I was using the AWS SDK primarily for authorization using Amazon Cognito. I was able to store an authorization token in the frontend, which automatically granted the client access to privileged AWS resources, such as a Lambda function used to fetch user information. The advantage of doing authorization is in the frontend is that the backend only has to handle the core logic of the program, without the hassle of validating the client's identity each time a request is made.\n\n## Why you shouldn't use AWS SDK in the Front-end\n\nHowever, this approach is not only insecure but also inefficient. Here's why I'd generally recommend against using AWS SDK in the frontend of your website:\n\n1. **Separation of Logic:** with authentication and authorization spread across the frontend and backend (i.e., when using AWS SDK in the front-end), coding the frontend becomes more complicated. Personally, I prefer a system where the frontend code only displays information, and the logic is separated from the user interface elements. Once I removed the AWS SDK for JavaScript from the frontend by switching to a RESTful API, I was able to easily move the authorization logic of my project into the backend, by using cookies.\n2. **Security:** speaking of cookies, they are a much better practice for security when compared to storing tokens directly on the client. In this context, I used a cookie to securely store JSON Web Tokens (JWT) containing my access and refresh tokens. Before this, I was using localStorage to persist the tokens on the client, because AWS SDK requires that JavaScript has access to authorization tokens. Storing tokens in localStorage is an insecure practice because anyone can access that information and therefore access resources as if they were a signed-in user. If you can `console.log(localStorage)` and see your access tokens, the site is probably not secure — it's probably vulnerable to attacks like [Cross-Site Scripting (XSS)](https://en.wikipedia.org/wiki/Cross-site_scripting).\n\nOn another note, calling asynchronous functions (such as Lambda functions) in the front-end creates complicated problems with timing while rendering the user interface, where everything usually runs synchronously. It often requires excessively checking for undefined values and writing messy code with callbacks. \n\n*Side note: every AWS function now supports promises, as noted in [this AWS blog post](https://aws.amazon.com/blogs/developer/support-for-promises-in-the-sdk/). This should help anyone becoming frustrated (as I was) with way too many nested callbacks.*\n\n## Conclusion\n\nWith a significant expense of blood, sweat, and tears, I have discovered that using the AWS SDK for JavaScript for authorization in the front-end is a recipe for countless hours of pain and suffering. Instead, I would recommend moving that logic to the backend, and considering the use of RESTful APIs for integrating the frontend and backend. I have yet to experiment with using it in the front-end for S3, DynamoDB or other AWS services, but I'd imagine those use cases would also bring up concerns surrounding separation of logic and security. What do you think of using AWS SDK in the front-end? I'd love to hear your thoughts in the comments below!","preview":"\nDisclaimer: this article (as the title suggests) is full of personal opinions and does not necessarily reflect hard facts. \n\nAfter several months of hair-pulling errors...","isoDate":"2020-08-09T04:00:00.000Z"},{"id":"bigandmini-automated-matching","layout":"post","title":"How I Built a Semi-Automated Matching System for Big & Mini","date":"13 Jun 2020","description":"A description of the new matching system we built for Big & Mini.","image":"/assets/images/2020/6/codeimage.jpg","author":"anthony","categories":["programming"],"tags":["programming","javascript","java","aws"],"content":"\nI co-founded Big & Mini in early April, as a nonprofit organization dedicated to combatting social isolation and creating virtual connections by matching older and younger people for video calls. Since then, the platform has taken off, featured in media outlets like [The Houston Chronicle](https://www.houstonchronicle.com/techburger/article/seniors-teens-coronavirus-social-distance-online-15228449.php?utm_campaign=tbgr) and we are continuously updating our technology to keep up.\n\nAs co-founder and CTO, I helped create a system where users can sign up and get matched to a Big or a Mini through our website, at [https://bigandmini.org](https://bigandmini.org). This is my report describing the latest major release of our technology.\n\n**Big & Mini 2.0 is now live!** It’s similar to the previous system, but more automated. In this report, I will share the reasoning behind this change, as well as an explanation of how the new system works. Our major new features are:\n\n- An automated training quiz\n- Automated matching system\n\nWith the initial version of Big & Mini, our team was spending too much time manually verifying training completion and making matches. Automation fixes both of these issues. Here’s how the system works. When they sign up, new users are sent to a staging area. Then, after verifying their email address and completing the training quiz, they are added to the pool of new users ready for a match.\n\nOnce a week on Sunday, a system administrator will press a button to trigger automatic matching for the upcoming week.\n\n\n## Training Quiz\nThe training quiz is pretty straightforward — answer all the questions correctly, and you have completed training. The multiple-choice questions on this quiz, based on our training documents, replace the short-answer Google Form we were using previously. Now, instead of asking a human reviewer to spend time reading through the same answers over and over again, the quiz answers are automatically graded by the computer.\n\n## Automated Matching\nThis is where the magic happens — in our new automated matching system, we use an algorithm that can achieve even better results than a human matcher.\n\nIt turns out matching is not a trivial problem. To illustrate my point, consider this simple example:\n\n| Times   | Bigs   | Minis            |\n|---------|--------|------------------|\n| Wednesday 10am | A, B, C | D, E, F, G, H, I |\n| Wednesday 2pm  | J, K, L | D, E, F          |\n\nA naive approach would be to match people up as we get to them (a greedy algorithm). But this would give us 3 matches for 10 am (A & D, B & E, C & F) and no matches for 2 pm, leaving out 6 of our 12 Bigs and Minis.\n\nIf you were asked to do the matching, how would you do it?\n\nWell, if you look closely, you’ll notice there’s a way to make sure everyone gets matched:\n\n| Times          | Bigs | Minis |\n|----------------|------|-------|\n| Wednesday 10am | A    | G     |\n| Wednesday 10am | B    | H     |\n| Wednesday 10am | C    | I     |\n| Wednesday 2pm  | J    | D     |\n| Wednesday 2pm  | K    | E     |\n| Wednesday 2pm  | L    | F     |\n\nNow all twelve of our Bigs & Minis will get to make a meaningful intergenerational connection right away!\n\nSo how do we go about solving this matching problem? Previously, we had a human administrator look through all the users who needed matches (much as you just did) and searched for the optimal solution by hand. As you might expect, this system did not work perfectly, and we ended up leaving out some of our Bigs & Minis. With Big & Mini 2.0, however, we have a new way of doing things: with an algorithm.\n\nThe algorithm, coded in Java, works as follows:\n\n1. Construct a weighted graph with Bigs & Minis as nodes.\n    a. The weights correspond to the number of connections a Big or a Mini has with another Big or Mini.\n    b. Edges are stored in an adjacency list\n2. At the same time, construct a list storing the number of edges each node has.\n3. Sort the list of edge counts in increasing order.\n4. For the node(s) with the least number of edges:\n    a. Sort the list of nodes by weight and pick the one with the lowest weighted edge (the least number of possible matches with a given Big/Mini).\n    b. Put this match onto a separate list as a pair and remove them from the main adjacency list.\n5. Repeat step 4 until there are no remaining nodes.\n\nUsing this algorithm, we end up maximizing the number of matches, by matching the least available people first. Because of the flexibility of this approach, we could even change the weights of the edges to account for factors like common interests or geographic proximity.\n\nHuge thanks to [Michael Zhao](https://michaelzhao.xyz/about) for writing this algorithm and Grace Liu for implementing it.\n\nAfter the algorithm returns a list of matches, the matched Bigs & Minis are removed from the staging area and given their matches with an initial time and meeting link, which will automatically show up on their Big & Mini dashboard. When the matches have finished processing, the browser automatically downloads a CSV containing the automatic matches made this week, for the convenience of the system administrator.\n\nBy automating the boring administrative parts of the job, Big & Mini 2.0 frees up our team to focus on more important things, like interacting with our users and making sure our Bigs & Minis have the best experience they possibly can.\n\n\n*This article was originally posted [on the Big & Mini website](https://bigandmini.org/whatsnewpage/ourposts/engineeringPost)*\n\n","preview":"\nI co-founded Big & Mini in early April, as a nonprofit organization dedicated to combatting social isolation and creating virtual connections by matching older and...","isoDate":"2020-06-13T04:00:00.000Z"},{"id":"daily-schedule","layout":"post","title":"How to be Productive This Summer","date":"17 May 2020","description":"A sample daily schedule, designed to maximize your productivity.","image":"/assets/images/2020/5/schedule.jfif","author":"anthony","categories":["tutorials","essay"],"tags":["scheduling","productivity","planning"],"content":"\nWhat do you want to accomplish this summer break? Let's say you want to do something meaningful. And yet, you may find that the weeks start to blur by without your accomplishing anything, and as soon as you know it summer will be over. The most rewarding goals require that you schedule time to sit down and get to work. If you're like me, scheduling time to do hard things is easier said than done, but behavioral science research has yielded some answers that might make our lives a little bit easier.\n\nNo matter what it is you might want to do — learn a new language, write a book, or improve your cooking skills — being productive will require you to set aside focused time each day to work. \n\nInspired in part by Dan Pink's book *When: the scientific secrets of perfect timing*, I have compiled a daily schedule designed to optimize your performance, based on recent behavioral science research. This exact schedule will not work for everyone — it's tailored for students like me with a particular kind of sleep schedule (which I'm told is relatively early) — but I hope this provides a good model to build upon. \n\n## The schedule\n\n### 7:15 am — Wake up\nSleeping makes us dehydrated, so Pink recommends we down a glass of water right after getting up. If you are hungry, eat a light snack of healthy carbs.\n\n### 7:30 am — Exercise\nExercising, for most people, increases alertness, allowing you to perform at your best for the rest of the day. Also, it's a healthy habit, and habits are easiest to keep when done in the morning.\n\n### 8:30 am — Breakfast\n\n### 9:00 am — Begin Deep Work\n\n> *Deep work* is a uninterrupted period of work that requires intense focus and creates value.\n\nIf you're unfamiliar with the concept of deep work, Cal Newport defines the concept in his groundbreaking book *Deep Work*. The book explains how deep work is essential to thriving in the modern economy, and how you can implement deep work in your own life. For the purpose of this schedule, consider this a time for the most intellectually demanding tasks of your day. Take breaks as needed, but make sure they are restorative: checking social media or your email inbox during this time can derail your focus and leave you feeling drained. \n\n### 12:00 pm — Lunch\n\n### 1:00 pm — Walk\nFor most people, the two hours after lunch are the most unproductive of the day. It's the dreaded mid-afternoon slump, and it's a good time to perform tasks that are less cognitively demanding, like listening to a podcast or walking the dog.\n\nResearch increasingly supports the fact that walking is beneficial to physiological and psychological health. An hour might be a long time to walk, but in a time of remote work and school, this can take the place of a one-hour commute. As Geoffrey West points out in his book *Scale: The Universal Laws of Life and Death in Organisms, Cities and Companies*, a roughly one-hour commute time has held constant for humanity throughout history, whether we are farmers walking to the village or suburbanites driving to the office.  \n\n### 2:00 pm — Shallow work\nFinally, this is the time to answer emails and check social media. While both activities are necessary, they are not cognitively demanding, and generally don't help us learn new skills or create lots of value. Using this hour to handle emails and other odds-and-ends helps assure that digital communications don't seep into other parts of your day and diminish your focus. \n\n### 3:00 pm — Nap\nNaps are most effective when they are 10-20 minutes long, before you enter deep sleep. Dan Pink typically sets a 25 minute nap timer, because it takes most people around 7 minutes to fall asleep.\n\n### 3:30 pm — Medium work\nLet me know if you think of a better name for it, but many of the things we do are not demanding enough to be considered deep work, nor are they as simple as sending emails and text messages. \n\n### 6:00 pm — Dinner\nDieticians usually recommend that we eat dinner around 4-6 hours before going to bed. \n\n### 7:00 pm — Relax\nThis is a great time to hang out (virtually?) with friends. In general, our moods will be happier but we may not have enough brainpower to work on things.\n\n### 10:00 pm — Read a book\n\n### 11:00 pm — Sleep\n\n ## A Caveat\n According to Dan Pink, this type of schedule does not work for one particular group of people: night owls. Teenagers and young adults, in fact, are especially likely to be owls. But before you immediately jump to conclusions, here's Dan Pink's 3 step formula for testing your chronotype:\n 1. On a day when you can sleep whenever you want, when do you usually sleep?\n 2. On a day when you can wake up whenever you want, when do you usually wake up?\n 3. What is the midpoint of those two times? (if I sleep at 12 am and wake up at 8 am, then my midpoint of sleep would be 4 am)\n\nDan Pink demonstrates that most people are not larks or owls, but a third type of bird, which he calls — you guessed it — \"third birds.\" In general, people's midpoints of sleep are distributed along a bell curve, with most people (60-70%) falling in the middle (roughly 3:30 - 5:30 am). Most of us are third birds. The further your midpoint of sleep falls from the center of this group, the less likely traditional schedules will work for you. If you're interested in learning more about chronotypes, I highly recommend Dan Pink's book, which takes a deeper dive into recent research on the topic.\n\n## Conclusion\nWhile this schedule may not work for everyone, it serves as a starting point for planning a productive day. After following the schedule for a while, I've come to enjoy the sense of accomplishment at having finished difficult tasks in the morning, giving me the freedom to relax without feeling guilty. If you follow a different schedule or tried out this one, leave a comment below! ","preview":"\nWhat do you want to accomplish this summer break? Let's say you want to do something meaningful. And yet, you may find that the weeks...","isoDate":"2020-05-17T04:00:00.000Z"},{"id":"covid-media-silence","layout":"post","title":"What does media sensationalism say about our ability to fight pandemics?","date":"16 Apr 2020","description":"An essay describing the reasons we should feel hopeful for our ability to persevere and help others amidst a global pandemic.","image":"/assets/images/2020/4/newspapers.jpg","author":"anthony","categories":["media","essay"],"tags":["coronavirus","COVID-19","optimism"],"content":"\nChaos. Everything I see is a blur of red and gray: people in the streets doubled over in pain, hacking up fresh crimson blood, ambulances rushing back and forth, only serving to move the bodies from one hellish place to another; and the uninfected, hoping that if they walk fast enough the horror will never catch up with them.\n\nIt's a scene from a movie, but I can't quite remember what it's about — maybe I'll think of it later. I switch off the television and nearly jump in shock at what I hear next in the real world, right at this moment:\n\nSilence.\n\nAfter a while, my ears adjust. Through the open window, I begin to hear again. The serenity of the neighborhood's sounds are comforting: the call of a mockingbird, the gentle bubbling of a creek, and the giggles of children as they struggle to rein in their new puppies while wearing rollerblades. The weather is marvelous, and a gentle breeze wafts by me with the scent of fresh blossoms.\n\nThe gentle hope of springtime leads me to start thinking and asking questions. I sit down to write.\n\nAt this historic moment, many of us are experiencing a confusing dichotomy of information: on one side, the media bombards us with warnings and horror and acute respiratory distress syndrome. Meanwhile, the reality of living in isolation, at least for those of us in suburbia, is almost unnervingly peaceful. What can we make of this disparity?\n\nFirst, it reflects our desire for sensationalist content. As the notable psychologist Daniel Kahneman has pointed out, our brains value coherence over statistical facts. In an increasingly competitive industry, even our most trusted news sources have no choice but to tell increasingly visceral stories, at the expense of more rational viewpoints. However, their attempts to remain relevant are rooted in a desire to share necessary information, a task they fulfill admirably. Journalists help us make sense of an overly complex world.\n\nHere is the disconnect: from hunter-gatherers eaten by lions to medieval serfs being beaten to death, we are born from a place of violence. Yet the world we are born into is safer than ever before, across the board, from criminal justice to modern medicine. The reminders of death and suffering, so frequent we become desensitized, are just a small red blotch on the large white canvas of the world: as we work to address the startling horror of this new disease, we must take moments to step back and take in the remarkable tranquility of our world, to see how far we've come.\n\nPause the television for a moment and listen to the silence. Only then will you begin to hear again.\n\nYes, we've got plenty of problems left to solve: disease, war, and climate change, to name a few. And some have it much worse than others — inequality is a vice that must be ended. But, as history demonstrates, we have endless reasons to be hopeful, to realize that the disparity between television violence and real-world tranquility is a blessing for hope in our long and dangerous fight against this virus. The same gap should make us resolve to help in any way possible, because the marvels of modern innovation have shown us what can be accomplished through collective social action. Instead of helplessness or indifference, those of us who experience peace during these unprecedented times can channel hope and optimism to make a difference. \n\nI remember what the movie was about now. It tells a story of finding the bright side in a seemingly desolate world. It's a movie about hope.","preview":"\nChaos. Everything I see is a blur of red and gray: people in the streets doubled over in pain, hacking up fresh crimson blood, ambulances...","isoDate":"2020-04-16T04:00:00.000Z"},{"id":"covid-19-open-source-technology","layout":"post","title":"Get Involved: Open Source Technology Fighting COVID-19","date":"3 Apr 2020","description":"A list of open source initiatives fighting the outbreak of novel coronavirus.","image":"/assets/images/cdc-coronavirus.jpg","author":"anthony","categories":["programming"],"tags":["coronavirus","COVID-19","open source"],"content":"\nThe purpose of this document is to provide a guide to the open source initiatives working to fight the novel coronavirus (COVID-19). Read on if you would like to help out.\n\n> __Open source technology__ refers to projects where the source code is freely available to the public, many of which would welcome anyone who can contribute.\n\nIf you’re interested in contributing, see if these projects have issues in their GitHub repositories or other tasks that you might be able to help with. If you don’t think you have enough technical expertise, help spread the word! See if any of your friends and family might be able to help out with open source (1) Software or (2) Hardware.\n\n## 1. Software\nIn the table below you will find a few open source projects to contribute to/look at. [Here is a more complete list](https://github.com/soroushchehresa/awesome-coronavirus) (note that the projects with more stars are probably more impactful and better to contribute to).\n\n\n| Project Name  |  Objective |  Language | Code  |  Demo |\n|---|------|---|---|---|\n|  CHIME: Covid-19 Hospital Impact Model for Epidemics |  Estimates the resources needed for hospitals (# of patients requiring ICUs, ventilation, etc.) for capacity planning. | Python  |  [GitHub](https://github.com/CodeForPhilly/chime/) |  [Demo](http://penn-chime.phl.io/) |\n| Locale.ai Covid-19 live visualization  | Visualizes the pandemic using open source data.  | JavaScript (Vue.js)  | [GitHub](https://github.com/localeai/covid19-live-visualization)  | [Demo](https://covid19.locale.ai/)  |\n| Tokyo Covid-19 Task Force Website  | Official government website for all information about Covid-19 in Tokyo.  | JavaScript (Node.js) | [GitHub](https://github.com/tokyo-metropolitan-gov/covid19)  | [Demo](https://stopcovid19.metro.tokyo.lg.jp/) |\n| Novel Coronavirus Dataset (from JHU CSSE)  |  Largest available official dataset of Covid-19 cases. | N/A  | [Dataset](https://github.com/CSSEGISandData/COVID-19) | [Visualization](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6)  |\n\n\n## 2. Hardware\nProbably even more important than software right now is open source hardware. Current projects include low-cost ventilators, PCR tools, and distributed computing. Whether you’re a designer, builder, or anything in between, these projects could use your help! Contributing is not as standardized as with software, so I’d suggest you go join the [Open Hardware Summit Discord Server](https://discord.gg/duAtG5h) to find out how you can help (they have a #covid-19 channel). \n\nFor an introduction to some of the biggest projects going on right now, check out [this article](https://opensource.com/article/20/3/open-hardware-covid19).\n\n\nThanks for reading and best of luck contributing. Stay safe out there!\n\n— Anthony Zhou, open source advocate\n","preview":"\nThe purpose of this document is to provide a guide to the open source initiatives working to fight the novel coronavirus (COVID-19). Read on if...","isoDate":"2020-04-03T04:00:00.000Z"},{"id":"audio-identification-app copy 3","layout":"post","title":"My Attempt at On-Device Machine Learning","date":"19 Mar 2020","description":"Using machine learning to identify audio from an iOS device.","image":"/assets/images/iPhone in hand.jpg","author":"anthony","role":"Independent project","skills":"Machine learning, iOS Development","categories":["narrative","programming"],"tags":["tensorflow","keras","python","swift","ios"],"content":"\nIn the past few years, on-device machine learning has taken the world by storm (or at least the worlds of some mobile app developers). The innovation — under development at companies like Apple, Google, and Facebook — promises to make any application of machine learning, from object classification to movie recommendation, seamless to experience on a mobile device. The underlying idea is simple: rather than using up ridiculous amounts of data sending files over the internet to be processed, why not do it all on the device itself?\n\nOn-device machine learning opens up revolutionary possibilities for apps in every industry. As a spring break project, I decided to explore what it might mean for automated audio identification, which could help those with hearing impairment identify the sounds in their environments. In documenting my progress, I made sure to note challenges and failure as well as achievements and success, so that this report can be as helpful as possible to anyone attempting such a project in the future.\n\n# Planning\n\n## My goal\nTo create a mobile app that can identify sounds in a recorded audio clip using machine learning. \n\n## My approach\nI hoped to use YAMNet, a pretrained machine learning model developed at Google, to run audio classification on an iOS app developed in Swift. The model makes inferences by taking in a WAV file as input and returning the sounds that might be present in the recording (helicopter, guitar, explosion, whistle, etc.). \n\nThere are two ways I could go about this:\n1. **On-Device Machine Learning:** pre-train a machine-learning model and load it into a device, such as an iPhone or a Raspberry Pi. The model can then make inferences using the computational power of the device itself.\n2. **Traditional Machine Learning:** host the machine-learning model on a server, and make requests to the server whenever inference is performed. The server then responds to the request with the results of running the model on the given data.\n\nSo what do these two options mean in the context of this project? Traditional machine learning would involve sending a request across the internet every time we need to make inferences (i.e., every time we need to identify the sounds in a recording). On-device machine learning eliminates the need to make these requests, so it is not only faster but also can run offline in areas with no data service.\n\nGiven the clear advantages in speed and reliability, I decided to attempt on-device machine learning:\n\n# Making the App\n\n## 1. Install and run the model\nFor this step, I followed the [installation instructions](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) in the YAMNet GitHub repository. Then, I downloaded some sample wav files from free audio sources online (such as [this one](http://soundbible.com/)) to test out the model. \n\nThis whole process went smoothly because the installation instructions were clear.\n\n## 2. Set up an iOS Recording App\nSetting up a recording app in Swift was relatively simple, because of how much Apple documentation and Stack Overflow discussions explain the topic. Specifically, I decided to follow [this tutorial](https://www.youtube.com/watch?v=CrLbuzM9IDs) from The Swift Guy on Youtube (parts one and two). I ended up with a basic recording app that stores and plays back m4a recordings.\n\nThis was my first time using Swift and XCode, and I must say I was quite impressed with the simplicity and ease of this work environment (especially the exceptional autocomplete in XCode). If the developer's time is the most important resource in software development, this iOS development environment seems to get the job done.\n\n## 3. Run the model on the iOS App\nHere's where we get into uncharted territory, and where I get to choose from the two options I listed earlier.\n\nFor machine learning on an iOS device, the two solutions I tried out were Google's [Tensorflow Lite](https://www.tensorflow.org/lite) and Apple's [Core ML](https://developer.apple.com/documentation/coreml) framework. Sparing the details, I did not end up succeeding with either approach, because I was not able to convert the pre-trained YAMNet model, made in Keras, to either a Tensorflow Lite (using TFLiteConverter) or Core ML model (using coremltools in Python). I believe the reason I kept getting errors was because the Keras model loaded pre-trained weights from an h5 file, resulting in a model structure that is slightly different from a freshly trained Keras model. As a result, the conversion libraries, like TFLiteConverter and coremltools, did not recognize the model. \n\nAt this point, I could decide either to retrain the entire model and risk it not working either, or pivot to a different approach. \n\n### Creating an API\nAs an alternative to on-device machine learning, I decided to deploy the model as an API using Flask, a lightweight Python library. For an introduction to Flask APIs, check out [this tutorial](https://programminghistorian.org/en/lessons/creating-apis-with-python-and-flask). My API would accept a POST request with a wav file and return the sounds discovered by the model.\n\n![Audio App Data Model](/assets/images/Audio App Data Model.jpg)\n\nSpecifically, my function for inference, returning the top five identified classes, was a slightly modified version of the sample prediction function provided in the YAMNet GitHub repository: \n\n        def infer(file):\n            graph = tf.Graph()\n            with graph.as_default():\n                yamnet = yamnet_model.yamnet_frames_model(params)\n                yamnet.load_weights('yamnet.h5')\n            yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')\n\n            # Decode the WAV file.\n            wav_data, sr = sf.read(io.BytesIO(file), dtype=np.int16)\n            assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n            waveform = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n\n            # Convert to mono and the sample rate expected by YAMNet.\n            if len(waveform.shape) > 1:\n                waveform = np.mean(waveform, axis=1)\n            if sr != params.SAMPLE_RATE:\n                waveform = resampy.resample(waveform, sr, params.SAMPLE_RATE)\n\n            # Predict YAMNet classes.\n            # Second output is log-mel-spectrogram array (used for visualizations).\n            # (steps=1 is a work around for Keras batching limitations.)\n            with graph.as_default():\n                scores, _ = yamnet.predict(np.reshape(waveform, [1, -1]), steps=1)\n            # Scores is a matrix of (time_frames, num_classes) classifier scores.\n            # Average them along time to get an overall classifier output for the clip.\n            prediction = np.mean(scores, axis=0)\n            # Report the highest-scoring classes and their scores.\n            top5_i = np.argsort(prediction)[::-1][:5]\n            return {\"predictions\": [(yamnet_classes[i], str(prediction[i])) for i in top5_i ]}\n\nIt loads the model weights, reads in the file, adjusts the data to fit model requirements, makes the prediction, and returns a dictionary of 5 predictions.\n\n### Attempting to deploy the API (Unsuccessful)\nAfter making the API, I decided the next step was to deploy it online, so that I could call it from any device. However, due to the extraordinary size of the `tensorflow` module on Python, the program required over 550 MB of space, which could not be hosted on any free-tier service I experimented with, including Heroku and PythonAnywhere.\n\n### Calling the API from the iOS App\nFinally, I decided to simply call the API locally from the iOS App, assuming that deploying the API online would be simple if I decided to pursue a paid or more complicated hosting plan. \n\nTo call the API, I used the `URLSession` module in Swift, and simply displayed the response as an alert. My request was as follows:\n\n        func postRequest() {\n            let url = URL(string: \"http://localhost:5000/api/v1.0/classify\")!\n            var request = URLRequest(url: url)\n            request.setValue(\"audio/wav\", forHTTPHeaderField: \"Content-Type\")\n            request.httpMethod = \"POST\"\n            \n            let filename = getDirectory().appendingPathComponent(\"\\(numberOfRecords).wav\")\n            \n            let task = URLSession.shared.uploadTask(with: request, fromFile: filename) { data, response, error in\n                guard let data = data,\n                    let response = response as? HTTPURLResponse,\n                    error == nil else {                                              // check for fundamental networking error\n                    print(\"error\", error ?? \"Unknown error\")\n                    return\n                }\n\n                guard (200 ... 299) ~= response.statusCode else {                    // check for http errors\n                    print(\"statusCode should be 2xx, but is \\(response.statusCode)\")\n                    print(\"response = \\(response)\")\n                    return\n                }\n\n                let responseString = String(data: data, encoding: .utf8)\n                \n            \n                \n                do {\n                    let json = try JSONSerialization.jsonObject(with: data, options: [])\n                    if let object = json as? [String: [NSArray]] {\n                        print(object[\"predictions\"]!)\n                    }\n                } catch {\n                    print(error.localizedDescription)\n                }\n                UI {\n                    self.displayAlert(title: \"Response received\", message: responseString ?? \"No message received\")\n                }\n            }\n\n            task.resume()\n        }\n\n# Final product\nI ended up with an iOS app that takes recordings and identifies the sounds in the recordings. Here is a screen recording of the app where I recorded myself whistling (video has no sound):\n\n<video style=\"margin:auto\" controls>\n  <source src=\"/assets/videos/Audio App Screen Recording.mp4\" type=\"video/mp4\">\n</video>\n\n# Summary\nOverall, even though I was not successful in converting the machine learning model to an on-device version, I was happy with the end product of an audio identification app. Maybe the technology of on-device machine learning is not yet mature enough for use in all contexts, or maybe I'm just taking the wrong approach. Have any thoughts, questions, or suggestions? Leave a comment below!\n\n# References\n1. Hershey, Shawn, et al. “CNN Architectures for Large-Scale Audio Classification.” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, doi:10.1109/icassp.2017.7952132.","preview":"\nIn the past few years, on-device machine learning has taken the world by storm (or at least the worlds of some mobile app developers). The...","isoDate":"2020-03-19T04:00:00.000Z"},{"id":"climate-change-economic-sustainability","layout":"post","title":"Why Sustainability Makes Economic Sense","date":"4 Mar 2020","description":"An analysis of investment incentives reveals a possible light at the end of the tunnel for the climate crisis.","image":"/assets/images/economics-of-sustainability.png","author":"anthony","categories":["essay","economics"],"tags":["climate change","investment","sustainability"],"content":"\n> \"We are on the edge of a fundamental reshaping of finance.\"  \n> – Larry Fink, 2020\n\nThis January, BlackRock CEO Larry Fink announced in his [annual letter to CEOs](https://www.blackrock.com/corporate/investor-relations/larry-fink-ceo-letter) that climate change needs to be a major priority of companies.\n\nBlackRock – and by extension Larry Fink – manages nearly *$7 trillion* in investments, making it the largest asset manager in the world. With a proven track record as an astute financial advisor, the billionaire is a trusted voice in the world of business. If he says we should protect the climate, he is not acting as a tree-hugging hippie but as a representative of his clients' financial interests.\n\nIn other words, Fink is saying that it makes financial sense for a company to invest in sustainability. To me, this was counterintuitive. Although I certainly hope we can slow climate change, I had always assumed that the profit models for companies that damage the environment demanded exploitation, even though such a model would be unsustainable in the long term. In other words, I thought that short-term interests will trump long-term goals.\n\n## The Tragedy of the Commons\n\nWhy would I and so many others think so lowly of companies? For me, this was a classic case of William Lloyd's *Tragedy of the Commons*. If you've never heard of it, here is a brief example: in Lloyd's time (1833), English villagers customarily shared a piece of land where all their cattle grazed. Lloyd hypothesized that if one villager let too many cattle graze on the land, overgrazing would result. The renegade villager would profit greatly in the short term from owning so many cows, at the expense of the pasture's long-term health. What if the other herders decide to do the same, so they can continue to profit? After a few years of this, the common would be completely destroyed, just because each one of the villagers made the individually rational decision to allow more cattle graze in the common. Wikipedia has [a great list of similar examples in the real world](https://en.wikipedia.org/wiki/Tragedy_of_the_commons#Examples).\n\n![Cows on a pasture](/assets/images/Cows_on_Selsley_Common.jpg)\n<em style=\"display: block; font-size: 0.75em\">Cows on a typical English common. (Image: Sharon Loxton / Cows on Selsley Common)</em>\n\nIt's not hard to see how the same tragedy of the commons can apply to environmental destruction. In a capitalist economy, companies act in their economic self-interest, using up resources like coal and oil and forests and fish and even water. Abusing natural resources gives companies a clear competitive advantage over others who attempt to protect the environment. To a large extent, this is the economic dynamic that has caused rampant environmental destruction in the modern world.\n\nBut what about climate change? At first glance, the tragedy of the commons seems to fit perfectly: the corporations are the herders, their manufacturing processes are the cattle, and the grass is the carbon they are releasing into the atmosphere, causing global warming. Unsustainable manufacturing processes are simply more profitable. Any company that tries to slow climate change is either just giving lip service for PR or is bound to fail, and even governments fall into the same trap as they compete for the investments of multinational corporations – climate change is inevitable and the world is going to burn! Tragedy of the Commons, QED!\n\nAnd that would be the case, except for one eccentric group of people: investors. Most investors, like Larry Fink, are not interested in the success of individual companies, but rather in the success of their portfolios as a whole, over the long term. It just so happens that investors control many of the world's largest companies, because the investors provide their lifeblood: money. So it's not exactly right to say that companies are always interested in short-term profitability. In fact, they are interested in whatever their stakeholders happen to be interested in, and for the past few decades the answer was always short-term profitability. But the tide is turning.\n\nEvidence is beginning to pile on for researchers who argue that climate change will fundamentally undermine the way our economy and society work. For investors, the evidence points to a simple axiom: **climate risk is investment risk.** As Fink describes in his 2020 letter to CEOs, climate change touches nearly every building block of the modern economy, including municipal bonds, the 30-year mortgage, inflation, interest rates, and even emerging markets where extreme heat causes productivity to decline. It throws a wrench into every economic model and growth projection. \n\nMany investors are beginning to ask themselves whether it is still safe to invest in unsustainable practices. Perhaps even more than the rest of us, investors have a clear economic stake in avoiding climate risk: to protect the long-term sustainability of their portfolios. As a result, companies are starting to embrace sustainability disclosures and frameworks for managing environmental issues. Close scrutiny is still needed, but the foundations are in place for a revolutionary shift towards sustainability.\n\n## The Implications of Investor Incentives\n\nIt's easy to get lost in the complicated turns of logic and the cascading of incentives, but the takeaway from Fink's letter is simply that there is now a strong economic incentive to invest in sustainability. In other words, perhaps for the first time in history, environmentalism is no longer a conflict between economic and moral interests – it is a collaboration.\n\nThis is such a critical point that I think it's worth repeating:\n\n> Perhaps for the first time in history, environmentalism is no longer a conflict between economic and moral interests – it is a collaboration.\n\nThat's why Fink's letter has given me hope for the Earth's climate. While certainly touching, moralizing and guilt-tripping have clearly been ineffective methods of catalyzing change, despite the impressive efforts of climate activists. Finally, economics has given us a real way out, a tangible answer to the crisis.\n\n## Conclusion\n\nThis is not to say that everything is okay or that we should stop caring about the climate. In fact, human-caused climate change has already caused irreversible damage to the planet. Rather, having an economic understanding of climate change should give us motivation to pursue climate activism even more strongly, scrutinizing corporate sustainability disclosures and demanding environmentally conscious actions, because we know our actions have a real economic impact. As the evidence piles on, growing numbers of people are beginning to realize that the economic impact of climate change will be a net negative. Simply put,\n\n**Sustainability makes economic sense.**\n","preview":"\n> \"We are on the edge of a fundamental reshaping of finance.\"  \n> – Larry Fink, 2020\n\nThis January, BlackRock CEO Larry Fink announced in...","isoDate":"2020-03-04T05:00:00.000Z"},{"id":"florence-incident","layout":"post","title":"My Dealings with Museum Security in Florence","date":"26 Feb 2020","description":"A true story from my trip to Florence.","image":"/assets/images/motorcycles-florence.jpg","author":"anthony","categories":["narrative","travel"],"tags":["italy","florence","language"],"content":"\nIt was a blistering hot day in Florence, part of the record-breaking heat wave of 2019. The kind of day where, despite the thick white coat of sunscreen I lathered on layer by layer, my normally tan skin shone the same red-orange hue of the tomato sauce in pizza margherita. In fact, just a few sprigs of basil would complete the picture.\n\nFlorence is a beautiful city. It would have been so easy to fall into a heat-induced daze, to sink into the soft plush clouds nestled against the pure blue sky and lose all sense of time and place and direction. But to do so would be fatal—the scammers were at the top of their game.\n\nFor my dog, our family’s backyard is his outhouse. My mother likes to call them *dilei*, land mines. Finding my way through the grass makes me feel a bit like Tom Cruise in Mission Impossible. It wasn’t until going to Florence that I realized the value of this daily practice.\n\nDuring our daytime tour of the city, the Florentian scammers demonstrated what I must admit is an ingenious strategy: they would cover the street with a tapestry of prints, featuring landmarks like the Leaning Tower, the Cathedral of Santa Maria, and the Baptistery of Saint John. The prints, obviously of cheap quality and mass produced, stretched across the street, covering nearly all of the age-worn cobblestones of the old town.\n\nAll this in the hopes that some hapless, naive (and probably American) tourist would step on one of these prints.\n\n“20 euros! 20 euros!” The shout startled me from my ruminations. A scamster had ensnared a member of our tour group. Judging by the victim’s face during this exchange, his brain was in conflict. On one hand, the American values of trust and minding your own business demanded that he compensate the merchant for his damaged merchandise. On the other hand, this was not America and no one was guaranteed to play by American rules. I watched to see what he would do next.\n\nIn the end, he paid up. For these Florentian scammers, fear was probably the greatest factor in their ability to convert innocent tourists into lucrative assets. Fear of being in an unknown place with unknown people and language and customs and who knows what could happen if I do something wrong and suddenly the police and the mafia and the carabinieri are after me what do I do?\n\nBut fear is not the only lesson I learned from the scammers. In fact, I noticed that these people were actually masterful artists, crafting alternate realities out of thin air.\n\nIn other words, these Florentians had perfected the craft of subjective truth. That afternoon, subjective truth is exactly what I decided to apply with a security guard at the Accademia Gallery in Florence, a museum featuring Michelangelo’s David. All I wanted was to keep my water and my reusable water bottle. Should be a piece of cake, right? Not so much.\n\nBeing a museum of great stature, the gallery prides itself on airtight security. As a result, part of the airport-style entry rules forbids beverage containers with a capacity of 1.5 liters or more. I, being a connoisseur of fine beverages, had with me a half-full 1.5 liter bottle of refreshing sparkling water, along with a reusable bottle of possibly suspicious tap water.\n\nAt this point, I realized I needed to take action, immediately. In my backpack were a reusable bottle (smaller than 1.5 liters) filled with water I did not need to keep, along with a single-use bottle (1.5 liters) filled with water that I needed as hydration for the remaining half of that sweltering day. Logically, then, I should empty out the reusable bottle and fill it with sparkling water, dispensing the 1.5 liter bottle.\n\nWhile my plan appeared foolproof, I forgot to account for one glaring fact: the security guard spoke even less English than I could speak Italian. To give you a sense of my knowledge of Italian, my vocabulary is limited to just a few words: *buon giorno*, *andiamo* and *pizza margherita*. How would it be possible to convey any sense of truth without a common language?\n\nThe museum security had graciously provided a receptacle for these unwanted beverage containers, in the form of a cardboard box with a trash bag stretched over it. By this time, it was already filled with bottles of all denominations, some left partially open and leaking. I wanted to get rid of the water in my reusable bottle. I wanted to keep the water in the larger bottle.\n\nSo I poured the water into the cardboard box. Whoops.\n\nWhat followed was bedlam: the security guard exploded into a vicious verbal tirade, ridiculing my unprecedented stupidity. Hearing the commotion, the neighboring security guard joined him in his outraged comments, also in Italian. All eyes in the room turned towards me, some with expressions of disgust and others with quiet sympathy. Apparently, I had done something terribly wrong, though I couldn’t understand what it could have been. But I wasn’t done yet—I still had to transfer the water from my larger bottle into the now-empty reusable one.\n\nIt must have taken no more than five seconds. And yet, the seconds it took to refill my water bottle slowed the rushing river of time to the pace of a half-frozen, mushy creek in the winter. Meanwhile, despite the museum’s ample air-conditioning system, my ears began to redden as I absorbed the heat shooting at me from the security guards’ incomprehensible insults and the crowd’s inexplicable attention.\n\nFinally, the moment was finished. With a satisfying *click!*, I screwed tight and shut the water bottle, replaced it in my bag, and continued into the museum with red ears, feigning deafness to the enraged shouts of the guards that still echoed off the walls behind me.\n\nReflecting on this moment now, I realize it may have been a mistake to pour the water into the box, even though it didn't hurt anyone. I have now concluded that some knowledge of Italian would have been necessary to avoid completely misunderstanding the situation. What do you think? Leave a comment below!","preview":"\nIt was a blistering hot day in Florence, part of the record-breaking heat wave of 2019. The kind of day where, despite the thick white...","isoDate":"2020-02-26T05:00:00.000Z"},{"id":"brain-machine-interface","layout":"post","title":"Can we achieve symbiosis with machines? Neuralink and Facebook try to answer the question.","date":"3 Feb 2020","description":"Recent attempts to create a brain-machine interface.","image":"/assets/images/neuralink logo.jpg","author":"anthony","categories":["psychology","essays","science"],"tags":["neuroscience","artificial intelligence"],"content":"Today's internet-enabled devices are competing for our attention, all the time. What if there was some way we could escape the loop of digital addiction? What if we could become one with our machines?\n\nEnter the Brain-Machine Interface (BMI), also known as a brain-computer interface (BCI), mind-machine interface (MMI), or direct neural interface (DNI): a device that translates neural signals into commands that control hardware or software.\n\nThe premise is simple. Currently, we interface with technology, and the outside world in general, through low-bandwidth options like keyboards and speech. For argument's sake, assume that your words-per-minute (WPM) when speaking is 150 (about average, according to the National Center for Voice and Speech), and that your WPM when typing is 75 (you can get a rough estimate of your typing speed with [this minute-long test](https://thetypingcat.com/typing-speed-test/1m)). Compare that with your brain, a highly efficient computer that may have as many as 50 processes running at the same time, as noted in this article from [the MIT Technology Review](https://www.technologyreview.com/s/532291/fmri-data-reveals-the-number-of-parallel-processes-running-in-the-brain/). Each one of us has a brain full of countless ideas and connections that never make it out into the real world, due simply to the ridiculously slow rate of communication between our powerful brains and the outside world.\n\nThink about how much time the average American spends on any kind of technology. [According to Inc magazine](https://www.inc.com/melanie-curtin/are-you-on-your-phone-too-much-average-person-spends-this-many-hours-on-it-every-day.html), the answer is *over four hours*. Our society is just beginning to realize the way digital overload can isolate us from nature and the people closest to us, but becoming a hermit and moving to a cabin in the woods doesn't seem so feasible. Is there any way we could use technology to enhance our lives rather than distracting from them? This is the vision Facebook Reality Labs has for its Brain-Computer Interface:\n\n## Facebook Reality Labs\n\n> Imagine a world where all the knowledge, fun, and utility of today’s smartphones were instantly accessible and completely hands-free. Where you could spend quality time with the people who matter most in your life, whenever you want, no matter where in the world you happen to be. And where you could connect with others in a meaningful way, regardless of external distractions, geographic constraints, and even physical disabilities and limitations.\n\nIn July of 2019, Facebook Reality Labs released [this report](https://tech.fb.com/imagining-a-new-interface-hands-free-communication-without-saying-a-word/) detailing some cutting edge work with the BCI. The lab sponsored a group of University of California, San Francisco (UCSF) researchers who are working to help patients with neurological diseases like ALS use brain-computer interfaces to communicate. BCI technology, as the report notes, is not new: it already helps people \"feed themselves, hold the hand of a loved one, and even fly a jet simulator.\" So why don't we all have computers in our brains? There are two limiting factors in BCI development: speed and invasiveness.\n\n### Talking fast and talking slow: balancing speed and invasiveness in BMIs\n\nEmily Mugler, an engineer on the Facebook Reality Labs BCI team, describes how the conventional approach of electroencephalography (EEG), where a cap of electrodes is placed on the subjects head, was just not fast enough for patients with ALS to communicate their ideas effectively. \"It sometimes took 70 minutes for a patient to type a single sentence,\" she says.\n\nLater, some labs attempted using electrocorticography (ECoG), which was faster but required a surgical operation inserting electrodes into the brain. Herein lies the problem that plagued past attempts to develop an effective BCI: fast technologies tend to be too invasive, whereas noninvasive solutions tend to be too slow. Researchers found themselves perplexed with the problem of getting inside the brain without physically getting inside the brain.\n\nFacebook Reality Labs, together with the lab at UCSF, plans to solve this problem with near-infrared light: by beaming harmless light waves into the subject's brain, a wearable device can sense blood oxygenation—much like in an fMRI (functional magnetic resonance imaging) machine—and use the measurements to guess at brain activity. While the system is currently \"bulky, slow, and unreliable,\" the UCSF researchers have successfully converted thoughts to words in real time and hope to reach a speed of 100 words per minute on a 1000 word vocabulary with an error of less than 17% ([Read more in their *Nature Communications* journal article](https://www.nature.com/articles/s41467-019-10994-4)).\n\nThe results are promising. Within a decade we might see keyboards by and large replaced by brain-machine interfaces, bringing us into a world where we could spend our lives connecting with others in a meaningful way, enabled by rather than encumbered by technology. But the universal adoption of BCIs raises serious issues of privacy. Can privacy even exist when our inner thoughts are exposed to the public? Moreover, Facebook is not the company many would most trust to safeguard their personal information, given its recent scandals.\n\nFear not, for Facebook is not the only one developing this technology: Elon Musk has founded a company developing the Neuralink, a brain-machine interface with no qualms about physically invading your brain.\n\n## Neuralink - Elon Musk\n\nElon Musk's justification for the need for a brain-machine interface goes something like this:\n\n> You could sort of think of humanity as a biological bootloader for digital superintelligence\n> — Elon Musk\n\nHere, \"bootloader\" refers to the small piece of code needed to start up a computer. To explain this idea further, let's examine the recent trends in artificial intelligence, as viewed in a Muskian mindset:\n\nAI is set to overtake humanity as the next stage in the evolution of consciousness. We'll have gone from amoebas to lizards to chimps to humans to machines. Or something like that. Scary, right? While the truth of Musk's statement is debatable, the underlying trend rings true: artificial intelligence, and technology in general, is expanding to replace many traditionally human tasks, from manufacturing to truck driving to being a cashier at McDonald's. How can we escape the ultimate supremacy of AI?\n\nThe answer is that we integrate the machines into ourselves, so that we can work together with our revolutionary technology rather than competing against it. A brain-machine interface would help us both drastically increase our productivity and avoid becoming obsolete. If you subscribe to Musk's view of the inevitable march of technology (detailed in [this interview](https://www.youtube.com/watch?v=f3lUEnMaiAU)), we have two options: brain-machine interface or extinction as AI takes over.\n\nHow does the Neuralink work? By inserting tiny electrodes linked to wires with diameter of 4-6 microns, or less than a quarter of the thickness of a typical human hair. The procedure goes like this: drill a 2 mm hole in the subject's brain, then use a \"sewing machine\" to weave up to 96 tiny threads into the brain, creating 3,072 channels. As with all things Elon, Musk plans to push the Neuralink forward at a rapid pace, starting human clinical trials by the end of 2020, focusing on brain diseases, and envisioning widespread adoption in 4 to 5 years.\n\nIt sounds scary because it is. With apparently little regard to the potential brain damage incurred by invasive surgery, the Neuralink in its current state creates two major health risks: glial scarring and broken electrodes. Glial scarring refers to scarring in the brain tissue that can inhibit communication and represents a reaction to serious brain damage. Meanwhile, the small electrodes used by the Neuralink are impossible to extract if broken, meaning they'll just be stuck in the patient's brain.\n\nMusk offers a compelling vision of the future—one in which we have achieved a \"symbiosis with machines\"—but the Neuralink's radical approach to and bold vision for the BMI may end up rushing to offer the world a half-cooked dish that leaves us a little sick to the stomach. Whether or not the technical and health-related challenges are resolved, the Muskian view would tell us that the need to stay relevant in a world of increasing automation outweighs the health risks incurred.\n\n## What happens next\n\nAs with any technology, the brain-machine interface has great benefits and drawbacks that will affect people unequally, although both the utopian and dystopian views are probably overstated. In the end, many of our fears will probably be nullified as the technology improves and as a lack of privacy becomes the new normal. After all, the subjective truths of today, like privacy, may not hold steady even ten years into the future.\n\nAnd China's going to do it anyway.\n\nNo matter what, it remains essential that the public holds Facebook, Neuralink, and the smorgasbord of BMI startups accountable, to ensure we don't rush blindly into a technology without asking ourselves whether it truly makes the world a better place.\n\n### Further reading and works cited\n\n- [VentureBeat on the Neuralink](https://venturebeat.com/2019/07/16/neuralinks-technology-embeds-tiny-wires-in-the-brain-to-read-electrical-pulses/)\n- [Neuralink plans to start clinical trials by the end of 2020](https://www.cnbc.com/2019/07/17/elon-musk-brain-machine-startup-neuralink-plans-human-trials-in-2020.html)\n- [Wikipedia article describing glial scarring and broken electrodes](https://en.wikipedia.org/wiki/Neuralink)\n","preview":"Today's internet-enabled devices are competing for our attention, all the time. What if there was some way we could escape the loop of digital addiction?...","isoDate":"2020-02-03T05:00:00.000Z"},{"id":"cultural-intelligence","layout":"post","title":"The Many Definitions of Intelligence","date":"15 Jan 2020","description":"Improving the way we define intelligence","image":"/assets/images/intelligence.png","author":"anthony","categories":["psychology","essays","science"],"tags":["intelligence"],"content":"\nWhat was Albert Einstein's IQ? Many sources online will tell you it was 160. But a quick foray into the supporting evidence for this number reveals that most sources have no scientific proof of Einstein's IQ score. In fact, 160 may just be a relic of the scale used for measurement: WAIS-IV (Wechsler Adult Intelligence Scale 4), probably the most common IQ scoring system, happens to have a max score of — you guessed it — 160. Einstein could be the dictionary definition of genius in today's society, yet we don't even know his IQ. How, then, can we be sure of his intelligence?\n\nWell, we could look at his creation of a revolutionary new way of understanding the world through theoretical physics. In addition, IQ is a construct related to but separate from intelligence. Given that IQ does not answer our question, how can we truly measure Albert Einstein's intelligence?\n\nAs many recent critics of IQ have noted (duly), intelligence is a highly complex trait that cannot be reduced to a single score. However, IQ does demonstrate some correlation with job success, as noted [by Inc magazine](https://www.inc.com/business-insider/why-iq-big-factor-future-success-job-performance-according-science-research.html). That is not to say that *intelligence* correlates to job success, however.\n\nWhat exactly is intelligence, how can we quantify it, and how does it differ from IQ? These are the questions this article will attempt to answer, based on psychological research. After outlining the broad types of intelligence, defining intelligence becomes even more problematic when examining the details. The cultural assumptions inherent in almost all attempts to measure intelligence lead to the formation of problems like exclusivity and perhaps even cultural supremacy.\n\n### What's the difference between IQ and intelligence?\n\nBeginning with some definitions will help us clarify this discussion. First, IQ (Intelligence Quotient) is an artificial score, much like a high score in Mario Kart, that measures a person's ability to perform on certain metrics. With Mario Kart, these skills depend on a person's ability to navigate complex terrain at a high pace, dodging banana peels and throwing turtle shells. IQ tests, meanwhile, measure a person's aptitude for problem-solving and verbal and spatial skills. Simple enough.\n\nIntelligence, however, has an deceptively simple definition. Oxford dictionary defines intelligence as \"the ability to acquire and apply knowledge and skills.\" This statement actually encompasses a vast variety of cognitive skills: memory, flexibility, and focus, just to name a few. Critically, defining intelligence in this way implies its dependence on environmental factors, because evidence has demonstrated that education has a clear influence on an individual's ability to learn. Environmentally-influenced intelligence is the type that IQ is attempting to measure.\n\nSo how can we measure intelligence as an intrinsic trait, based in genetics? We start out by recognizing this type of intelligence as separate from the traditional idea of IQ. Let's call it Intelligence A.\n\n### Intelligence A vs. Intelligence B vs. Intelligence C\n\nA more nuanced way of defining intelligence is to realize that there is more than one definition. In a book comparing genetic and environmental determinants of intelligence, Philip Vernon helps define three types of intelligence: Intelligence A is intrinsic. It depends on basic physiological factors like reaction time. Intelligence B includes the effects from the environment. It refers to the application of Intelligence A to the real world, and fits quite well with the Oxford definition of intelligence. Meanwhile, Intelligence C refers to the measurement of intelligence, as in IQ tests. Measured values of intelligence often correlate much more closely with Intelligence B, because it has closely matched the popular conception of intelligence for the last few decades.\n\nWhy is this distinction useful? Intelligence B, while clearly correlated with success, depends on the way a society is structured. In other words, it varies widely between cultures, and therefore between people. In other words, attempts to measure Intelligence B are highly variable and require that a new test is developed for every different culture. Meanwhile, Intelligence A consists of biological skills that should not vary across cultures.\n\nSo it's not surprising that IQ tests correlate with success — they reflect traits valued by our society and are therefore strengthened by other success-enhancing factors like education and socioeconomic status. If we want to truly measure the influence of intelligence on success *irrespective of culture*, we need to examine the biological basis of intelligence.\n\nWhat exactly does a test of this type look like? The twentieth-century psychologist Hans Eysenck, known especially for his theory of introversion and extroversion, gives a possible example in the form of AEP (Auditory Evoked Potential), a test that allows researchers to measure the reaction time of subjects. He cited preliminary studies that found at least a 0.83 correlation between the AEP results and those of a WAIS test (the standard IQ test mentioned earlier). Apparently, this level of correlation is higher than those between functionally similar IQ tests like the WAIS and Binet tests. However, the study finding this correlation was performed on a notably small sample of individuals; therefore, it poorly represents the vast diversity in levels of education and socioeconomic status around the world.\n\n### The cultural result of defining intelligence\n\nAs noted by [this American Psychological Association (APA) cover story](https://www.apa.org/monitor/feb03/intelligence), traditional measures of intelligence translate quite poorly across borders, raising problematic implications for cross-cultural interaction — an IQ test as currently given encapsulates the traits that are considered intelligent *in a particular culture*. If a person comes from a culture where those traits are not so valued, they may suddenly find themselves perceived as less intelligent for arbitrary cultural reasons. As noted earlier, IQ is correlated with success, so this cultural disparity may be a major factor behind the cultural barriers to success afflicting diverse modern societies.\n\nThe APA cover story explains that many psychologists no longer believe in the idea that a test can be completely absent of cultural bias, and probably for good reason. After all, the tests they are thinking of measure *Intelligence B*, which by definition encompasses cultural knowledge. Perhaps focusing instead on intelligence A as the standard definition for intelligence, as Hans Eysenck suggested, may be a better way to obviate cultural biases by directly measuring brain activity. Modern brain-scanning technology certainly makes this a promising alternative to current tests. As the world as a whole moves to adopt fair values that appreciate, rather than punish, diversity, intelligence, critical to economic success, should be reconceived in a form that transcends cultural boundaries, as Intelligence A.\n\n### Conclusion\n\nIntelligence, as traditionally defined, represents the sum of myriad genetic and cultural factors, including reaction time, education, socioeconomic status, and so on. Also known as Intelligence B, this is the type of trait commonly measured by IQ tests. Isolating the purely biological elements of intelligence as Intelligence A demonstrates that intelligence does not have to be the politically loaded, culturally and economically insensitive term that it is now, that there is hope for refining our definition of intelligence as both a biological and cultural phenomenon. Only then can we fully acknowledge the potential for growth that our current paradigm ignores all too frequently.\n\n\n## Notes\n\nHere are some works that I drew content from, other than those linked in the article:\n\n- Eysenck, H. J. (1988). The biological basis of intelligence. In S. H. Irvine & J. W. Berry (Eds.), Human abilities in cultural context (p. 87–104). Cambridge University Press. https://psycnet.apa.org/record/1988-98683-003\n- Wikipedia page on Intelligence Quotient: https://en.wikipedia.org/wiki/Intelligence_quotient\n- Oxford Dictionary definition of intelligence: https://www.lexico.com/definition/intelligence\n- Philip Vernon's book on intelligence: https://trove.nla.gov.au/work/11823287?q&versionId=45253153\n\n## Further reading\n\n- Definition of cultural intelligence from David C. Thomas: https://doi.org/10.1002/9781118785317.weom060051\n","preview":"\nWhat was Albert Einstein's IQ? Many sources online will tell you it was 160. But a quick foray into the supporting evidence for this number...","isoDate":"2020-01-15T05:00:00.000Z"},{"id":"mood-and-productivity-one","layout":"post","title":"Mood and Productivity, Pt. 1: Does happiness live up to the hype?","date":"8 Jan 2020","description":"What is happiness useful for?","image":"/assets/images/happiness-running.png","author":"anthony","categories":["psychology","essays","science"],"tags":["happiness"],"content":"\nToday, Vincent Van Gogh is indisputably recognized as a gifted creative. In his lifetime, however, he was unsuccessful by nearly every measure of the word: his art was never commercially appreciated, and many of his contemporaries considered him a madman. Suffering from severe depression for his whole life, van Gogh famously severed part of his own left ear, after an argument prompted him to start hearing voices. When he was thirty-seven years old, van Gogh walked out into a field and shot himself with a revolver.\n\nVan Gogh represents the classic ideal of the tortured artist. Alienated, misunderstood, yet ruthlessly innovative. Strangely enough, he is not alone in this predicament. Some of history's greatest creatives were notably depressed, including Ernest Hemingway (*The Old Man and the Sea*, *A Farewell to Arms*), Sylvia Plath (*The Bell Jar*), and George Orwell (*1984*, *Animal Farm*). It's enough to make one wonder if the trope of the tortured genius is not the exception but in fact is the rule.\n\nOur increasing understanding of neuroscience and psychology gives us revolutionary insights into how our brains work. This knowledge allows us to explore the surprising links between mood and productivity. Used properly, this knowledge could enhance creativity and work output, all without compromising life satisfaction. In this series of articles, I will draw on history and science to address a potentially controversial series of questions relating mood and productivity:\n- Is there any truth to the trope of the tortured genius?\n- How can happiness be enhanced?\n- What does happiness do for productivity?\n- Is negativity good for anything?\n- How can *negativity* be enhanced?\n\nFirst off, let's turn our attention to a hot topic of the moment: happiness. There is certainly great value to the pursuit of happiness, but neglecting the less desirable emotions may not be sustainable. Where should happiness be applied, and when does it hinder us more than it helps? Read on to find out.\n\n# Mood and productivity, Part One: What is happiness good for?\n\nIn the modern world, there is no shortage of \"positive psychology\" books, videos, and courses promising to help people find happiness in life. All these sources take for granted that happiness is an ideal goal. Intuitively, it seems obvious: a happy life is a good life. And yet, a growing body of evidence demonstrates that happiness is just one facet in living a life of fulfilment. What follows is an attempt to question the assumptions of those who promise the \"good life\" through happiness alone by presenting a more nuanced and scientific perspective, along with a guide to the areas where happiness might be best applied and where other emotions are more effective.\n\n## The pragmatic view\n\nTo determine the usefulness of a positive mood in measurable terms, we need to somehow measure how \"good at thinking\" a person can be, then see if those metrics change for happy and sad people. How can we measure that? First, by looking at easily studied traits, like short-term memory, then by applying such specific knowledge to a broader concept like creativity.\n\nSo how does happiness stack up on these metrics? Well, studies have demonstrated that there appears to be a link between positive mood and improved memory. Meanwhile, creativity is an area where being happier might not always improve performance.\n\n### Happiness improves short-term memory\n\nJustin Storbeck, a psychology professor at the City University of New York, studies the link between emotion and cognition. In 2015, a study of his found that a positive mood increases *working memory capacity*—that is, the more correct way of describing \"short-term memory\"—when compared to neutral and negative moods.\n\nThe study considered both verbal and spatial memory. Undergrad students were first induced to have a certain mood: the happy group was shown a 5-minute clip from *Jerry Seinfeld: Stand up in New York*, the negative group 5 minutes from *The Champ* and the neutral group 5 minutes from *If Dolphins Could Talk*. The participants were then instructed to complete a verbal task and an operation task. In the verbal task, participants were shown a list of 3, 5 or 7 words for 3 seconds each. Then—here's the catch—the operation task interrupted their focus: after the last word was shown, participants were asked to analyze several math problems (e.g., \"6/(3-2) = 0\") and indicate whether the equations were correct. Finally, they were asked to recall what words were presented in the verbal task, and in what order. The researchers then repeated the experiment, this time substituting the word-recall task for a task where participants memorize the locations of boxes presented in a grid.\n\nEarlier research seemed to indicate that positive moods were better for *verbal* memory, whereas negativity enhances *spatial* memory. In this study, however, the math problems served to help measure the persistence of memories through an unrelated task. This method revealed that a positive mood improved participants' performance on both verbal and spatial memory, supporting the idea that happiness is good for remembering all kinds of things. Storbeck goes on to cite a possible neurological explanation: \"positive mood increases dopamine, which is an important underlying biological mechanism for executive control and working memory.\"\n\nWhile focused on a relatively narrow segment of the population (undergrad students in New York), this study's implications for society at large could be enormous: if happy people truly think better than sad people, governments should consciously invest in the happiness of their citizens. Likewise, companies should recognize (as many already have) that a happy employee tends to be a productive employee. In this light, promoting positive psychology is not just a moral obligation but a pragmatic choice with quantifiable benefits.\n\nBut that's assuming that Storbeck's findings with regard to working memory capacity hold true for all the other metrics of cognition. Is that really the case? Let's take a closer look by examining another measure of brain function: creativity. In today's high tech economy, innovation is crucial to economic growth. Creativity is a requirement for innovators, and the question for many ambitious individuals is how to nurture and sustain creativity. Specifically relevant to our current discussion is how mood, positive or negative, can be leveraged to increase creativity.\n\n### Happiness can improve or hurt creativity\n\nCreativity is often thrown around as a blanket description for a complex batch of useful brain functions. But what is it exactly? For now, let's call it the production and execution of original ideas.\n\nDepression, obviously, is not a sustainable method of creativity. In fact, depression debilitates an individual, and we should not wish on anyone the fate of the tortured geniuses like Vincent Van Gogh and Sylvia Plath. Mild sadness, however, is a normal and healthy emotion that plays a vastly underappreciated role in our lives. In fact, research completed by professor Joseph P. Forgas at the University of New South Wales has determined that sadness can, among other things, [improve (long-term) memory, improve judgement, and increase motivation](https://greatergood.berkeley.edu/article/item/four_ways_sadness_may_be_good_for_you). Happiness, meanwhile, caused the opposite to occur in these studies, diminishing the participants' abilities to perform these critical cognitive functions.\n\nSo maybe happier people aren't necessarily smarter people. Barring the negative extreme of depression, a little bit of sadness can go a long way in improving performance at certain tasks. In other words, sadness can be an asset in pursuing the second part of creativity: the execution of ideas. But what about the production of original ideas?\n\nHere, evidence favors the happy. A 2008 meta-analysis of mood-creativity research revealed that happiness produces a measurable increase in a person's creativity, whereas emotions like fear and anxiety are linked to *lower* levels of creativity. Meanwhile, less active moods like relaxation and sadness demonstrated little correlation with creativity. In other words, being happy allows you to come up with more original ideas, which are the first requirement of creativity.\n\nIt seems that happiness is good for producing ideas, while sadness can help us follow through with them. Both are critical stages of the creative process: Ideation is nothing without execution. Execution is worthless without ideation. In the same way, perhaps, happiness is nothing without sadness, and the full array of human emotion as well. Amidst the deafening hype of positive psychology, it's often useful to slow down and remind ourselves that there are specific areas where happiness is appropriate, and others where sadness might serve us better. \"Being happy\" is just one element of the diverse source of fulfilment.\n\nRather than using our increasing understanding of our minds as a weapon to promote an ideology, we can apply a rigorous approach of questioning, so that, even in this nuanced mass of information, we can eventually puzzle out a better way of life. When we do discover it—given the spectacular complexity of human psychology—we can probably rest assured that the secret to a good life doesn't fit into a soundbite.\n\n## Notes\n\nHere are some works that I drew content from:\n\n- [Vincent Van Gogh's Wikipedia page](https://en.wikipedia.org/wiki/Vincent_van_Gogh).\n- The [finding that happiness increases working memory capacity](https://www.ncbi.nlm.nih.gov/pubmed/25947579) is presented by Storbeck and Maswood.\n- Baas, M., De Dreu, C. K. W., & Nijstad, B. A. (2008). A meta-analysis of 25 years of mood-creativity research: Hedonic tone, activation, or regulatory focus? Psychological Bulletin, 134(6), 779–806. [https://doi.org/10.1037/a0012815](https://doi.org/10.1037/a0012815)\n\n## Further reading\n\n- Catherine Weismann-Arcache and Sylvie Tordjman have studied the relationship between depression and high intellectual potential in children. Their article can be found at [nih.gov](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3356869/).\n- This Fast Company article provides an executive summary of scientific results promoting the usefulness of negativity: [https://www.fastcompany.com/3038199/the-positive-results-of-being-negative](https://www.fastcompany.com/3038199/the-positive-results-of-being-negative).\n- [A top-ten list of some tortured artists](https://www.toptenz.net/top-10-tortured-artists.php).\n- Karuna Subramaniam and Sophia Vinogradov [address links between happiness and improved cognition](https://www.frontiersin.org/articles/10.3389/fnhum.2013.00452/full)\n","preview":"\nToday, Vincent Van Gogh is indisputably recognized as a gifted creative. In his lifetime, however, he was unsuccessful by nearly every measure of the word:...","isoDate":"2020-01-08T05:00:00.000Z"},{"id":"service-workers-for-version-management","layout":"post","title":"Version Management with a JavaScript Service Worker","date":"1 Jan 2020","description":"How to manage an app's version numbers with the Service Worker API","image":"/assets/images/js-serviceworker.png","author":"anthony","categories":["programming","tutorials"],"tags":["javascript"],"content":"The JavaScript service worker API is a way to give online web apps offline functionality. By running in the background of the browser, service workers are able to cache important files and give users a smooth offline experience.\n\n## What is a service worker?\nA service worker is a script (i.e., a JavaScript file) that the browser runs in the background. It can be used for features such as offline apps and push notifications. In the case of an offline app, service workers can be used to determine whether a new version of the app is available as an update.\n\nA tip for Chrome users: to see what service workers are currently running in your browser, open Developer Tools (F12), click \"Application\" on the top bar, and click \"Service Workers\" on the sidebar that comes up.\n\n## How a service worker works\nA service worker has three steps in its lifecycle:\n1. Registration\n2. Installation\n3. Activation\n\nOnce these three steps are finished, the service worker stays in the browser indefinitely, even if the user refreshes the page, making it useful for storing information in the cache to be used offline. Read more on the service worker lifecycle [here](https://developers.google.com/web/fundamentals/primers/service-workers/lifecycle).\n\nHere is a [guide from Google](https://developers.google.com/web/fundamentals/primers/service-workers) detailing the code you need to set up a service worker.\n\n## Using service workers for version management\nEach time a page is refreshed, the service worker file is checked for any changes. If even a byte has changed, a new service worker is loaded, ready to place the old one whenever the `skipWaiting` event is triggered. We can take advantage of this fact by changing the service worker file in each new version of the app.\n\nTypically, the top of a service worker file will have something like the following variable declaration:\n    const CACHE_NAME = 'my-site-static-v1';\n\nEach time you create a new version of the app, just update the service worker script, changing 'v1' to whatever the new version is (e.g., 'v4.5.2') and a new service worker will be created automatically.\n\nIf you don't want to do this manually, you can replace the text in your service worker's JavaScript file automatically on build. The specific implementation of this differs depending on what system the app is using for build tasks.\n\nNote: if you happen to be using `grunt` for build tasks, make sure to use `grunt-text-replace` to replace the version number ([installation instructions for grunt-text-replace](https://www.npmjs.com/package/grunt-text-replace)), as `grunt-text-replace` supports in-place overwriting, which means you can directly edit the source file for your service worker. All of the other text-replace modules I found for grunt only allow editing in the build files.\n\n### Fetching the new service workers\nNow that you have a new service worker for each new version of the app, the user should be prompted to fetch the new version. This involves triggering the `skipWaiting` event on a service worker and reloading the page. A working implementation of this feature can be found [here](https://deanhume.com/displaying-a-new-version-available-progressive-web-app/).\n\nService workers are not easy to deal with, but they can enable effective version management with offline web apps. This tutorial represents just one of the many ways version management can be implemented in an offline web app.\n","preview":"The JavaScript service worker API is a way to give online web apps offline functionality. By running in the background of the browser, service workers...","isoDate":"2020-01-01T05:00:00.000Z"},{"id":"oatmeal-and-emergence","layout":"post","title":"Oatmeal and Emergence: How a whole can be greater than the sum of its parts","date":"30 Dec 2019","description":"How a whole can be greater than the sum of its parts","image":"/assets/images/oatmeal.jpg","author":"anthony","categories":["food","essays","science"],"tags":["emergence"],"content":"\nOatmeal. At best, it is a nutritious and filling breakfast. At worst, a sloppy and unappetizing gruel. In short, oatmeal exhibits stunning diversity. However, amidst all these seemingly disparate varieties of oatmeal lies a surprising commonality, one that may leave you thinking about your breakfast long after the last bite.\n\nImagine you got up early to prepare breakfast, and you’re cooking a bowl of oatmeal, as one does. Only today’s oatmeal is different—today, you heat up the water and get out the bag of oatmeal, to find that just one singular oat remains in the bag. Persisting in the face of adversity, you decide to cook the oatmeal anyways.\n\nAs usual, you turn down the fire and let the oatmeal simmer until it reaches the desired consistency, that perfect level of softness and coherence that we all know and love. But it never does. Try as you might, a singular oat will never achieve that consistency, because the lonely flake has no companions to stick to.\n\nThis brings to mind a scientific property called emergence: the idea that a whole can be greater than the sum of its parts. Truly great-tasting oatmeal reflects something more than just the taste of individual oats combined—it is the interaction between oats that creates oatmeal’s hallmark texture. The texture of oatmeal is what’s known as an emergent property.\n\nBreakfast foods are not the only things that demonstrate emergence.\n\nWater, for example, is very similar to cooked oatmeal in that water molecules like to stick to each other. Water’s intermolecular interactions give rise to emergent properties like cohesion, high specific heat, and evaporative cooling. Without emergence, we might not be alive today.\n\nOur brains also have emergent properties. Looking at an individual neuron, it is impossible to explain how thoughts might be formed. It’s only when we look at the brain as a system of neurons that we can study how these remarkable organs can process information, form memories, and read articles about oatmeal.\n\nThe animal world is full of emergence. Ant colonies, along with those of other social insects (and perhaps even humans), are especially interesting in their collective intelligence. An individual ant/bee/human is relatively unintelligent. Put together, the colony of ants/bees/humans can form tunnel networks, scout for food, and even invent the Internet.\n\nBut what’s so special about emergence? Why do we care?\n\nEmergence while mentioned in earlier texts, didn’t begin to take on serious scientific interest until the late 19th century. In the early days of science, reductionism, the idea that anything can be understood if we can break it down into its simpler components, was the prevailing mode of thought. If you want to know how a car works, take it apart and you’ll see the engine, battery, radiator, and so on, each with a specific function that contributes logically to the functioning of the car. Reductionism works for simple examples like motor vehicles, but breaks down when we start to analyze many natural systems.\n\nWhat if you want to know how the human brain works? Nothing in an individual neuron suggests the ability to think. Taking a brain apart doesn’t help us understand it. Here, we need to recognize that cognition is an emergent property—rather than analyzing brain cells, we can only consider the functioning of the brain as a whole.\n\nSome things can’t be explained as a combination of parts.\n\nYour morning oatmeal might be delicious, or disgusting, or one of the million options in between. Nevertheless, something magical happens every time oats (plural) are cooked. An unsuspecting individual, the singular oat, has potential for greatness, if only allowed to join a whole and form emergent properties.\n\nBehind a humble breakfast food lies a scientific principle essential to our understanding of the world. Simple as it may be, oatmeal is symbolic of the power of a whole to be greater than the sum of its parts, giving us humans (plural) a good reason to play with our food in the morning.\n","preview":"\nOatmeal. At best, it is a nutritious and filling breakfast. At worst, a sloppy and unappetizing gruel. In short, oatmeal exhibits stunning diversity. However, amidst...","isoDate":"2019-12-31T00:00:00.000Z"},{"id":"how-to-download-an-image-as-a-pdf-using-jsPDF","layout":"post","title":"How to Download an Image as a PDF using jsPDF","date":"11 Dec 2019","description":"Using Javascript to give users a \"Save as PDF\" option","image":"/assets/images/png-to-pdf.png","author":"anthony","categories":["programming","tutorials"],"tags":["javascript"],"content":"\nPDF documents are great because they preserve the format of a page. In other words, the content in a PDF document typically does not get distorted, pixelated, stretched, or compressed. Because of this, an image, once it's been converted to a PDF, will typically preserve its dimensions and clarity. Making sure an image always looks sharp and focused is almost always a concern of designers, developers, and even just regular old people. When making a site that deals with images, developers may increasingly consider implementing \"Save as PDF\" as an attractive option for exporting an image. That's why I decided to document some of the successes and struggles I encountered while working on this feature.\n\nMy goal was to add a \"Save as PDF\" option to an image processing web app. The dependency I chose to use was [jspdf](https://www.npmjs.com/package/jspdf), which is a free library that lets you use Javascript to generate PDFs of all kinds. Because the library is not designed specifically for converting images to PDF documents, the main struggle I faced was creating a PDF document with dimensions that matched those of the image, so I could avoid those distasteful white margins and unintentionally cropped images that would result from using a standard-sized page. Here's the way I set up jsPDF to make sure images could be smoothly downloaded as PDFs:\n\n# Set up jsPDF\nFollow the instructions from [npm](https://www.npmjs.com/package/jspdf). The easiest way to use jsPDF in a web-based project is to add a script tag linking to the cloud:\n        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/jspdf/1.5.1/jspdf.debug.js\" integrity=\"sha384-THVO/sM0mFD9h7dfSndI6TS0PgAGavwKvB5hAxRRvc0o9cPLohB0wb/PTA7LdUHs\" crossorigin=\"anonymous\"></script>\n\n# Convert the image to a data URI\nFor now, jsPDF can only add an image to a PDF if the image is in [data URI format](https://en.wikipedia.org/wiki/Data_URI_scheme). For the web app I was working on, the image I wanted to download already had a data URI as its source. In other words, it looked something like this:\n\n      <img src=\"data:image/png;base64,iVBORw0KGgoAAA\n      ANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4\n      //8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU\n      5ErkJggg==\" alt=\"Red dot\" />\n\n(Example taken from [Wikipedia](https://en.wikipedia.org/wiki/Data_URI_scheme#HTML))\n\nThis example would show up as a red dot: <img src=\"data:image/png;base64,iVBORw0KGgoAAA\nANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4\n//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU\n5ErkJggg==\" alt=\"Red dot\" />\n\nIf your image is not already a data URI, it's relatively easy to convert it. First, put the image on a canvas. Next, call canvas.toDataURL() to get the data URI of the image. You can find a great implementation of this method [here](https://davidwalsh.name/convert-image-data-uri-javascript).\n\n# Generate the PDF based on the data URI\nThe key to downloading an image as a PDF is to get the right image dimensions, to make sure the PDF page dimensions match those of the image.\nIn my case, the image dimensions were conveniently stored in order to facilitate image processing, but in a more general case you could get the image dimensions using `image.naturalHeight` and `image.naturalWidth`:\n\n    // Download the given image URL as a PDF file.\n    function savePDF(imageDataURL) {\n      // Get the dimensions of the image.\n      var image = new Image();\n\n      image.onload = function() {\n        let pageWidth = image.naturalWidth;\n        let pageHeight = image.naturalHeight;\n\n        // Create a new PDF with the same dimensions as the image.\n        const pdf = new jsPDF({\n          orientation: pageHeight > pageWidth ? \"portrait\": \"landscape\",\n          unit: \"px\",\n          format: [pageHeight, pageWidth]\n        });\n\n        // Add the image to the PDF with dimensions equal to the internal dimensions of the page.\n        pdf.addImage(imageDataURL, 0, 0, pdf.internal.pageSize.getWidth(), pdf.internal.pageSize.getHeight());\n\n        // Save the PDF with the filename specified here:\n        pdf.save(\"index.pdf\");\n      }\n\n      image.src = imageDataURL;\n    }\nNote: naturalWidth and naturalHeight are only supported in modern browsers. [These are the browsers that support these properties.](https://caniuse.com/#feat=img-naturalwidth-naturalheight)\n\nAnd that's how to use Javascript to give users the option to download an image as a PDF. To summarize:\n1. Set up jsPDF.\n2. Make sure the image you want to download is in data URI form.\n3. Use jsPDF to add the image to a new PDF document.\n\n\n#### A footnote: Data URL vs. Data URI\nThe form of image data dealt with in this tutorial can be called both a data URL and a data URI. This is because URLs (Uniform Resource Locators) are a subset of URIs (Uniform Resource Identifiers). As to whether HTML data URIs can be considered URLs,\n","preview":"\nPDF documents are great because they preserve the format of a page. In other words, the content in a PDF document typically does not get...","isoDate":"2019-12-12T00:00:00.000Z"},{"id":"how-to-scan-a-barcode-in-flutter","layout":"post","title":"How to Scan a Barcode with Flutter","date":"25 Nov 2019","description":"How to set up a cross-platform app to scan barcodes.","image":"/assets/images/scan-barcode-with-flutter.jpeg","author":"anthony","categories":["programming","tutorials"],"tags":["flutter","dart"],"content":"\nHere's how to set up a simple barcode scanning function, starting from a basic flutter app. In this tutorial we'll be using the [barcode_scan Flutter package](https://pub.dev/packages/barcode_scan). The package site has its own tutorial, but the tutorial has some outdated information, so I thought I'd try and correct that here.\n\n# Set up the configuration files\nAs listed on [barcode_scan's about page](https://pub.dev/packages/barcode_scan) on Dart Pub, start by making the following changes:\n\n## For Android:\n\nAdd this line to your AndroidManifest.xml to request permission to use the user's camera:\n\n        <uses-permission android:name=\"android.permission.CAMERA\" />\n\nAdd this line (to allow the BarcodeScanner activity) to your AndroidManifest.xml. Do NOT modify the name.\n\n        <activity android:name=\"com.apptreesoftware.barcodescan.BarcodeScannerActivity\"/>\n\nInstall the Kotlin plugin for Android Studio or your default editor\n\nEdit your project-level build.gradle file to look like this:\n\n          buildscript {\n              ext.kotlin_version = '1.2.31'\n              ...\n              dependencies {\n                  ...\n                  classpath \"org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version\"\n              }\n          }\n          ...\n\nEdit your app-level build.gradle file to look like this:\n\n        apply plugin: 'kotlin-android'\n        ...\n        dependencies {\n            implementation \"org.jetbrains.kotlin:kotlin-stdlib-jre7:$kotlin_version\"\n            ...\n        }\n\nIn your pubspec.yaml file, add `barcode_scan: ^1.0.0` to the list of dependencies, as follows:\n\n        dependencies:\n          flutter:\n            sdk: flutter\n          # other dependencies...\n          barcode_scan: ^1.0.0\n\n    Note: for some reason many tutorials, including that of the publishers themselves, say to add `barcode_scan: ^0.0.3` to your pubspec.yaml file. This tends to cause problems because 0.0.3 is an unsupported version of this dependency.\n\nClick \"Packages get\" in Android Studio or run flutter packages get in your project folder.\n\n## For iOS:\nTo use on iOS, you must add the the camera usage description to your Info.plist\n\n        <key>NSCameraUsageDescription</key>\n        <string>Camera permission is required for barcode scanning.</string>\n\n\n# Write the testing code\nThe package barcode_scan should now be successfully installed and configured in your flutter application. Now all that remains is to use the package in your app!\n\nOutside of the formatting, the key to this example is the call to `BarcodeScanner.scan()`, which uses the imported package `barcode_scan`.\n\n## Completed code in main.dart\n```\nimport 'dart:async';\nimport 'package:barcode_scan/barcode_scan.dart';\nimport 'package:flutter/material.dart';\nimport 'package:flutter/services.dart';\n\nvoid main() {\n  runApp(new MyApp());\n}\n\nclass MyApp extends StatefulWidget {\n  @override\n  _MyAppState createState() => new _MyAppState();\n}\n\nclass _MyAppState extends State<MyApp> {\n  String barcode = \"\";\n\n  @override\n  initState() {\n    super.initState();\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return new MaterialApp(\n      theme: ThemeData(\n        primarySwatch: Colors.blue,\n      ),\n      home: Scaffold(\n          appBar: AppBar(\n            title: Text('Barcode Scanner'),\n          ),\n          body: Center(\n            child: Column(\n              children: <Widget>[\n                Padding(\n                  padding: const EdgeInsets.all(8.0),\n                  child: Text(\"Press the button to scan an item:\"),\n                ),\n                Container(\n                  child: RaisedButton(onPressed: scan, child: Text(\"Scan\")),\n                  padding: const EdgeInsets.all(8.0),\n                ),\n                Padding(\n                  padding: const EdgeInsets.all(8.0),\n                  child: Text(barcode),\n                ),\n              ],\n            ),\n          )),\n    );\n  }\n\n  //scan a barcode, store result in this.barcode\n  Future scan() async {\n    try {\n      //use barcode_scan to scan the barcode\n      String barcode = await BarcodeScanner.scan();\n\n      //if the barcode has been obtained, display it\n      if (barcode.length > 0)\n        setState(\n            () => this.barcode = \"The barcode you scanned was: \" + barcode);\n    } on PlatformException catch (e) {\n      if (e.code == BarcodeScanner.CameraAccessDenied) {\n        setState(() {\n          this.barcode = 'Camera permission was not granted';\n        });\n      } else {\n        setState(() => this.barcode = 'Unknown error: $e');\n      }\n    } on FormatException {\n      //the user probably just pressed the back button, no need to print an error message\n      setState(() => this.barcode = '');\n    } catch (e) {\n      setState(() => this.barcode = 'Unknown error: $e');\n    }\n  }\n}\n```\n","preview":"\nHere's how to set up a simple barcode scanning function, starting from a basic flutter app. In this tutorial we'll be using the [barcode_scan Flutter...","isoDate":"2019-11-26T00:00:00.000Z"},{"id":"how-to-make-toast","layout":"post","title":"How to make toast","date":"9 Sep 2018","description":"How to make toast, with a detailed flowchart.","image":"/assets/images/toast-cropped.jpg","author":"anthony","categories":["food","tutorials"],"content":"Though not everyone can make a juicy Beef Wellington or perfectly crunchy macarons, well-toasted bread should be something we can all have. After all, the materials are so simple: a toaster and a slice of bread. It couldn’t be so hard, right? Wrong.\n\nIn fact, perfectly-toasted bread is not merely a simple daily task; it is a science. Consistently creating a crispy, golden-brown exterior and a soft, flavorful interior requires extreme care, from the cooking time to the model of toaster, to even the way the bread is sliced.\n\nFortunately for us, this delicacy has been meticulously researched. We even know how to make toast without a toaster. Unfortunately, reading all that information about toast would take a lifetime, so here's a simple flowchart for all your toast-making needs:\n\n![Toast flowchart](/assets/images/toast-flowchart.jpg)\n\nSo now you know more than you ever wanted to know about making toast. Time to revel in your enlightenment and savor the glorious golden-brown toast. Pretty soon, you'll be making the perfect toast for lunch and dinner, when friends come over, when you're hosting a meal for the General Assembly of the United Nations! Or maybe just for breakfast.\n","preview":"Though not everyone can make a juicy Beef Wellington or perfectly crunchy macarons, well-toasted bread should be something we can all have. After all, the...","isoDate":"2018-09-09T04:00:00.000Z"},{"id":"communication-between-trees","layout":"post","title":"Communication Between Trees","date":"17 Apr 2017","description":"A paper I wrote a few years ago about trees and how cool they are.","image":"/assets/images/forest.jpg","author":"anthony","categories":["biology","science","essays"],"content":"## Introduction\n\nAll over the world, trees stand guard over their forest ecosystems, like still, silent, sentinels. However, these trees might not be so silent after all. There is evidence that trees can communicate and help each other, uprooting the traditional idea of competition between trees as the only way they adapt.\n\n## The Importance of Fungi\nThere are many adaptations and structures that make communication and aid between trees possible. Many of these structures are ectomycorrhizal fungi, members a type of fungi that have a symbiotic relationship with trees and partially live within the tree roots. One study found that thirty-nine, give or take one percent, of total soil microbial biomass was contributed by ectomycorrhizal fungi, showing the important role of mycorrhizal networks in the below-ground transfer of carbon between trees (Högberg, Mona N., et al. 491). This evidence explains that the ectomycorrhizal fungi are an important part of the ecosystem. These mycorrhizal fungi form underground networks of mycelium, root-like structures. Specifically, Laccaria bicolor is a species of ectomycorrhizal fungi that has been studied. Researchers discovered that the genome of Laccaria bicolor increased in the areas of protein-protein interaction and signal transduction mechanisms, which could signal specialization for the purpose of symbiosis” (Martin, F., et al. 88). They also observed other interesting features of the species’ genome and life: “expression of several SSPs was downregulated in ectomycorrhizal root tips, suggesting a complex interplay between these secreted proteins in the symbiosis interaction” (Martin, F., et al. 88). These results show that the fungi L. bicolor may use small secreted proteins to aid in their mutualistic relationship with trees. In conclusion, “the rich assortment of MISSPs[mycorrhiza-induced small secreted proteins] may therefore act as effector proteins to manipulate host cell signalling or to suppress defence pathways during infection” (Martin, F., et al. 88). L. bicolor certainly contributes greatly to the networks between trees, and trees also help the fungi. One group researching fungi found that ”the C [carbon] used for production of these sporocarps is from tree below-ground allocation in the late season” (Högberg, Mona N., et al. 490). Trees place carbon underground and send some to other trees, while some of this carbon goes to benefit the fungi. Ectomycorrhizae are the main structures in the the networks connecting trees.\n\n![Mushrooms](/assets/images/mushroom.jpg)\n\n## Forms of Communication\nTrees can communicate a variety of signals and materials. One of the most common of these materials is carbon, which is transferred through root connections between trees. The transfer of carbon has been shown in numerous studies (Simard, Perry 581). Trees also send each other defense signals, though one study suggests that the communication of defense signals may not be through the root networks: “Even though root connections can trigger physiological responses to defoliation in non-defoliated aspen suckers (Baret and DesRochers 2011), our results suggest that these root pathways do not automatically lead to induction of defensive traits that are expressed in plants directly under herbivore attack” (Jelínková, Hana, et al. 1354).  Rather, it seems that the expression of defense signals and responses is regulated through airborne volatiles. The same study found that there was an upregulation of dihydroflavonol reductase and Kunitz trypsin inhibitor, two signs of defense response, in sibling ramets, whether they were connected to the control group or the wound-treated plants. This suggests a role of herbivore-induced plant volatiles (HIPV) in spreading defense signals throughout the population (Jelínková, Hana, et al. 1353). Communications are sent through the air as well as the roots. Trees can send carbon and defense signals to each other.\n\n> \"Trees send each other defense signals... through airborne volatiles.\"\n\n## Further Evidence of Tree Communication\nTrees help each other in other ways as well, creating mutualistic relationships between them. In order to continue the genetic lineage, clonal parent plants will help out their connected ramets. In one species of a water plant, clonal parent plants were found to transfer 10% of their carbon to connected ramets, and this has been shown to increase the survival and growth of the ramets (Warembourg, Roy 1459).  One group of Canadian scientists researched the relationship between Douglas Fir (Pseudotsuga menziesii) and Paper Birch (Betula papyrifera) using radioactive and stable isotopes of carbon. They noted that “Net transfer from B. papyrifera to P. menziesii was associated with whole-seedling net photosynthetic rates 1.5 and 4.3 times greater for B. papyrifera than for P. menziesii in full light and deep shade, respectively, and foliar nitrogen concentrations twice as great for B. papyrifera as for P. menziesii” (Simard, Perry 580). In other words, the birch helped out the fir because the fir was converting less energy from sunlight into glucose. They concluded that “when P. menziesii seedlings are establishing and growing in the shade of illuminated B. papyrifera in natural, mixed communities, they may benefit directly from their association with B. papyrifera through supplemental gains in transferred carbon” (Simard, Perry 581). Trees benefit from their relationships with each other.\n\n![Mushrooms](/assets/images/birch.jpg)\n\n## Consequences of Tree Communication\nIn general, trees that communicate and have mutualistic relationships with other trees are more successful than other trees. A group of researchers studying Canadian pine forests found that the late-season increase in the carbon allocation of boreal pine forests doubled the increase found in a study of temperate poplar forests, and this could be the reason for the decrease in leaf area of poplars during the late season (Högberg, Mona N., et al. 490). The trees that communicated in the form of carbon had larger leaf area later in the season. There is a growing body of evidence showing that trees communicate, and that the communication could be a large part of their survival strategies. However, there is still reasonable room for doubt, because of the proven fallibility of science, and the principles governing scientific reasoning. Pioneering studies may prove to be flawed, and many biologists acknowledge this, including the group studying colonies of aspen: “To clarify the role of aspen root networks in anti-herbivore defense, further studies employing real insect feeding are needed” (Jelínková, Hana, et al. 1354). In the world of science, study results are always taken with skepticism, as they should be, and the relatively new science of tree communication should be treated as such. However, so far, the trees can be seen to benefit from communication.\n\n## Conclusion\nTrees can likely communicate in ways that are beneficial to them, through carbon transfer, airborne volatiles, and networks of fungi. Behind the quiet exterior of a stand of trees, there may be a lively conversation taking place. It just takes a careful listener to hear it.\n\n\n\n## Works Cited\n\nAlbert, P., F. R. Warembourg, and J. Roy. \"Transport of Carbon among Connected Ramets of\nEichhornia Crassipes (Pontederiaceae) at Normal and High Levels of CO2.\"American Journal of Botany 78 (1991): 1459-466. United States Department of Agriculture National Agricultural Library. United States Department of Agriculture. Web. 04 Apr. 2017.\n\nHögberg, Mona N., et al. \"Quantification of Effects of Season and Nitrogen Supply on Tree\nBelow-Ground Carbon Transfer to Ectomycorrhizal Fungi and Other Soil Organisms in a Boreal Pine Forest.\" New Phytologist, vol. 187, no. 2, 15 July 2010, pp. 485-493. EBSCOhost, doi:10.1111/j.1469-8137.2010.03274.x.\n\nJelínková, Hana, et al. \"Herbivore-Simulated Induction of Defenses in Clonal Networks of\nTrembling Aspen (Populus Tremuloides).\" Tree Physiology, vol. 32, no. 11, Nov. 2012, pp. 1348-1356. EBSCOhost, search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=83483949.\n\nMartin, F., et al. \"The genome of Laccaria bicolor provides insights into mycorrhizal symbiosis.\"\nNature, vol. 452, no. 7183, 2008, p. 88+. General OneFile, [go.galegroup.com](go.galegroup.com/ps/i.do?p=ITOF&sw=w&u=j220919043&v=2.1&it=r&id=GALE%7CA189654068&asid=07b463289bd8b6cce0e7a45ad784f41e). Accessed 30 Mar. 2017.\n\nSimard, Suzanne W. and David A. Perry. \"Net Transfer of Carbon between Ectomycorrhizal Tree\nSpecies in the Field. (Cover Story).\" Nature, vol. 388, no. 6642, 07 Aug. 1997, p. 579. EBSCOhost, search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=9708140900.\n","preview":"## Introduction\n\nAll over the world, trees stand guard over their forest ecosystems, like still, silent, sentinels. However, these trees might not be so silent after...","isoDate":"2017-04-17T04:00:00.000Z"},{"id":"concrete","layout":"post","title":"Concrete","date":"25 Feb 2015","description":"A paper I wrote several years ago about concrete.","image":"/assets/images/concrete.jpg","author":"anthony","categories":["concrete","science","essays"],"content":"A mixture that is both universally noticed and universally ignored.\n\nWhen you think of concrete, you may imagine it as something boring and common. However, it is actually fascinating in that such a simple substance could change our world’s infrastructure so greatly. Here I will explain where it came from, how it is made, and how it works.\n\nIt all started in the year 3000 BC. According to Auburn University, that was when Egyptians first began mixing mud with straw to bind dried bricks. Also around this time, the ancient Chinese began using cementitious materials to hold together the bamboo on their boats and to make the Great Wall of China. Concrete was made in Ancient Rome by mixing one part lime, four parts sand, and animal fat, milk or blood. However, sometime around AD 400, the art of concrete was lost with the fall of the Roman Empire. Finally, in 1756, John Smeaton, a British engineer, rediscovered hydraulic cement (mixing cement with water) by repeatedly testing mortar in both fresh and salt water. In 1779, Bry Higgins was issued a patent for hydraulic cement (also known as stucco). In 1824,Joseph Aspdin, bricklayer and mason in Leeds, England, patented what he called Portland cement, since it resembled the stone quarried on the Isle of Portland off the British coast. Little did he know how important the cement would be in the near future. Throughout the rest of the 1800s Portland cement concrete became widely used in poles, beams, ships, buildings, canals, bridges, even skyscrapers… In just 100 years from rediscovery, concrete was already becoming essential to infrastructure. Concrete was becoming more and more popular by the day. Now, we are surrounded by it.\n\nBut how does this amazing mixture get made? According to cement.org, concrete is basically just three things: water, Portland cement, and aggregates (rock and sand). It could also contain admixtures, which are just things added to alter the texture or other properties of the finished concrete. The proportions are roughly 6% air, 11% portland cement, 41% coarse aggregate (think gravel-like), 26% fine aggregate (sand), and 16% water. Any drinkable water can be used for concrete; however, the purer the better. The materials are then mixed together until they are workable. After that all that remains is to put the concrete in the form and cure it by sprinkling water fog. The older the concrete is, the stronger it gets.\n\nSure, concrete is useful and we know how it’s made, but how does this all work? According to Northwestern University, the first step is called dissolution, where the cement dissolves, releasing ions into the water. Eventually, the density of the ionic species becomes so great that they will want to become solid. This brings us to the next step, precipitation, where the water-cement mixture forms a new solid substance. This substance coats and sticks to the aggregates to form concrete.\n\nAs you can see, concrete is a very interesting substance that has greatly affected the world in many positive ways.\n","preview":"A mixture that is both universally noticed and universally ignored.\n\nWhen you think of concrete, you may imagine it as something boring and common. However, it...","isoDate":"2015-02-25T05:00:00.000Z"}]
  
  export default posts;